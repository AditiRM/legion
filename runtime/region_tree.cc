/* Copyright 2015 Stanford University, NVIDIA Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


#include "legion.h"
#include "runtime.h"
#include "legion_ops.h"
#include "legion_tasks.h"
#include "region_tree.h"
#include "legion_spy.h"
#include "legion_logging.h"
#include "legion_profiling.h"
#include "interval_tree.h"
#include "rectangle_set.h"

namespace LegionRuntime {
  namespace HighLevel {

    // Extern declarations for loggers
    extern Logger::Category log_run;
    extern Logger::Category log_task;
    extern Logger::Category log_region;
    extern Logger::Category log_index;
    extern Logger::Category log_field;
    extern Logger::Category log_inst;
    extern Logger::Category log_spy;
    extern Logger::Category log_garbage;
    extern Logger::Category log_leak;
    extern Logger::Category log_variant;

    /////////////////////////////////////////////////////////////
    // Region Tree Forest 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RegionTreeForest::RegionTreeForest(Runtime *rt)
      : runtime(rt)
    //--------------------------------------------------------------------------
    {
      this->lookup_lock = Reservation::create_reservation();
#ifdef DEBUG_PERF
      this->perf_trace_lock = Reservation::create_reservation();
      int max_local_id = 1;
      int local_space = 
        Processor::get_executing_processor().address_space();
      std::set<Processor> procs;
      runtime->machine.get_all_processors(procs);
      for (std::set<Processor>::const_iterator it = procs.begin();
            it != procs.end(); it++)
      {
        if (local_space == int(it->address_space()))
        {
          int local = it->local_id();
          if (local > max_local_id)
            max_local_id = local;
        }
      }
      // Reserve enough space for traces for each processor
      traces.resize(max_local_id+1);
      for (unsigned idx = 0; idx < traces.size(); idx++)
        traces[idx].push_back(PerfTrace()); // empty perf trace for no recording
#endif
    }

    //--------------------------------------------------------------------------
    RegionTreeForest::RegionTreeForest(const RegionTreeForest &rhs)
      : runtime(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    RegionTreeForest::~RegionTreeForest(void)
    //--------------------------------------------------------------------------
    {
      lookup_lock.destroy_reservation();
      lookup_lock = Reservation::NO_RESERVATION;
#ifdef DEBUG_PERF
      perf_trace_lock.destroy_reservation();
      perf_trace_lock = Reservation::NO_RESERVATION;
#endif
    }

    //--------------------------------------------------------------------------
    RegionTreeForest& RegionTreeForest::operator=(const RegionTreeForest &rhs)
    //--------------------------------------------------------------------------
    {
      // should never happen
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_index_space(IndexSpace handle,
                                              const Domain &domain,
                                              IndexSpaceKind kind,
                                              AllocateMode mode) 
    //--------------------------------------------------------------------------
    {
      create_node(handle, domain, NULL/*parent*/, 
                  ColorPoint(0)/*color*/, kind, mode);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_index_space(IndexSpace handle,
                                              const Domain &hull,
                                              const std::set<Domain> &domains,
                                              IndexSpaceKind kind,
                                              AllocateMode mode)
    //--------------------------------------------------------------------------
    {
      // Note that it is safe that we do this in two passes
      // because we haven't given back the handle yet for
      // the index space so no one actually knows it exists yet.
      IndexSpaceNode *node = create_node(handle, hull, NULL/*parent*/, 
                                         ColorPoint(0)/*color*/, kind, mode);
      node->update_component_domains(domains);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_index_partition(IndexPartition pid,
        IndexSpace parent, ColorPoint part_color, 
        const std::map<DomainPoint,Domain> &coloring, 
        const Domain &color_space, PartitionKind part_kind, AllocateMode mode)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *parent_node = get_node(parent);
      if (!part_color.is_valid())
        part_color = ColorPoint(DomainPoint::from_point<1>(
                              Arrays::Point<1>(parent_node->generate_color())));
      IndexPartNode *new_part;
      UserEvent disjointness_event = UserEvent::NO_USER_EVENT;
      if (part_kind == COMPUTE_KIND)
      {
        disjointness_event = UserEvent::create_user_event();
        new_part = create_node(pid, parent_node, part_color, color_space,
                               disjointness_event, mode);
      }
      else
        new_part = create_node(pid, parent_node, part_color, color_space, 
                               (part_kind == DISJOINT_KIND), mode);
#ifdef LEGION_SPY
      bool disjoint = (part_kind == DISJOINT_KIND);
      LegionSpy::log_index_partition(parent.id, pid.id, disjoint,
          part_color.get_point());
#endif
      // Now do all the child nodes
      for (std::map<DomainPoint,Domain>::const_iterator it = 
            coloring.begin(); it != coloring.end(); it++)
      {
        if (!color_space.contains(it->first))
        {
          log_index.error("Invalid child color specified "
                                "for create index partition.  All colors "
                                "must be contained within the "
                                "given color space");
#ifdef DEBUG_HIGH_LEVEL
          assert(false);
#endif
          exit(ERROR_INVALID_PARTITION_COLOR);
        }
        IndexSpace handle(runtime->get_unique_index_space_id(),
                          pid.get_tree_id());
        create_node(handle, it->second, new_part, ColorPoint(it->first),
                    parent_node->kind, mode);
#ifdef LEGION_SPY
        LegionSpy::log_index_subspace(pid.id, handle.id, it->first);
#endif
      } 
      if (part_kind == COMPUTE_KIND)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(disjointness_event.exists());
#endif
        // Launch a task to compute the disjointness
        DisjointnessArgs args;
        args.hlr_id = HLR_DISJOINTNESS_TASK_ID;
        args.handle = pid;
        args.ready = disjointness_event;
        runtime->issue_runtime_meta_task(&args, sizeof(args),
                                         HLR_DISJOINTNESS_TASK_ID);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_index_partition(IndexPartition pid,
       IndexSpace parent, ColorPoint part_color, 
       const std::map<DomainPoint,Domain> &convex_hulls,
       const std::map<DomainPoint,std::set<Domain> > &component_domains,
       const Domain &color_space, PartitionKind part_kind, AllocateMode mode)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *parent_node = get_node(parent);
      if (!part_color.is_valid())
        part_color = ColorPoint(DomainPoint::from_point<1>(
                              Arrays::Point<1>(parent_node->generate_color())));
      IndexPartNode *new_part;
      UserEvent disjointness_event = UserEvent::NO_USER_EVENT;
      if (part_kind == COMPUTE_KIND)
      {
        disjointness_event = UserEvent::create_user_event();
        new_part = create_node(pid, parent_node, part_color, color_space, 
                               disjointness_event, mode);
      }
      else
        new_part = create_node(pid, parent_node, part_color, color_space, 
                                            (part_kind == DISJOINT_KIND), mode);
#ifdef LEGION_SPY
      bool disjoint = (part_kind == DISJOINT_KIND);
      LegionSpy::log_index_partition(parent.id, pid.id, disjoint,
          part_color.get_point());
#endif
      // Now do all the child nodes
      std::map<DomainPoint,std::set<Domain> >::const_iterator comp_it = 
        component_domains.begin();
      for (std::map<DomainPoint,Domain>::const_iterator it = 
            convex_hulls.begin(); it != convex_hulls.end(); it++, comp_it++)
      {
        if (!color_space.contains(it->first))
        {
          log_index.error("Invalid child color specified "
                                "for create index partition.  All colors "
                                "must be contained within the given"
                                "color space");
#ifdef DEBUG_HIGH_LEVEL
          assert(false);
#endif
          exit(ERROR_INVALID_PARTITION_COLOR);
        }
        IndexSpace handle(runtime->get_unique_index_space_id(),
                          pid.get_tree_id());
        IndexSpaceNode *child = create_node(handle, it->second, 
                                            new_part, ColorPoint(it->first),
                                            parent_node->kind, mode);
        child->update_component_domains(comp_it->second);
#ifdef LEGION_SPY
        LegionSpy::log_index_subspace(pid.id, handle.id, it->first);
#endif
      }
      if (part_kind == COMPUTE_KIND)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(disjointness_event.exists());
#endif
        // Launch a task to compute the disjointness
        DisjointnessArgs args;
        args.hlr_id = HLR_DISJOINTNESS_TASK_ID;
        args.handle = pid;
        args.ready = disjointness_event;
        runtime->issue_runtime_meta_task(&args, sizeof(args),
                                         HLR_DISJOINTNESS_TASK_ID);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::compute_partition_disjointness(IndexPartition handle,
                                                          UserEvent ready_event)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = get_node(handle);
      node->compute_disjointness(ready_event);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::destroy_index_space(IndexSpace handle,
                                               AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      node->destroy_node(source);
      // Return true if this is a top level node
      return (node->parent == NULL);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::destroy_index_partition(IndexPartition handle,
                                                   AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = get_node(handle);
      node->destroy_node(source);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_equal_partition(IndexPartition pid,
                                                   size_t granularity)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *new_part = get_node(pid);
      return new_part->create_equal_children(granularity);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_weighted_partition(IndexPartition pid,
                                                      size_t granularity,
                                       const std::map<DomainPoint,int> &weights)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *new_part = get_node(pid);
      return new_part->create_weighted_children(weights, granularity);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_partition_by_union(IndexPartition pid,
                                                      IndexPartition handle1,
                                                      IndexPartition handle2)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *new_part = get_node(pid);
      IndexPartNode *node1 = get_node(handle1);
      IndexPartNode *node2 = get_node(handle2);
      return new_part->create_by_operation(node1, node2,
                                           LowLevel::IndexSpace::ISO_UNION);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_partition_by_intersection(IndexPartition pid,
                                                         IndexPartition handle1,
                                                         IndexPartition handle2)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *new_part = get_node(pid);
      IndexPartNode *node1 = get_node(handle1);
      IndexPartNode *node2 = get_node(handle2);
      return new_part->create_by_operation(node1, node2,
                                           LowLevel::IndexSpace::ISO_INTERSECT);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_partition_by_difference(IndexPartition pid,
                                                       IndexPartition handle1,
                                                       IndexPartition handle2)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *new_part = get_node(pid);
      IndexPartNode *node1 = get_node(handle1);
      IndexPartNode *node2 = get_node(handle2);
      return new_part->create_by_operation(node1, node2,
                                           LowLevel::IndexSpace::ISO_SUBTRACT);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_cross_product_partitions(IndexPartition base,
                                                          IndexPartition source,
                                  std::map<DomainPoint,IndexPartition> &handles)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *base_node = get_node(base);
      IndexPartNode *source_node = get_node(source);
      std::set<Event> ready_events;
      // Iterate over all our sub-regions and fill in the intersections
      for (std::map<DomainPoint,IndexPartition>::const_iterator it = 
            handles.begin(); it != handles.end(); it++)
      {
        ColorPoint child_color(it->first);
        IndexSpaceNode *child_node = base_node->get_child(child_color);
        IndexPartNode *part_node = get_node(it->second);
        Event ready = part_node->create_by_operation(child_node, source_node,
                                        LowLevel::IndexSpace::ISO_INTERSECT);
        ready_events.insert(ready);
      }
      return Event::merge_events(ready_events);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::compute_pending_color_space(IndexSpace parent,
                                                       IndexPartition handle1,
                                                       IndexPartition handle2,
                                                       Domain &color_space,
                                   LowLevel::IndexSpace::IndexSpaceOperation op)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      std::vector<ColorPoint> path;
      switch (op)
      {
        case LowLevel::IndexSpace::ISO_UNION:
          {
            // Check that parent is an ancestor of both partitions
            if (!compute_partition_path(parent, handle1, path))
            {
              log_index.error("Index space %d is not an ancestor of "
                                    "index partition %d in create partition "
                                    "by union call!", parent.id, handle1.id);
              assert(false);
              exit(ERROR_INDEX_PARTITION_ANCESTOR);
            }
            path.clear();
            if (!compute_partition_path(parent, handle2, path))
            {
              log_index.error("Index space %d is not an ancestor of "
                                    "index partition %d in create partition "
                                    "by union call!", parent.id, handle1.id);
              assert(false);
              exit(ERROR_INDEX_PARTITION_ANCESTOR);
            }
            break;
          }
        case LowLevel::IndexSpace::ISO_INTERSECT:
          {
            // Check that parent is an ancestor of one of the partitions
            if (!compute_partition_path(parent, handle1, path))
            {
              path.clear();
              if (!compute_partition_path(parent, handle2, path))
              {
                log_index.error("Index space %d is not an ancestor of "
                                      "either index partition %d or index "
                                      "partition %d in create partition by "
                                      "intersection call!", 
                                      parent.id, handle1.id, handle2.id);
                assert(false);
                exit(ERROR_INDEX_PARTITION_ANCESTOR);
              }
            }
            break;
          }
        case LowLevel::IndexSpace::ISO_SUBTRACT:
          {
            // Check that the parent is an ancestor of the first index partition
            if (!compute_partition_path(parent, handle1, path))
            {
              log_index.error("Index space %d is not an ancestor of "
                                    "index partition %d in create partition "
                                    "by difference call!", 
                                    parent.id, handle1.id);
              assert(false);
              exit(ERROR_INDEX_PARTITION_ANCESTOR);
            }
            break;
          }
        default:
          assert(false); // should never get here
      }
#endif
      IndexPartNode *node1 = get_node(handle1);
      IndexPartNode *node2 = get_node(handle2);
      // Compute the color space
      IndexTreeNode::compute_intersection(node1->color_space, 
                                          node2->color_space,
                                          color_space, true/*compute*/);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_pending_partition(IndexPartition pid,
                                                    IndexSpace parent,
                                                    const Domain &color_space,
                                                    ColorPoint partition_color,
                                                    PartitionKind part_kind,
                                                    bool allocable,
                                                    Event handle_ready,
                                                    Event domain_ready,
                                                    bool create_separate)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *parent_node = get_node(parent);
      if (!partition_color.is_valid())
        partition_color = ColorPoint(DomainPoint::from_point<1>(
                              Arrays::Point<1>(parent_node->generate_color()))); 
      UserEvent disjointness_event = UserEvent::NO_USER_EVENT;
      IndexPartNode *partition_node;
      if (part_kind == COMPUTE_KIND)
      {
        disjointness_event = UserEvent::create_user_event();
        partition_node = create_node(pid, parent_node, partition_color,
                                     color_space, disjointness_event,
                                     allocable ? MUTABLE : NO_MEMORY);
      }
      else
        partition_node = create_node(pid, parent_node, partition_color,
                                     color_space, (part_kind == DISJOINT_KIND),
                                     allocable ? MUTABLE : NO_MEMORY);
#ifdef LEGION_SPY
      bool disjoint = (part_kind == DISJOINT_KIND);
      LegionSpy::log_index_partition(parent.id, pid.id, disjoint,
          partition_color.get_point());
#endif
      // We also need to explicitly instantiate all the children so
      // that they know the domains will be ready at a later time.
      // We instantiate them with an empty domain that will be filled in later
      for (Domain::DomainPointIterator itr(color_space); itr; itr++)
      {
        IndexSpace is(runtime->get_unique_index_space_id(), pid.get_tree_id());
        ColorPoint child_color(itr.p);
        if (create_separate)
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(!handle_ready.exists());
          assert(!domain_ready.exists());
#endif
          // Create a separate handle ready event for each node
          UserEvent local_handle_ready = UserEvent::create_user_event();
          UserEvent local_domain_ready = UserEvent::create_user_event();
          create_node(is, local_handle_ready, local_domain_ready,
                      partition_node, child_color, parent_node->kind, 
                      allocable ? MUTABLE : NO_MEMORY);
          partition_node->add_pending_child(child_color, local_handle_ready,
                                            local_domain_ready);
        }
        else
          create_node(is, handle_ready, domain_ready,
                      partition_node, child_color, parent_node->kind, 
                      allocable ? MUTABLE : NO_MEMORY);
#ifdef LEGION_SPY
        LegionSpy::log_index_subspace(pid.id, is.id, itr.p);
#endif
      }
      // If we need to compute the disjointness, only do that
      // after the partition is actually ready
      if (part_kind == COMPUTE_KIND)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(disjointness_event.exists());
#endif
        // Launch a task to compute the disjointness
        DisjointnessArgs args;
        args.hlr_id = HLR_DISJOINTNESS_TASK_ID;
        args.handle = pid;
        args.ready = disjointness_event;
        runtime->issue_runtime_meta_task(&args, sizeof(args),
                                         HLR_DISJOINTNESS_TASK_ID, NULL,
                                         domain_ready);
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(domain_ready, disjointness_event);
#endif
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_pending_cross_product(IndexPartition handle1,
                                                        IndexPartition handle2,
                            std::map<DomainPoint,IndexPartition> &our_handles,
                            std::map<DomainPoint,IndexPartition> &user_handles,
                                                        PartitionKind kind,
                                                        ColorPoint &part_color,
                                                        bool allocable,
                                                        Event handle_ready,
                                                        Event domain_ready)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *base = get_node(handle1);
      IndexPartNode *source = get_node(handle2);
      // Iterate over all our sub-regions and generate partitions
      for (Domain::DomainPointIterator itr(base->color_space); itr; itr++)
      {
        ColorPoint child_color(itr.p);
        IndexSpaceNode *child_node = base->get_child(child_color); 
        ColorPoint partition_color = part_color;
        if (!partition_color.is_valid())
          partition_color = ColorPoint(DomainPoint::from_point<1>(
                              Arrays::Point<1>(child_node->generate_color())));
        IndexPartition pid(runtime->get_unique_index_partition_id(),
                           handle1.get_tree_id());
        create_pending_partition(pid, child_node->handle,
                                 source->color_space, partition_color,
                                 kind, allocable, handle_ready, domain_ready);
        // Save the handles for ourselves 
        our_handles[itr.p] = pid;
        // If the user requested the handle for this point return it
        std::map<DomainPoint,IndexPartition>::iterator finder = 
          user_handles.find(itr.p);
        if (finder != user_handles.end())
          finder->second = pid;
      }
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_partition_by_field(RegionTreeContext ctx,
                                                  Processor proc,
                                                  const RegionRequirement &req,
                                                  IndexPartition pending,
                                                  const Domain &color_space,
                                                  Event term_event,
                                                  VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == SINGULAR);
      assert(req.privilege_fields.size() == 1);
#endif
      IndexPartNode *pending_node = get_node(pending);
      IndexSpaceNode *parent_node = pending_node->parent;
      RegionNode *top_node = get_node(req.region);
      FieldSpaceNode *field_space = top_node->get_column_source();
      // Get the index for the field
      unsigned fid_idx = 
        field_space->get_field_index(*(req.privilege_fields.begin()));
      // Traverse the target node and get all the field data descriptors
      std::set<Event> preconditions;
      std::vector<FieldDataDescriptor> field_data;
      {
        FieldMask user_mask;
        user_mask.set_bit(fid_idx);
        RegionUsage usage(req);
        top_node->find_field_descriptors(ctx.get_id(), term_event, usage,
                                       user_mask, fid_idx, proc,
                                       field_data, preconditions, version_info);
      }
      // Enumerate the color space so we can get back a different index
      // for each color in the color space
      std::map<DomainPoint,LowLevel::IndexSpace> subspaces;
      for (Domain::DomainPointIterator itr(color_space); itr; itr++)
      {
        subspaces[itr.p] = LowLevel::IndexSpace::NO_SPACE;
      }
      // Merge preconditions for all the field data descriptors
      Event precondition = Event::merge_events(preconditions);
      // Ask the parent node to make all the subspaces
      Event result = parent_node->create_subspaces_by_field(field_data,
                subspaces, ((pending_node->mode & MUTABLE) != 0), precondition);
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, result);
#endif
      // Now update the domains for all the sub-regions
      for (Domain::DomainPointIterator itr(color_space); itr; itr++)
      {
        IndexSpaceNode *child_node = pending_node->get_child(ColorPoint(itr.p));
        child_node->set_domain(subspaces[itr.p]);
      }
      return result;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_partition_by_image(RegionTreeContext ctx,
                                                  Processor proc,
                                                  const RegionRequirement &req,
                                                  IndexPartition pending,
                                                  const Domain &color_space,
                                                  Event term_event,
                                                  VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == PART_PROJECTION);
      assert(req.privilege_fields.size() == 1);
#endif
      IndexPartNode *pending_node = get_node(pending);
      IndexSpaceNode *parent_node = pending_node->parent;
      PartitionNode *projection_node = get_node(req.partition);
      FieldSpaceNode *field_space = projection_node->get_column_source();
      // Get the index for the field
      unsigned fid_idx = 
        field_space->get_field_index(*(req.privilege_fields.begin()));
      // Traverse the target node and get all the field data descriptors
      // Get all the index spaces from the color space in the projection
      std::set<Event> preconditions;
      std::vector<FieldDataDescriptor> field_data;
      std::map<LowLevel::IndexSpace,LowLevel::IndexSpace> subspaces;
      for (Domain::DomainPointIterator itr(color_space); itr; itr++)
      {
        FieldMask user_mask;
        user_mask.set_bit(fid_idx);
        ColorPoint child_color(itr.p);
        // Open up the child on the partition node
        projection_node->open_physical_child(ctx.get_id(), child_color, 
                                             user_mask, version_info);
        RegionNode *child_node = projection_node->get_child(child_color);
        // Get the field data on this child node
        RegionUsage usage(req);
        child_node->find_field_descriptors(ctx.get_id(), term_event, usage,
                                            user_mask, fid_idx, proc,
                                       field_data, preconditions, version_info);
        Event child_pre;
        const Domain &child_dom = 
                        child_node->row_source->get_domain(child_pre);
        if (child_pre.exists())
          preconditions.insert(child_pre);
        subspaces[child_dom.get_index_space()] = LowLevel::IndexSpace::NO_SPACE;
      }
      // Merge the preconditions for all the field descriptors
      Event precondition = Event::merge_events(preconditions);
      // Ask the parent node to make all the subspaces
      Event result = parent_node->create_subspaces_by_image(field_data,
                subspaces, ((pending_node->mode & MUTABLE) != 0), precondition);
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, result);
#endif
      // Now update the domains for all the sub-regions
      for (Domain::DomainPointIterator itr(color_space); itr; itr++)
      {
        ColorPoint child_color(itr.p);
        RegionNode     *orig_child = projection_node->get_child(child_color);
        IndexSpaceNode *next_child = pending_node->get_child(child_color);
        const Domain &orig_dom = orig_child->get_domain_no_wait();
        next_child->set_domain(subspaces[orig_dom.get_index_space()]);
      }
      return result;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::create_partition_by_preimage(RegionTreeContext ctx,
                                                  Processor proc,
                                                  const RegionRequirement &req,
                                                  IndexPartition projection,
                                                  IndexPartition pending,
                                                  const Domain &color_space,
                                                  Event term_event,
                                                  VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == SINGULAR);
      assert(req.privilege_fields.size() == 1);
#endif
      IndexPartNode *pending_node = get_node(pending);
      IndexSpaceNode *parent_node = pending_node->parent;
      IndexPartNode *projection_node = get_node(projection);
      RegionNode *top_node = get_node(req.region);
      FieldSpaceNode *field_space = top_node->get_column_source();
      // Get the index for the field
      unsigned fid_idx = 
        field_space->get_field_index(*(req.privilege_fields.begin()));
      // Traverse the target node and get all the field data structures
      std::set<Event> preconditions;
      std::vector<FieldDataDescriptor> field_data;
      {
        FieldMask user_mask;
        user_mask.set_bit(fid_idx);
        RegionUsage usage(req);
        top_node->find_field_descriptors(ctx.get_id(), term_event, usage,
                                         user_mask, fid_idx, proc,
                                       field_data, preconditions, version_info);
      }
      // Get all the index spaces from the color space in the projection
      std::map<LowLevel::IndexSpace,LowLevel::IndexSpace> subspaces;
      for (Domain::DomainPointIterator itr(color_space); itr; itr++)
      {
        IndexSpaceNode *child_node = 
          projection_node->get_child(ColorPoint(itr.p));
        Event child_pre;
        const Domain &child_dom = child_node->get_domain(child_pre);
        if (child_pre.exists())
          preconditions.insert(child_pre);
        subspaces[child_dom.get_index_space()] = LowLevel::IndexSpace::NO_SPACE;
      }
      // Merge the preconditions for all the field descriptors
      Event precondition = Event::merge_events(preconditions);
      // Ask the parent node to make all the subspaces
      Event result = parent_node->create_subspaces_by_preimage(field_data,
                subspaces, ((pending_node->mode & MUTABLE) != 0), precondition);
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, result);
#endif
      // Now update the domains for all the sub-regions
      for (Domain::DomainPointIterator itr(color_space); itr; itr++)
      {
        ColorPoint child_color(itr.p);
        IndexSpaceNode *orig_child = projection_node->get_child(child_color);
        IndexSpaceNode *next_child = pending_node->get_child(child_color);
        const Domain &orig_dom = orig_child->get_domain_no_wait();
        next_child->set_domain(subspaces[orig_dom.get_index_space()]);
      }
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpace RegionTreeForest::find_pending_space(IndexPartition parent,
                                                    const DomainPoint &color,
                                                    UserEvent &handle_ready,
                                                    UserEvent &domain_ready)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *parent_node = get_node(parent);
      ColorPoint child_color(color);
      // First get the child node   
      if (!parent_node->has_child(child_color))
      {
        log_run.error("Invalid color in compute pending space!");
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_PARTITION_COLOR);
      }
      IndexSpaceNode *child_node = parent_node->get_child(child_color);
      if (!parent_node->get_pending_child(child_color, 
                                          handle_ready, domain_ready))
      {
        log_run.error("Invalid pending child!");
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_PENDING_CHILD);
      }
      return child_node->handle;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::compute_pending_space(IndexSpace target,
                                         const std::vector<IndexSpace> &handles,
                                                                  bool is_union)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *child_node = get_node(target);
      IndexPartNode *parent_node = child_node->parent;
      // Compute the new index space 
      std::set<Event> preconditions;
      std::vector<LowLevel::IndexSpace> spaces(handles.size());
      unsigned idx = 0;
      for (std::vector<IndexSpace>::const_iterator it = handles.begin();
            it != handles.end(); it++, idx++)
      {
        IndexSpaceNode *node = get_node(*it); 
        Event precondition;
        const Domain &dom = node->get_domain(precondition);
        spaces[idx] = dom.get_index_space();
        if (precondition.exists())
          preconditions.insert(precondition);
      }
      Event parent_precondition;
      const Domain &parent_dom = 
              parent_node->parent->get_domain(parent_precondition);
      if (parent_precondition.exists())
        preconditions.insert(parent_precondition);
      // Now we can compute the low-level index space
      Event precondition = Event::merge_events(preconditions);
      LowLevel::IndexSpace result;
      Event ready = LowLevel::IndexSpace::reduce_index_spaces(
          is_union ? LowLevel::IndexSpace::ISO_UNION : 
                     LowLevel::IndexSpace::ISO_INTERSECT, spaces, result, 
          ((parent_node->mode & MUTABLE) != 0)/* allocable */,
          parent_dom.get_index_space(), precondition);
      // Now set the result and trigger the handle ready event
      child_node->set_domain(Domain(result));
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, ready);
#endif
      return ready;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::compute_pending_space(IndexSpace target,
                                                  IndexPartition handle,
                                                  bool is_union)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *child_node = get_node(target);
      IndexPartNode *parent_node = child_node->parent;
      IndexPartNode *reduce_node = get_node(handle);
      std::set<Event> preconditions;
      std::vector<LowLevel::IndexSpace> 
        spaces(reduce_node->color_space.get_volume());
      unsigned idx = 0;
      for (Domain::DomainPointIterator itr(reduce_node->color_space); 
            itr; itr++, idx++)
      {
        ColorPoint node_color(itr.p);
        IndexSpaceNode *node = reduce_node->get_child(node_color);
        Event precondition;
        const Domain &dom = node->get_domain(precondition);
        spaces[idx] = dom.get_index_space();
        if (precondition.exists())
          preconditions.insert(precondition);
      }
      Event parent_precondition;
      const Domain &parent_dom = 
            parent_node->parent->get_domain(parent_precondition);
      if (parent_precondition.exists())
        preconditions.insert(parent_precondition);
      // Now we can compute the low-level index space
      Event precondition = Event::merge_events(preconditions);
      LowLevel::IndexSpace result;
      Event ready = LowLevel::IndexSpace::reduce_index_spaces(
          is_union ? LowLevel::IndexSpace::ISO_UNION : 
                     LowLevel::IndexSpace::ISO_INTERSECT, spaces, result,
          ((parent_node->mode & MUTABLE) != 0)/* allocable */,
          parent_dom.get_index_space(), precondition);
      // Now set the result and trigger the handle ready event
      child_node->set_domain(Domain(result));
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, ready);
#endif
      return ready;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::compute_pending_space(IndexSpace target,
                                                  IndexSpace initial,
                                         const std::vector<IndexSpace> &handles)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *child_node = get_node(target);
      IndexPartNode *parent_node = child_node->parent;
      std::set<Event> preconditions;
      std::vector<LowLevel::IndexSpace> spaces(handles.size()+1);
      IndexSpaceNode *init_node = get_node(initial);
      Event init_precondition;
      const Domain &init_dom = init_node->get_domain(init_precondition);
      spaces[0] = init_dom.get_index_space();
      if (init_precondition.exists())
        preconditions.insert(init_precondition);
      unsigned idx = 1;
      for (std::vector<IndexSpace>::const_iterator it = handles.begin();
            it != handles.end(); it++, idx++)
      {
        IndexSpaceNode *node = get_node(*it);  
        Event precondition;
        const Domain &dom = node->get_domain(precondition);
        spaces[idx] = dom.get_index_space();
        if (precondition.exists())
          preconditions.insert(precondition);
      }
      Event parent_precondition;
      const Domain &parent_dom = 
              parent_node->parent->get_domain(parent_precondition);
      if (parent_precondition.exists())
        preconditions.insert(parent_precondition);
      // Now we can compute the low-level index space
      Event precondition = Event::merge_events(preconditions);
      LowLevel::IndexSpace result;
      Event ready = LowLevel::IndexSpace::reduce_index_spaces(
                             LowLevel::IndexSpace::ISO_SUBTRACT, spaces, result,
          ((parent_node->mode & MUTABLE) != 0)/* allocable */,
          parent_dom.get_index_space(), precondition);
      // Now set the result and trigger the handle ready event
      child_node->set_domain(Domain(result));
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, ready);
#endif
      return ready;
    }

    //--------------------------------------------------------------------------
    IndexPartition RegionTreeForest::get_index_partition(IndexSpace parent,
                                                       const ColorPoint &color)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *parent_node = get_node(parent);
      IndexPartNode *child_node = parent_node->get_child(color);
      return child_node->handle;
    }

    //--------------------------------------------------------------------------
    IndexSpace RegionTreeForest::get_index_subspace(IndexPartition parent,
                                                    const ColorPoint &color)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *parent_node = get_node(parent);
      IndexSpaceNode *child_node = parent_node->get_child(color);
      return child_node->handle;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_multiple_domains(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      return node->has_component_domains();
    }

    //--------------------------------------------------------------------------
    Domain RegionTreeForest::get_index_space_domain(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      return node->get_domain_blocking();
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::get_index_space_domains(IndexSpace handle,
                                                   std::vector<Domain> &domains)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      node->get_domains_blocking(domains); 
    }

    //--------------------------------------------------------------------------
    Domain RegionTreeForest::get_index_partition_color_space(IndexPartition p)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = get_node(p);
      return node->color_space;
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::get_index_space_partition_colors(IndexSpace sp,
                                                   std::set<ColorPoint> &colors)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(sp);
      node->get_colors(colors); 
    }

    //--------------------------------------------------------------------------
    ColorPoint RegionTreeForest::get_index_space_color(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      return node->color;
    }

    //--------------------------------------------------------------------------
    ColorPoint RegionTreeForest::get_index_partition_color(
                                                          IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = get_node(handle);
      return node->color;
    }

    //--------------------------------------------------------------------------
    IndexSpace RegionTreeForest::get_parent_index_space(IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = get_node(handle);
      return node->parent->handle;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_parent_index_partition(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      return (node->parent != NULL);
    }

    //--------------------------------------------------------------------------
    IndexPartition RegionTreeForest::get_parent_index_partition(
                                                              IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      if (node->parent == NULL)
      {
        log_run.error("Parent index partition requested for "
                            "index space %x with no parent. Use "
                            "has_parent_index_partition to check "
                            "before requesting a parent.", handle.id);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_PARENT_REQUEST);
      }
      return node->parent->handle;
    }

    //--------------------------------------------------------------------------
    IndexSpaceAllocator* RegionTreeForest::get_index_space_allocator(
                                                              IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      return node->get_allocator();
    }

    //--------------------------------------------------------------------------
    size_t RegionTreeForest::get_domain_volume(IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *node = get_node(handle);
      return node->get_domain_volume(true/*app query*/); 
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::is_index_partition_disjoint(IndexPartition p)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = get_node(p);
      return node->is_disjoint(true/*app query*/);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_field_space(FieldSpace handle)
    //--------------------------------------------------------------------------
    {
      create_node(handle, Event::NO_EVENT);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::destroy_field_space(FieldSpace handle,
                                               AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      node->destroy_node(source);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::allocate_field(FieldSpace handle, size_t field_size,
                                          FieldID fid, bool local)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      if (local && node->has_field(fid))
        return true;
      node->allocate_field(fid, field_size, local);
      return false;
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::free_field(FieldSpace handle, FieldID fid, 
                                      AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      node->free_field(fid, source);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::allocate_fields(FieldSpace handle, 
                                           const std::vector<size_t> &sizes,
                                           const std::vector<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(sizes.size() == fields.size());
#endif
      // We know that none of these field allocations are local
      FieldSpaceNode *node = get_node(handle);
      for (unsigned idx = 0; idx < fields.size(); idx++)
      {
        node->allocate_field(fields[idx], sizes[idx], false/*local*/);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::free_fields(FieldSpace handle,
                                       const std::set<FieldID> &to_free,
                                       AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      for (std::set<FieldID>::const_iterator it = to_free.begin();
            it != to_free.end(); it++)
      {
        node->free_field(*it, source);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::allocate_field_index(FieldSpace handle, 
                                                size_t field_size, FieldID fid,
                                                unsigned index, 
                                                AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      node->allocate_field_index(fid, field_size, source, index);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::allocate_field_indexes(FieldSpace handle,
                                        const std::vector<FieldID> &fields,
                                        const std::vector<size_t> &sizes,
                                        const std::vector<unsigned> &indexes,
                                        AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(fields.size() == sizes.size());
      assert(fields.size() == indexes.size());
#endif
      FieldSpaceNode *node = get_node(handle);
      for (unsigned idx = 0; idx < fields.size(); idx++)
      {
        unsigned index = indexes[idx];
        node->allocate_field_index(fields[idx], sizes[idx], source, index);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::invalidate_field_index(
                       const std::set<RegionNode*> &regions, unsigned field_idx)
    //--------------------------------------------------------------------------
    {
      FieldInvalidator invalidator(field_idx);
      for (std::set<RegionNode*>::const_iterator it = regions.begin();
            it != regions.end(); it++)
      {
        (*it)->visit_node(&invalidator);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::get_all_fields(FieldSpace handle, 
                                          std::set<FieldID> &to_set)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      node->get_all_fields(to_set);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::get_all_regions(FieldSpace handle,
                                           std::set<LogicalRegion> &regions)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      node->get_all_regions(regions);
    }

    //--------------------------------------------------------------------------
    size_t RegionTreeForest::get_field_size(FieldSpace handle, FieldID fid)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *node = get_node(handle);
      if (!node->has_field(fid))
      {
        log_run.error("FieldSpace %x has no field %d", handle.id, fid);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_FIELD_ID);
      }
      return node->get_field_size(fid);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::create_logical_region(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      create_node(handle, NULL/*parent*/);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::destroy_logical_region(LogicalRegion handle,
                                                  AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      RegionNode *node = get_node(handle);
      node->destroy_node(source);
      // Return true if this was a top-level region
      return (node->parent == NULL);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::destroy_logical_partition(LogicalPartition handle,
                                                     AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      PartitionNode *node = get_node(handle);
      node->destroy_node(source);
    }

    //--------------------------------------------------------------------------
    LogicalPartition RegionTreeForest::get_logical_partition(
                                    LogicalRegion parent, IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      // No lock needed for this one
      return LogicalPartition(parent.tree_id, handle, parent.field_space);
    }

    //--------------------------------------------------------------------------
    LogicalPartition RegionTreeForest::get_logical_partition_by_color(
                                     LogicalRegion parent, const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      RegionNode *parent_node = get_node(parent);
      IndexPartNode *index_node = parent_node->row_source->get_child(c);
      LogicalPartition result(parent.tree_id, index_node->handle, 
                              parent.field_space);
      return result;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_logical_partition_by_color(LogicalRegion parent,
                                                        const ColorPoint &color)
    //--------------------------------------------------------------------------
    {
      RegionNode *parent_node = get_node(parent);
      return parent_node->has_color(color);
    }

    //--------------------------------------------------------------------------
    LogicalPartition RegionTreeForest::get_logical_partition_by_tree(
                      IndexPartition handle, FieldSpace space, RegionTreeID tid)
    //--------------------------------------------------------------------------
    {
      // No lock needed for this one
      return LogicalPartition(tid, handle, space);
    }

    //--------------------------------------------------------------------------
    LogicalRegion RegionTreeForest::get_logical_subregion(
                                    LogicalPartition parent, IndexSpace handle)
    //--------------------------------------------------------------------------
    {
      // No lock needed for this one
      return LogicalRegion(parent.tree_id, handle, parent.field_space);
    }
    
    //--------------------------------------------------------------------------
    LogicalRegion RegionTreeForest::get_logical_subregion_by_color(
                                  LogicalPartition parent, const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      PartitionNode *parent_node = get_node(parent);
      IndexSpaceNode *index_node = parent_node->row_source->get_child(c);
      LogicalRegion result(parent.tree_id, index_node->handle,
                           parent.field_space);
      return result;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_logical_subregion_by_color(
                               LogicalPartition parent, const ColorPoint &color)
    //--------------------------------------------------------------------------
    {
      PartitionNode *parent_node = get_node(parent);
      return parent_node->has_color(color);
    }

    //--------------------------------------------------------------------------
    LogicalRegion RegionTreeForest::get_logical_subregion_by_tree(
                          IndexSpace handle, FieldSpace space, RegionTreeID tid)
    //--------------------------------------------------------------------------
    {
      // No lock needed for this one
      return LogicalRegion(tid, handle, space);
    }

    //--------------------------------------------------------------------------
    ColorPoint RegionTreeForest::get_logical_region_color(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      RegionNode *node = get_node(handle);
      return node->row_source->color;
    }

    //--------------------------------------------------------------------------
    ColorPoint RegionTreeForest::get_logical_partition_color(
                                                        LogicalPartition handle)
    //--------------------------------------------------------------------------
    {
      PartitionNode *node = get_node(handle);
      return node->row_source->color;
    }

    //--------------------------------------------------------------------------
    LogicalRegion RegionTreeForest::get_parent_logical_region(
                                                        LogicalPartition handle)
    //--------------------------------------------------------------------------
    {
      PartitionNode *node = get_node(handle);
      return node->parent->handle;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_parent_logical_partition(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      RegionNode *node = get_node(handle);
      return (node->parent != NULL);
    }

    //--------------------------------------------------------------------------
    LogicalPartition RegionTreeForest::get_parent_logical_partition(
                                                           LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      RegionNode *node = get_node(handle);
      if (node->parent == NULL)
      {
        log_run.error("Parent logical partition requested for "
                            "logical region (%x,%x,%d) with no parent. "
                            "Use has_parent_logical_partition to check "
                            "before requesting a parent.", 
                            handle.index_space.id,
                            handle.field_space.id,
                            handle.tree_id);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_PARENT_REQUEST);
      }
      return node->parent->handle;
    }

    //--------------------------------------------------------------------------
    size_t RegionTreeForest::get_domain_volume(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      RegionNode *node = get_node(handle);
      Domain d = node->get_domain_blocking();
      if (d.get_dim() == 0)
      {
        const LowLevel::ElementMask &mask = 
          d.get_index_space().get_valid_mask();
        return mask.get_num_elmts();
      }
      else
        return d.get_volume();
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::perform_dependence_analysis(
                                                  Operation *op, unsigned idx,
                                                  RegionRequirement &req,
                                                  VersionInfo &version_info,
                                                  RestrictInfo &restrict_info,
                                                  RegionTreePath &path)
    //--------------------------------------------------------------------------
    {
      // If this is a NO_ACCESS, then we'll have no dependences so we're done
      if (IS_NO_ACCESS(req))
        return;
      SingleTask *parent_ctx = op->get_parent();
      RegionTreeContext ctx = parent_ctx->get_context();
#ifdef DEBUG_PERF
      begin_perf_trace(REGION_DEPENDENCE_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
#endif
      RegionNode *parent_node = get_node(req.parent);
      
      FieldMask user_mask = 
        parent_node->column_source->get_field_mask(req.privilege_fields);
      // Then compute the logical user
      LogicalUser user(op, idx, RegionUsage(req), user_mask); 
      // Check to see if we need to do any restricted tests
      if (parent_ctx->has_tree_restriction(req.parent.get_tree_id(),user_mask))
      {
        restrict_info.set_check();
      }
      version_info.set_upper_bound_node(parent_node);
      TraceInfo trace_info(op->already_traced(), op->get_trace(), idx, req); 
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, idx, op->get_logging_name(),
                                     op->get_unique_op_id(), parent_node,
                                     ctx.get_id(), true/*before*/, 
                                     false/*premap*/,
                                     false/*closing*/, true/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      // Finally do the traversal, note that we don't need to hold the
      // context lock since the runtime guarantees that all dependence
      // analysis for a single context are performed in order
      parent_node->register_logical_node(ctx.get_id(), user, path, version_info,
                      restrict_info, trace_info, (req.handle_type != SINGULAR));
      // Once we are done we can clear out the list of recorded dependences
      op->clear_logical_records();
      // If we have a restriction, then record it on the region requirement
      if (restrict_info.has_restrictions())
        req.restricted = true;
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, idx, op->get_logging_name(),
                                     op->get_unique_op_id(), parent_node,
                                     ctx.get_id(), false/*before*/, 
                                     false/*premap*/,
                                     false/*closing*/, true/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::perform_reduction_close_analysis(Operation *op,
                                                  unsigned idx,
                                                  RegionRequirement &req,
                                                  VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
      SingleTask *parent_ctx = op->get_parent();
      RegionTreeContext ctx = parent_ctx->get_context();
#ifdef DEBUG_PERF
      begin_perf_trace(REGION_DEPENDENCE_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
      assert(req.privilege == REDUCE);
#endif
      RegionNode *parent_node = get_node(req.parent);

      FieldMask user_mask = 
        parent_node->column_source->get_field_mask(req.privilege_fields);
      // Then compute the logical user
      LogicalUser user(op, idx, RegionUsage(req), user_mask);
      // Make the user read-only so we catch dependences on 
      // all the outstanding reductions
      user.usage.privilege = READ_ONLY;
      version_info.set_upper_bound_node(parent_node);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, idx, op->get_logging_name(),
                                     op->get_unique_op_id(), parent_node,
                                     ctx.get_id(), true/*before*/, 
                                     false/*premap*/,
                                     false/*closing*/, true/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      parent_node->close_reduction_analysis(ctx.get_id(), user, version_info); 
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, idx, op->get_logging_name(),
                                     op->get_unique_op_id(), parent_node,
                                     ctx.get_id(), false/*before*/, 
                                     false/*premap*/,
                                     false/*closing*/, true/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::perform_fence_analysis(RegionTreeContext ctx,
                                                  Operation *fence,
                                                  LogicalRegion handle,
                                                  bool dominate)
    //--------------------------------------------------------------------------
    {
      // Register dependences for this fence on all users in the tree
      RegionNode *top_node = get_node(handle);
      if (dominate)
      {
        LogicalRegistrar<true> registrar(ctx.get_id(), fence, 
                                         FieldMask(FIELD_ALL_ONES));
        top_node->visit_node(&registrar);
      }
      else
      {
        LogicalRegistrar<false> registrar(ctx.get_id(), fence, 
                                         FieldMask(FIELD_ALL_ONES));
        top_node->visit_node(&registrar);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::analyze_destroy_index_space(RegionTreeContext ctx,
                                                       IndexSpace handle,
                                                       Operation *op,
                                                       LogicalRegion region)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *delete_node = get_node(handle);
      // Because we lazily instantiate the region tree, we need to do
      // tree comparisons from the the source nodes at the top of the tree
      IndexSpaceNode *top_index = delete_node;
      while (top_index->parent != NULL)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(top_index->parent->parent != NULL);
#endif
        top_index = top_index->parent->parent;
      }
      if (top_index->has_instance(region.get_tree_id()))
      {
        RegionNode *start_node = get_node(region);
        RegionTreePath path;
        initialize_path(delete_node,start_node->row_source,path);
        LogicalPathRegistrar reg(ctx.get_id(), op, 
                                 FieldMask(FIELD_ALL_ONES), path);
        reg.traverse(start_node); 
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::analyze_destroy_index_partition(
                                                          RegionTreeContext ctx,
                                                          IndexPartition handle,
                                                          Operation *op,
                                                          LogicalRegion region)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *delete_node = get_node(handle);
      // Because we lazily instantiate the region tree, we need to do
      // tree comparisons from the the source nodes at the top of the tree
      IndexSpaceNode *top_index = delete_node->parent;
      while (top_index->parent != NULL)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(top_index->parent->parent != NULL);
#endif
        top_index = top_index->parent->parent;
      }
      if (top_index->has_instance(region.get_tree_id()))
      {
        RegionNode *start_node = get_node(region);
        RegionTreePath path;
        initialize_path(delete_node,start_node->row_source,path);
        LogicalPathRegistrar reg(ctx.get_id(), op, 
                                 FieldMask(FIELD_ALL_ONES), path);
        reg.traverse(start_node);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::analyze_destroy_field_space(RegionTreeContext ctx,
                                                       FieldSpace handle,
                                                       Operation *op,
                                                       LogicalRegion region)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *delete_node = get_node(handle);
      if (delete_node->has_instance(region.get_tree_id()))
      {
        RegionNode *start_node = get_node(region);
        LogicalRegistrar<false> registrar(ctx.get_id(), op, 
                                          FieldMask(FIELD_ALL_ONES));
        start_node->visit_node(&registrar);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::analyze_destroy_fields(RegionTreeContext ctx,
                                                  FieldSpace handle,
                                            const std::set<FieldID> &to_delete,
                                                  Operation *op,
                                                  LogicalRegion region)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *delete_node = get_node(handle);
      if (delete_node->has_instance(region.get_tree_id()))
      {
        RegionNode *start_node = get_node(region);
        LogicalRegistrar<false> registrar(ctx.get_id(), op, 
                                        delete_node->get_field_mask(to_delete));
        start_node->visit_node(&registrar);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::analyze_destroy_logical_region(RegionTreeContext ctx,
                                                          LogicalRegion handle,
                                                          Operation *op,
                                                          LogicalRegion region)
    //--------------------------------------------------------------------------
    {
      if (handle.get_tree_id() == region.get_tree_id())
      {
        RegionNode *start_node = get_node(region);
        RegionNode *delete_node = get_node(handle);
        RegionTreePath path;
        initialize_path(delete_node->row_source,start_node->row_source,path);
        LogicalPathRegistrar reg(ctx.get_id(), op, 
                                 FieldMask(FIELD_ALL_ONES), path);
        reg.traverse(start_node);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::analyze_destroy_logical_partition(
                                                        RegionTreeContext ctx,
                                                        LogicalPartition handle,
                                                        Operation *op,
                                                        LogicalRegion region)
    //--------------------------------------------------------------------------
    {
      if (handle.get_tree_id() == region.get_tree_id())
      {
        RegionNode *start_node = get_node(region);
        PartitionNode *delete_node = get_node(handle);
        RegionTreePath path;
        initialize_path(delete_node->row_source,start_node->row_source,path);
        LogicalPathRegistrar reg(ctx.get_id(), op, 
                                 FieldMask(FIELD_ALL_ONES), path);
        reg.traverse(start_node);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::initialize_logical_context(RegionTreeContext ctx,
                                                      LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      // For now we don't need to do anything here assuming we 
      // always clean up after ourselves by calling invalidate
      // logical context after we are done with a context
      // In debug mode we'll do a check just to make sure this is true
#ifdef DEBUG_HIGH_LEVEL
      RegionNode *top_node = get_node(handle); 
      LogicalInitializer init(ctx.get_id());
      top_node->visit_node(&init);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::invalidate_logical_context(RegionTreeContext ctx,
                                                      LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      RegionNode *top_node = get_node(handle);
      LogicalInvalidator invalidator(ctx.get_id());
      top_node->visit_node(&invalidator);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::restrict_user_coherence(SingleTask *parent_ctx,
                                                   LogicalRegion handle,
                                                const std::set<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
      RegionTreeContext ctx = parent_ctx->get_context();
      RegionNode *node = get_node(handle);
      FieldMask restrict_mask = node->column_source->get_field_mask(fields);
      RestrictionMutator<true/*restrict*/> mutator(ctx.get_id(),restrict_mask);
      node->visit_node(&mutator);
      // Tell the parent task about the restriction on this region tree
      parent_ctx->add_tree_restriction(handle.get_tree_id(), restrict_mask);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::acquire_user_coherence(SingleTask *parent_ctx,
                                                  LogicalRegion handle,
                                                const std::set<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
      RegionTreeContext ctx = parent_ctx->get_context();
      RegionNode *node = get_node(handle);
      FieldMask restrict_mask = node->column_source->get_field_mask(fields);
      RestrictionMutator<false/*restrict*/> mutator(ctx.get_id(),restrict_mask);
      node->visit_node(&mutator);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_restrictions(LogicalRegion handle, 
                                            const RestrictInfo &info,
                                            const std::set<FieldID> &fields)
    //--------------------------------------------------------------------------
    {
      RegionNode *node = get_node(handle);
      return info.has_restrictions(handle, node, fields);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::premap_physical_region(RegionTreeContext ctx,
                                                  RegionTreePath &path,
                                                  RegionRequirement &req,
                                                  VersionInfo &version_info,
                                                  Operation *op,
                                                  SingleTask *parent_ctx,
                                                  Processor local_proc
#ifdef DEBUG_HIGH_LEVEL
                                                  , unsigned index
                                                  , const char *log_name
                                                  , UniqueID uid
#endif
                                                  )
    //--------------------------------------------------------------------------
    {
      // If we are a NO_ACCESS then we are already done
      if (IS_NO_ACCESS(req))
        return true;
#ifdef DEBUG_PERF
      begin_perf_trace(PREMAP_PHYSICAL_REGION_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
#endif
      RegionNode *parent_node = get_node(req.parent);
      // Don't need to initialize the path since that was done
      // in the logical traversal.
      // Construct a premap traversal object
      FieldMask user_mask = 
        parent_node->column_source->get_field_mask(req.privilege_fields);
      MappableInfo info(ctx.get_id(), op, local_proc, 
                        req, version_info, user_mask); 
      PremapTraverser traverser(path, info);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     parent_node, ctx.get_id(), 
                                     true/*before*/, true/*premap*/, 
                                     false/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      const bool result = traverser.traverse(parent_node);
#ifdef DEBUG_HIGH_LEVEL
      if (result)
      {
        TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                       parent_node, ctx.get_id(), 
                                       false/*before*/, true/*premap*/, 
                                       false/*closing*/, false/*logical*/,
                                       FieldMask(FIELD_ALL_ONES), user_mask);
      }
#endif
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    MappingRef RegionTreeForest::map_physical_region(RegionTreeContext ctx,
                                                     RegionTreePath &path,
                                                     RegionRequirement &req,
                                                     unsigned index,
                                                     VersionInfo &version_info,
                                                     Operation *op,
                                                     Processor local_proc,
                                                     Processor target_proc
#ifdef DEBUG_HIGH_LEVEL
                                                     , const char *log_name
                                                     , UniqueID uid
#endif
                                                     )
    //--------------------------------------------------------------------------
    {
      if (IS_NO_ACCESS(req))
        return MappingRef();
#ifdef DEBUG_PERF
      begin_perf_trace(MAP_PHYSICAL_REGION_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
      assert(req.handle_type == SINGULAR);
#endif
      RegionNode *child_node = get_node(req.region);
      FieldMask user_mask = 
        child_node->column_source->get_field_mask(req.privilege_fields);
      // Construct the mappable info
      MappableInfo info(ctx.get_id(), op, local_proc, 
                        req, version_info, user_mask);
      // Get the start node
      RegionTreeNode *start_node = child_node;
      for (unsigned idx = 0; idx < (path.get_path_length()-1); idx++)
        start_node = start_node->get_parent();
      // Construct the traverser
      MappingTraverser<false/*restrict*/> traverser(path, info, 
                                                    RegionUsage(req), user_mask,
                                                    target_proc, index);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     start_node, ctx.get_id(), 
                                     true/*before*/, false/*premap*/, 
                                     false/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      bool result = traverser.traverse(start_node);
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      if (result)
        return traverser.get_instance_ref();
      else
        return MappingRef();
    }

    //--------------------------------------------------------------------------
    MappingRef RegionTreeForest::remap_physical_region(RegionTreeContext ctx,
                                                      RegionRequirement &req,
                                                      unsigned index,
                                                      VersionInfo &version_info,
                                                      const InstanceRef &ref
#ifdef DEBUG_HIGH_LEVEL
                                                      , const char *log_name
                                                      , UniqueID uid
#endif
                                                      )
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
      assert(req.handle_type == SINGULAR);
#endif
      // Reductions don't need any update fields
      if (IS_REDUCE(req))
      {
        return MappingRef(ref.get_instance_view(), FieldMask());
      }
#ifdef DEBUG_PERF
      begin_perf_trace(REMAP_PHYSICAL_REGION_ANALYSIS);
#endif
      RegionNode *target_node = get_node(req.region);
      FieldMask user_mask = 
        target_node->column_source->get_field_mask(req.privilege_fields);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     target_node, ctx.get_id(), 
                                     true/*before*/, false/*premap*/, 
                                     false/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      MaterializedView *view = ref.get_materialized_view();
      FieldMask needed_mask;
      target_node->remap_region(ctx.get_id(), view, user_mask, 
                                version_info, needed_mask);
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      return MappingRef(view, needed_mask);
    }

    //--------------------------------------------------------------------------
    MappingRef RegionTreeForest::map_restricted_region(RegionTreeContext ctx,
                                                      RegionRequirement &req,
                                                      unsigned index,
                                                      VersionInfo &version_info,
                                                      Processor target_proc
#ifdef DEBUG_HIGH_LEVEL
                                                      , const char *log_name
                                                      , UniqueID uid
#endif
                                                      )
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
      assert(req.handle_type == SINGULAR);
#endif
#ifdef DEBUG_PERF
      begin_perf_trace(MAP_PHYSICAL_REGION_ANALYSIS);
#endif
      RegionNode *child_node = get_node(req.region);
      FieldMask user_mask = 
        child_node->column_source->get_field_mask(req.privilege_fields);
      // Make an empty path 
      RegionTreePath single_path;
      single_path.initialize(child_node->get_depth(), child_node->get_depth());
      // Construct a dummy mappable info
      MappableInfo info(ctx.get_id(), NULL, Processor::NO_PROC, 
                        req, version_info, user_mask);
      MappingTraverser<true/*restricted*/> traverser(single_path, info, 
                                                     RegionUsage(req), 
                                                     user_mask, 
                                                     target_proc, index);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     child_node, ctx.get_id(), 
                                     true/*before*/, false/*premap*/, 
                                     false/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      bool result = traverser.traverse(child_node);
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      if (result)
        return traverser.get_instance_ref();
      else
        return MappingRef();
    }

    //--------------------------------------------------------------------------
    MappingRef RegionTreeForest::map_restricted_region(RegionTreeContext ctx,
                                                      RegionTreePath &path,
                                                      RegionRequirement &req,
                                                      unsigned index,
                                                      VersionInfo &version_info,
                                                      Processor target_proc
#ifdef DEBUG_HIGH_LEVEL
                                                      , const char *log_name
                                                      , UniqueID uid
#endif
                                                      )
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
      assert(req.handle_type == SINGULAR);
#endif
#ifdef DEBUG_PERF
      begin_perf_trace(MAP_PHYSICAL_REGION_ANALYSIS);
#endif
      RegionNode *child_node = get_node(req.region);
      FieldMask user_mask = 
        child_node->column_source->get_field_mask(req.privilege_fields);
      // Construct a dummy mappable info
      MappableInfo info(ctx.get_id(), NULL, Processor::NO_PROC, 
                        req, version_info, user_mask);
      // Get the start node
      RegionTreeNode *start_node = child_node;
      for (unsigned idx = 0; idx < (path.get_path_length()-1); idx++)
        start_node = start_node->get_parent();
      MappingTraverser<true/*restricted*/> traverser(path, info, 
                                                     RegionUsage(req), 
                                                     user_mask, 
                                                     target_proc, index);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     start_node, ctx.get_id(), 
                                     true/*before*/, false/*premap*/, 
                                     false/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      bool result = traverser.traverse(start_node);
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      if (result)
        return traverser.get_instance_ref();
      else
        return MappingRef();
    }

    //--------------------------------------------------------------------------
    InstanceRef RegionTreeForest::register_physical_region(
                                                      RegionTreeContext ctx,
                                                      const MappingRef &ref,
                                                      RegionRequirement &req,
                                                      unsigned index,
                                                      VersionInfo &version_info,
                                                      Operation *op,
                                                      Processor local_proc,
                                                      Event term_event
#ifdef DEBUG_HIGH_LEVEL
                                                      , const char *log_name
                                                      , UniqueID uid
                                                      , RegionTreePath &path
#endif
                                                      ) 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      begin_perf_trace(REGISTER_PHYSICAL_REGION_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx.exists());
      assert(req.handle_type == SINGULAR);
      assert(ref.has_ref());
#endif
      RegionNode *child_node = get_node(req.region);
      FieldMask user_mask = 
        child_node->column_source->get_field_mask(req.privilege_fields);
      // Construct the mappable info
      MappableInfo info(ctx.get_id(), op, local_proc, 
                        req, version_info, user_mask);
      // Construct the user
      RegionUsage usage(req);
      LogicalView *view = ref.get_view();
      InstanceRef result = child_node->register_region(info, term_event,
                                                       usage, user_mask,
                                                       view, ref.get_mask());
      // If the user requested that this view become persistent make it so
      if (req.make_persistent)
      {
        if (view->is_instance_view() && 
            view->as_instance_view()->is_materialized_view())
        {
          MaterializedView *mat_view = 
            view->as_instance_view()->as_materialized_view();
          if (!mat_view->is_persistent())
          {
            unsigned parent_index = op->find_parent_index(index);
            UserEvent wait_on = UserEvent::create_user_event();
            mat_view->make_persistent(op->get_parent(), parent_index,
                                      runtime->address_space, wait_on);
            // Have to wait for the persistence to be confirmed
            wait_on.wait();
          }
        }
        else
        {
          log_run.warning("Ignoring mapper request to make a non-materialized "
                          "view persistent");
        }
      }
#ifdef DEBUG_HIGH_LEVEL 
      RegionTreeNode *start_node = child_node;
      for (unsigned idx = 0; idx < (path.get_path_length()-1); idx++)
        start_node = start_node->get_parent();
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     start_node, ctx.get_id(), 
                                     false/*before*/, false/*premap*/, 
                                     false/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      return result;
    }
    
    //--------------------------------------------------------------------------
    InstanceRef RegionTreeForest::initialize_physical_context(
                                                RegionTreeContext ctx,
                                                const RegionRequirement &req,
                                                PhysicalManager *manager,
                                                Event term_event,
                                                Processor local_proc,
                                                unsigned depth,
                            std::map<PhysicalManager*,InstanceView*> &top_views)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == SINGULAR);
#endif
      RegionNode *top_node = get_node(req.region);
      RegionUsage usage(req);
      FieldMask user_mask = 
        top_node->column_source->get_field_mask(req.privilege_fields);
      InstanceView *new_view = NULL;
      if (manager->is_reduction_manager())
      {
        std::map<PhysicalManager*,InstanceView*>::const_iterator finder = 
          top_views.find(manager);
        if (finder == top_views.end())
        {
          new_view = manager->as_reduction_manager()->create_view();
          top_views[manager] = new_view;
        }
        else
          new_view = finder->second;
      }
      else
      {
        InstanceManager *inst_manager = manager->as_instance_manager();
#ifdef DEBUG_HIGH_LEVEL
        assert(inst_manager != NULL);
#endif
        std::map<PhysicalManager*,InstanceView*>::const_iterator finder = 
          top_views.find(manager);
        MaterializedView *top_view = NULL;
        if (finder == top_views.end())
        {
          top_view = inst_manager->create_top_view(depth);
          top_views[manager] = top_view;
        }
        else
          top_view = finder->second->as_materialized_view();
#ifdef DEBUG_HIGH_LEVEL
        assert(top_view != NULL);
#endif
        // Now walk from the top view down to the where the 
        // node is that we're initializing
        // First compute the path
        std::vector<ColorPoint> path;
#ifdef DEBUG_HIGH_LEVEL
        bool result = 
#endif
        compute_index_path(inst_manager->region_node->row_source->handle,
                           top_node->row_source->handle, path);
#ifdef DEBUG_HIGH_LEVEL
        assert(result);
        assert(!path.empty());
#endif
        // Note we don't need to traverse the last element
        for (int idx = int(path.size())-2; idx >= 0; idx--)
          top_view = top_view->get_materialized_subview(path[idx]);
        // Once we've made it down to the child we are done
#ifdef DEBUG_HIGH_LEVEL
        assert(top_view->logical_node == top_node);
#endif
        new_view = top_view;
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(new_view != NULL);
#endif
      // It's actually incorrect to do full initialization of the
      // region tree here in case we have multiple region requirements
      // that overlap with each other.
      // Now seed the top node
      return top_node->seed_state(ctx.get_id(), term_event, usage,
                                  user_mask, new_view, local_proc);
    }
    
    //--------------------------------------------------------------------------
    void RegionTreeForest::invalidate_physical_context(RegionTreeContext ctx,
                                                       LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
      RegionNode *top_node = get_node(handle);
      PhysicalInvalidator invalidator(ctx.get_id());
      top_node->visit_node(&invalidator);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::perform_close_operation(RegionTreeContext ctx,
                                                   RegionRequirement &req,
                                                   SingleTask *parent_ctx,
                                                   Processor local_proc,
                                    const std::set<ColorPoint> &target_children,
                                                   bool leave_open,
                                      const std::set<ColorPoint> &next_children,
                                                   Event &closed,
                                                   const MappingRef &target,
                                                   VersionInfo &version_info,
                                                   bool force_composite
#ifdef DEBUG_HIGH_LEVEL
                                                   , unsigned index
                                                   , const char *log_name
                                                   , UniqueID uid
#endif
                                                   )
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      begin_perf_trace(PERFORM_CLOSE_OPERATIONS_ANALYSIS);
#endif
      RegionNode *top_node = get_node(req.parent);
      FieldMask closing_mask = 
        top_node->column_source->get_field_mask(req.privilege_fields);
      MappableInfo info(ctx.get_id(), parent_ctx, 
                        local_proc, req, version_info, closing_mask);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     top_node, ctx.get_id(), 
                                     true/*before*/, true/*premap*/,
                                     true/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), closing_mask);
#endif
      RegionTreeNode *close_node = (req.handle_type == PART_PROJECTION) ?
                  static_cast<RegionTreeNode*>(get_node(req.partition)) : 
                  static_cast<RegionTreeNode*>(get_node(req.region));
      bool create_composite = false;
      bool result = false; 
      if (!force_composite)
        result = close_node->perform_close_operation(info, closing_mask,
                                                     target_children,
                                                     target,
                                                     version_info,
                                                     leave_open, 
                                                     next_children,
                                                     closed,
                                                     create_composite);
      else
        create_composite = true;
      // If we failed or they asked for a composite make it
      if (!result && create_composite)
      {
        close_node->create_composite_instance(info.ctx, target_children,
                        leave_open, next_children, closing_mask, version_info);
        // Making a composite always succeeds
        result = true;
        closed = Event::NO_EVENT;
      }
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     top_node, ctx.get_id(), 
                                     false/*before*/, true/*premap*/,
                                     true/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), closing_mask);
#endif
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::close_physical_context(RegionTreeContext ctx,
                                                  RegionRequirement &req,
                                                  VersionInfo &version_info,
                                                  Operation *op,
                                                  Processor local_proc,
                                                  const InstanceRef &ref 
#ifdef DEBUG_HIGH_LEVEL
                                                  , unsigned index
                                                  , const char *log_name
                                                  , UniqueID uid
#endif
                                                  )
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == SINGULAR);
#endif
      RegionNode *top_node = get_node(req.region);
      FieldMask user_mask = 
        top_node->column_source->get_field_mask(req.privilege_fields);
      RegionUsage usage(req);
      MappableInfo info(ctx.get_id(), op, local_proc, 
                        req, version_info, user_mask);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     top_node, ctx.get_id(), 
                                     true/*before*/, false/*premap*/, 
                                     true/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      Event result = top_node->close_state(info, Event::NO_EVENT, usage,
                                           user_mask, ref);
#ifdef DEBUG_HIGH_LEVEL
      TreeStateLogger::capture_state(runtime, &req, index, log_name, uid,
                                     top_node, ctx.get_id(), 
                                     false/*before*/, false/*premap*/, 
                                     true/*closing*/, false/*logical*/,
                                     FieldMask(FIELD_ALL_ONES), user_mask);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::copy_across(Operation *op,
                                        Processor local_proc,
                                        RegionTreeContext src_ctx,
                                        RegionTreeContext dst_ctx,
                                        RegionRequirement &src_req,
                                        VersionInfo &src_version_info,
                                        const RegionRequirement &dst_req,
                                        const InstanceRef &dst_ref,
                                        Event pre)
    //--------------------------------------------------------------------------
    {
 #ifdef DEBUG_PERF
      begin_perf_trace(COPY_ACROSS_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(src_req.handle_type == SINGULAR);
      assert(dst_req.handle_type == SINGULAR);
      assert(dst_ref.has_ref());
      assert(src_req.instance_fields.size() == dst_req.instance_fields.size());
#endif     
      MaterializedView *dst_view = dst_ref.get_materialized_view();
      // Find the valid instance views for the source and then sort them
      LegionMap<MaterializedView*,FieldMask>::aligned src_instances;
      LegionMap<DeferredView*,FieldMask>::aligned deferred_instances;
      RegionNode *src_node = get_node(src_req.region);
      FieldMask src_mask = 
        src_node->column_source->get_field_mask(src_req.privilege_fields);

      RegionNode *dst_node = get_node(dst_req.region);
      FieldMask dst_mask = 
        dst_node->column_source->get_field_mask(dst_req.privilege_fields);
      // Very important we pass the source version info here!
      MappableInfo info(src_ctx.get_id(), op, 
                        local_proc, src_req, src_version_info, src_mask);
      src_node->find_copy_across_instances(info, dst_view,
                                           src_instances, deferred_instances);
      // Now is where things get tricky, since we don't have any correspondence
      // between fields in the two different requirements we can't use our 
      // normal copy routines. Instead we'll issue copies one field at a time
      std::set<Event> result_events;
      std::vector<Domain::CopySrcDstField> src_fields;
      std::vector<Domain::CopySrcDstField> dst_fields;
      Event dst_pre = dst_ref.get_ready_event(); 
      Event precondition = Event::merge_events(pre, dst_pre);
      // Also compute all of the source preconditions for each instance
      std::map<MaterializedView*,
        LegionMap<Event,FieldMask>::aligned > src_preconditions;
      for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator it =
            src_instances.begin(); it != src_instances.end(); it++)
      {
        LegionMap<Event,FieldMask>::aligned &preconditions = 
                                          src_preconditions[it->first];
        it->first->find_copy_preconditions(0/*redop*/, true/*reading*/,
                                           it->second, src_version_info,
                                           preconditions);
      }
      for (unsigned idx = 0; idx < src_req.instance_fields.size(); idx++)
      {
        src_fields.clear();
        dst_fields.clear();
        unsigned src_index = src_node->column_source->get_field_index(
                                            src_req.instance_fields[idx]);
        bool found = false;
        // Iterate through the instances and see if we can find
        // a materialized views for the source field
        std::set<Event> local_results;
        for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator 
              sit = src_instances.begin(); sit != src_instances.end(); sit++)
        {
          if (sit->second.is_set(src_index))
          {
            // Compute the src_dst fields  
            sit->first->copy_field(src_req.instance_fields[idx], src_fields);
            dst_view->copy_field(dst_req.instance_fields[idx], dst_fields);
            // Compute the event preconditions
            std::set<Event> preconditions;
            preconditions.insert(precondition);
            // Find the source and destination preconditions
            for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                  src_preconditions[sit->first].begin(); it !=
                  src_preconditions[sit->first].end(); it++)
            {
              if (it->second.is_set(src_index))
                preconditions.insert(it->first);
            }
            // Now we've got all the preconditions so we can actually
            // issue the copy operation
            Event copy_pre = Event::merge_events(preconditions);
            Event copy_post = dst_node->perform_copy_operation(op,
                                  copy_pre, src_fields, dst_fields);
            // Register the users of the post condition
            FieldMask local_src; local_src.set_bit(src_index);
            sit->first->add_copy_user(0/*redop*/, copy_post,
                                      src_version_info,
                                      local_src, true/*reading*/);
            // No need to register a user for the destination because
            // we've already mapped it.
            local_results.insert(copy_post);
            found = true;
            break;
          }
        }
        if (!found)
        {
          // Check the composite instances
          for (LegionMap<DeferredView*,FieldMask>::aligned::const_iterator 
                it = deferred_instances.begin(); it != 
                deferred_instances.end(); it++)
          {
            if (it->second.is_set(src_index))
            {
              it->first->issue_deferred_copies_across(info, dst_view,
                                          src_req.instance_fields[idx],
                                          dst_req.instance_fields[idx],
                                          precondition, local_results);
              found = true;
              break;
            }
          }
        }
        // If we still didn't find it then there are no valid
        // instances for the data yet so we're done anyway
        // Now register a result user if necessary
        if (!local_results.empty())
        {
          Event result = Event::merge_events(local_results);
          if (result.exists())
            // Add the event to the result events
            result_events.insert(result);
        }
      }
      Event result = Event::merge_events(result_events);
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::copy_across(Operation *op,
                                        RegionTreeContext src_ctx, 
                                        RegionTreeContext dst_ctx,
                                        const RegionRequirement &src_req,
                                        const RegionRequirement &dst_req,
                                        const InstanceRef &src_ref,
                                        const InstanceRef &dst_ref,
                                        Event precondition)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      begin_perf_trace(COPY_ACROSS_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(src_req.handle_type == SINGULAR);
      assert(dst_req.handle_type == SINGULAR);
      assert(src_ref.has_ref());
      assert(dst_ref.has_ref());
#endif
      // We already have the events for using the physical instances
      // All we need to do is get the offsets for performing the copies
      std::vector<Domain::CopySrcDstField> src_fields;
      std::vector<Domain::CopySrcDstField> dst_fields;
      MaterializedView *src_view = src_ref.get_materialized_view();
      MaterializedView *dst_view = dst_ref.get_materialized_view();
      src_view->manager->compute_copy_offsets(src_req.instance_fields, 
                                              src_fields);
      dst_view->manager->compute_copy_offsets(dst_req.instance_fields, 
                                              dst_fields);

      std::set<Domain> dst_domains;
      RegionNode *dst_node = get_node(dst_req.region);
      Event dom_precondition = Event::NO_EVENT;
      if (dst_node->has_component_domains())
        dst_domains = dst_node->get_component_domains(dom_precondition);
      else
        dst_domains.insert(dst_node->get_domain(dom_precondition));

      Event copy_pre = Event::merge_events(src_ref.get_ready_event(),
                                           dst_ref.get_ready_event(),
                                           precondition, dom_precondition);
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
      if (!copy_pre.exists())
      {
        UserEvent new_copy_pre = UserEvent::create_user_event();
        new_copy_pre.trigger();
        copy_pre = new_copy_pre;
      }
#endif
#ifdef LEGION_LOGGING
      {
        Processor exec_proc = Processor::get_executing_processor();
        LegionLogging::log_event_dependence(exec_proc, 
            src_ref.get_ready_event(), copy_pre);
        LegionLogging::log_event_dependence(exec_proc,
            dst_ref.get_ready_event(), copy_pre);
        LegionLogging::log_event_dependence(exec_proc,
            precondition, copy_pre);
      }
#endif
#if 0
#ifdef LEGION_SPY
      LegionSpy::log_event_dependence(src_ref.get_ready_event(), copy_pre);
      LegionSpy::log_event_dependence(dst_ref.get_ready_event(), copy_pre);
      LegionSpy::log_event_dependence(precondition, copy_pre);
#endif
#endif
      std::set<Event> result_events;
      for (std::set<Domain>::const_iterator it = dst_domains.begin();
            it != dst_domains.end(); it++)
      {
        Event copy_result = issue_copy(*it, op, src_fields, 
                                       dst_fields, copy_pre);
        if (copy_result.exists())
          result_events.insert(copy_result);
      }
      Event result = Event::merge_events(result_events);
      // Note we don't need to add the copy users because
      // we already mapped these regions as part of the CopyOp.
#if 0
#ifdef LEGION_SPY
      if (!result.exists())
      {
        UserEvent new_result = UserEvent::create_user_event();
        new_result.trigger();
        result = new_result;
      }
      {
        RegionNode *src_node = get_node(src_req.region);
        FieldMask src_mask = 
          src_node->column_source->get_field_mask(src_req.privilege_fields);
        FieldMask dst_mask = 
          dst_node->column_source->get_field_mask(dst_req.privilege_fields);
        char *field_mask = src_node->column_source->to_string(src_mask);
        LegionSpy::log_copy_operation(src_view->manager->get_instance().id,
                                      dst_view->manager->get_instance().id,
                                      src_node->handle.index_space.id,
                                      src_node->handle.field_space.id,
                                      src_node->handle.tree_id,
                                      copy_pre, result, src_req.redop,
                                      field_mask);
        free(field_mask);
      }
#endif
#endif
#ifdef DEBUG_PERF
      end_perf_trace(Runtime::perf_trace_tolerance);
#endif
      // No need to add copy users since we added them when we
      // mapped this copy operation
      return result;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::reduce_across(Operation *op,
                                          Processor local_proc,
                                          RegionTreeContext src_ctx,
                                          RegionTreeContext dst_ctx,
                                          RegionRequirement &src_req,
                                          VersionInfo &src_version_info,
                                          const RegionRequirement &dst_req,
                                          const InstanceRef &dst_ref,
                                          Event pre)
    //--------------------------------------------------------------------------
    {
      // TODO: Implement this
      assert(false);
      return Event::NO_EVENT;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::reduce_across(Operation *op,
                                          RegionTreeContext src_ctx, 
                                          RegionTreeContext dst_ctx,
                                          const RegionRequirement &src_req,
                                          const RegionRequirement &dst_req,
                                          const InstanceRef &src_ref,
                                          const InstanceRef &dst_ref,
                                          Event precondition)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      begin_perf_trace(COPY_ACROSS_ANALYSIS);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(src_req.handle_type == SINGULAR);
      assert(dst_req.handle_type == SINGULAR);
      assert(src_ref.has_ref());
      assert(dst_ref.has_ref());
      assert(dst_req.privilege == REDUCE);
#endif
      // We already have the events for using the physical instances
      // All we need to do is get the offsets for performing the copies
      std::vector<Domain::CopySrcDstField> src_fields;
      std::vector<Domain::CopySrcDstField> dst_fields;
      InstanceView *src_inst = src_ref.get_instance_view();
      InstanceView *dst_inst = dst_ref.get_instance_view();
      if (src_inst->is_reduction_view())
      {
        FieldMask src_mask = src_inst->logical_node->column_source->
                                get_field_mask(src_req.privilege_fields);
        src_inst->as_reduction_view()->reduce_from(dst_req.redop, 
                                                   src_mask, src_fields);
      }
      else
      {
        MaterializedView *src_mat_view = src_inst->as_materialized_view();
        src_mat_view->manager->compute_copy_offsets(src_req.instance_fields,
                                                    src_fields);
      }
      FieldMask dst_mask = dst_inst->logical_node->column_source->
                            get_field_mask(dst_req.privilege_fields);
      dst_inst->reduce_to(dst_req.redop, dst_mask, dst_fields); 
      const bool fold = dst_inst->is_reduction_view();

      std::set<Domain> dst_domains;
      RegionNode *dst_node = get_node(dst_req.region);
      Event dom_precondition = Event::NO_EVENT;
      if (dst_node->has_component_domains())
        dst_domains = dst_node->get_component_domains(dom_precondition);
      else
        dst_domains.insert(dst_node->get_domain(dom_precondition));

      Event copy_pre = Event::merge_events(src_ref.get_ready_event(),
                                           dst_ref.get_ready_event(),
                                           precondition, dom_precondition);
      std::set<Event> result_events;
      for (std::set<Domain>::const_iterator it = dst_domains.begin();
            it != dst_domains.end(); it++)
      {
        Event copy_result = issue_reduction_copy(*it, op, dst_req.redop, fold,
                                            src_fields, dst_fields, copy_pre);
        if (copy_result.exists())
          result_events.insert(copy_result);
      }
      Event result = Event::merge_events(result_events);
      return result;
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::fill_fields(RegionTreeContext ctx,
                                       const RegionRequirement &req,
                                       const void *value, size_t value_size,
                                       VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == SINGULAR);
#endif
      RegionNode *fill_node = get_node(req.region);
      FieldMask fill_mask = 
        fill_node->column_source->get_field_mask(req.privilege_fields);
      // Fill in these fields on this node
      fill_node->fill_fields(ctx.get_id(), fill_mask, 
                             value, value_size, version_info); 
    }

    //--------------------------------------------------------------------------
    InstanceRef RegionTreeForest::attach_file(RegionTreeContext ctx,
                                              const RegionRequirement &req,
                                              AttachOp *attach_op,
                                              VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == SINGULAR);
#endif
      RegionNode *attach_node = get_node(req.region);
      FieldMask attach_mask = 
        attach_node->column_source->get_field_mask(req.privilege_fields);
      // Perform the attachment
      return attach_node->attach_file(ctx.get_id(), attach_mask,
                                      req, attach_op, version_info);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::detach_file(RegionTreeContext ctx,
                                       const RegionRequirement &req,
                                       const InstanceRef &ref)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(req.handle_type == SINGULAR);
#endif
      RegionNode *detach_node = get_node(req.region);
      FieldMask detach_mask = 
        detach_node->column_source->get_field_mask(req.privilege_fields);
      PhysicalManager *manager = ref.get_manager();
#ifdef DEBUG_HIGH_LEVEL
      assert(!manager->is_reduction_manager()); 
#endif
      detach_node->detach_file(ctx.get_id(), detach_mask, manager); 
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::check_context_state(RegionTreeContext ctx)
    //--------------------------------------------------------------------------
    {
      std::map<RegionTreeID,RegionNode*> trees;
      {
        AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
        trees = tree_nodes;
      }
      LogicalInitializer log_init(ctx.get_id());
      PhysicalInitializer phy_init(ctx.get_id());
      for (std::map<RegionTreeID,RegionNode*>::const_iterator it = 
            trees.begin(); it != trees.end(); it++)
      {
        it->second->visit_node(&log_init);
        it->second->visit_node(&phy_init);
      }
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* RegionTreeForest::create_node(IndexSpace sp,const Domain &d,
                                                  IndexPartNode *parent,
                                                  ColorPoint color, 
                                                  IndexSpaceKind kind,
                                                  AllocateMode mode)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
      IndexSpaceNode *result = new IndexSpaceNode(sp, d, parent, color, 
                                                  kind, mode, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      // Check to see if someone else has already made it
      {
        // Hold the lookup lock while modifying the lookup table
        AutoLock l_lock(lookup_lock);
        std::map<IndexSpace,IndexSpaceNode*>::const_iterator it =
          index_nodes.find(sp);
        if (it != index_nodes.end())
        {
          delete result;
          return it->second;
        }
        index_nodes[sp] = result;
      }
      if (parent != NULL)
        parent->add_child(result);
      
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* RegionTreeForest::create_node(IndexSpace sp,const Domain &d,
                                                  Event ready_event, 
                                                  IndexPartNode *parent,
                                                  ColorPoint color, 
                                                  IndexSpaceKind kind,
                                                  AllocateMode mode)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
      IndexSpaceNode *result = new IndexSpaceNode(sp, d, ready_event, parent, 
                                                  color, kind, mode, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      // Check to see if someone else has already made it
      {
        // Hold the lookup lock while modifying the lookup table
        AutoLock l_lock(lookup_lock);
        std::map<IndexSpace,IndexSpaceNode*>::const_iterator it =
          index_nodes.find(sp);
        if (it != index_nodes.end())
        {
          delete result;
          return it->second;
        }
        index_nodes[sp] = result;
      }
      if (parent != NULL)
        parent->add_child(result);
      
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* RegionTreeForest::create_node(IndexSpace sp, 
                                                  Event handle_ready,
                                                  Event domain_ready,
                                                  IndexPartNode *parent,
                                                  ColorPoint color,
                                                  IndexSpaceKind kind,
                                                  AllocateMode mode)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
      IndexSpaceNode *result = new IndexSpaceNode(sp, handle_ready, 
                                                  domain_ready, parent, 
                                                  color, kind, mode, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      // Check to see if someone else has already made it
      {
        // Hold the lookup lock while modifying the lookup table
        AutoLock l_lock(lookup_lock);
        std::map<IndexSpace,IndexSpaceNode*>::const_iterator it =
          index_nodes.find(sp);
        if (it != index_nodes.end())
        {
          delete result;
          return it->second;
        }
        index_nodes[sp] = result;
      }
      if (parent != NULL)
        parent->add_child(result);
      
      return result;
    }

    //--------------------------------------------------------------------------
    IndexPartNode* RegionTreeForest::create_node(IndexPartition p, 
                                                 IndexSpaceNode *parent,
                                                 ColorPoint color, 
                                                 Domain color_space,
                                                 bool disjoint,
                                                 AllocateMode mode)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
      IndexPartNode *result = new IndexPartNode(p, parent, color, color_space,
                                                disjoint, mode, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
      assert(result != NULL);
#endif
      // Check to see if someone else has already made it
      {
        // Hold the lookup lock while modifying the lookup table
        AutoLock l_lock(lookup_lock);
        std::map<IndexPartition,IndexPartNode*>::const_iterator it =
          index_parts.find(p);
        if (it != index_parts.end())
        {
          delete result;
          return it->second;
        }
        index_parts[p] = result;
      }
      if (parent != NULL)
        parent->add_child(result);
      
      return result;
    }

    //--------------------------------------------------------------------------
    IndexPartNode* RegionTreeForest::create_node(IndexPartition p, 
                                                 IndexSpaceNode *parent,
                                                 ColorPoint color, 
                                                 Domain color_space,
                                                 Event ready_event,
                                                 AllocateMode mode)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
      IndexPartNode *result = new IndexPartNode(p, parent, color, color_space,
                                                ready_event, mode, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
      assert(result != NULL);
#endif
      // Check to see if someone else has already made it
      {
        // Hold the lookup lock while modifying the lookup table
        AutoLock l_lock(lookup_lock);
        std::map<IndexPartition,IndexPartNode*>::const_iterator it =
          index_parts.find(p);
        if (it != index_parts.end())
        {
          delete result;
          return it->second;
        }
        index_parts[p] = result;
      }
      if (parent != NULL)
        parent->add_child(result);
      
      return result;
    }
 
    //--------------------------------------------------------------------------
    FieldSpaceNode* RegionTreeForest::create_node(FieldSpace space,
                                                  Event dist_alloc)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
      FieldSpaceNode *result = new FieldSpaceNode(space, dist_alloc, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      // Hold the lookup lock while modifying the lookup table
      AutoLock l_lock(lookup_lock);
      std::map<FieldSpace,FieldSpaceNode*>::const_iterator it =
        field_nodes.find(space);
      if (it != field_nodes.end())
      {
        delete result;
        return it->second;
      }
      field_nodes[space] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    RegionNode* RegionTreeForest::create_node(LogicalRegion r, 
                                              PartitionNode *parent)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      if (parent != NULL)
      {
        assert(r.field_space == parent->handle.field_space);
        assert(r.tree_id == parent->handle.tree_id);
      }
#endif
      IndexSpaceNode *row_src = get_node(r.index_space);
      FieldSpaceNode *col_src = get_node(r.field_space);
      RegionNode *result = new RegionNode(r, parent, row_src, 
                                          col_src, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      // Special case here in case multiple clients attempt to
      // make the node at the same time
      {
        // Hold the lookup lock when modifying the lookup table
        AutoLock l_lock(lookup_lock);
        // Check to see if it already exists
        std::map<LogicalRegion,RegionNode*>::const_iterator it =
          region_nodes.find(r);
        if (it != region_nodes.end())
        {
          // It already exists, delete our copy and return
          // the one that has already been made
          delete result;
          return it->second;
        }
        // Now we can add it to the map
        region_nodes[r] = result;
        // If this is a top level region add it to the collection
        // of top level tree IDs
        if (parent == NULL)
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(tree_nodes.find(r.tree_id) == tree_nodes.end());
#endif
          tree_nodes[r.tree_id] = result;
        }
      }
      // Now we can make the other ways of accessing the node available
      if (parent == NULL)
        col_src->add_instance(result);
      row_src->add_instance(result);
      if (parent != NULL)
        parent->add_child(result);
      
      return result;
    }

    //--------------------------------------------------------------------------
    PartitionNode* RegionTreeForest::create_node(LogicalPartition p,
                                                 RegionNode *parent)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, CREATE_NODE_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
      assert(p.field_space == parent->handle.field_space);
      assert(p.tree_id = parent->handle.tree_id);
#endif
      IndexPartNode *row_src = get_node(p.index_partition);
      FieldSpaceNode *col_src = get_node(p.field_space);
      PartitionNode *result = new PartitionNode(p, parent, row_src, 
                                                col_src, this);
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      // Special case here in case multiple clients attempt
      // to make the node at the same time
      {
        // Hole the lookup lock when modifying the lookup table
        AutoLock l_lock(lookup_lock);
        std::map<LogicalPartition,PartitionNode*>::const_iterator it =
          part_nodes.find(p);
        if (it != part_nodes.end())
        {
          // It already exists, delete our copy and
          // return the one that has already been made
          delete result;
          return it->second;
        }
        // Now we can put the node in the map
        part_nodes[p] = result;
      }
      // Now we can make the other ways of accessing the node available
      row_src->add_instance(result);
      parent->add_child(result);
      
      return result;
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* RegionTreeForest::get_node(IndexSpace space)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, GET_NODE_CALL);
#endif
      {
        AutoLock l_lock(lookup_lock,1,false/*exclusive*/); 
        std::map<IndexSpace,IndexSpaceNode*>::const_iterator finder = 
          index_nodes.find(space);
        if (finder != index_nodes.end())
          return finder->second;
      }
      // Couldn't find it, so send a request to the owner node
      AddressSpace owner = space.id % runtime->runtime_stride; 
#ifdef DEBUG_HIGH_LEVEL
      // Should never be local
      assert(owner != runtime->address_space); 
#endif
      UserEvent wait_on = UserEvent::create_user_event();
      Serializer rez;
      rez.serialize(space);
      rez.serialize(wait_on);
      runtime->send_index_space_request(owner, rez);
      // Wait on the event, be safe for now and block
      wait_on.wait();
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      std::map<IndexSpace,IndexSpaceNode*>::const_iterator finder = 
          index_nodes.find(space);
      if (finder == index_nodes.end())
      {
        log_index.error("Unable to find entry for index space %x."
                        "This is definitely a runtime bug.", space.id);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_INDEX_SPACE_ENTRY);
      }
      return finder->second;
    }

    //--------------------------------------------------------------------------
    IndexPartNode* RegionTreeForest::get_node(IndexPartition part)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, GET_NODE_CALL);
#endif
      {
        AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
        std::map<IndexPartition,IndexPartNode*>::const_iterator finder =
          index_parts.find(part);
        if (finder != index_parts.end())
          return finder->second;
      }
      // Couldn't find it, so send a request to the owner node
      AddressSpace owner = part.id % runtime->runtime_stride; 
#ifdef DEBUG_HIGH_LEVEL
      // Should never be local
      assert(owner != runtime->address_space); 
#endif
      UserEvent wait_on = UserEvent::create_user_event();
      Serializer rez;
      rez.serialize(part);
      rez.serialize(wait_on);
      runtime->send_index_partition_request(owner, rez);
      // Be safe and block for now
      wait_on.wait();
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      std::map<IndexPartition,IndexPartNode*>::const_iterator finder = 
        index_parts.find(part);
      if (finder == index_parts.end())
      {
        log_index.error("Unable to find entry for index partition %x. "
                        "This is definitely a runtime bug.", part.id);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_INDEX_PART_ENTRY);
      }
      return finder->second;
    }

    //--------------------------------------------------------------------------
    FieldSpaceNode* RegionTreeForest::get_node(FieldSpace space)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, GET_NODE_CALL);
#endif
      {
        AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
        std::map<FieldSpace,FieldSpaceNode*>::const_iterator finder = 
          field_nodes.find(space);
        if (finder != field_nodes.end())
          return finder->second;
      }
      // Couldn't find it, so send a request to the owner node
      AddressSpace owner = space.id % runtime->runtime_stride; 
#ifdef DEBUG_HIGH_LEVEL
      // Should never be local
      assert(owner != runtime->address_space); 
#endif
      UserEvent wait_on = UserEvent::create_user_event();
      Serializer rez;
      rez.serialize(space);
      rez.serialize(wait_on);
      runtime->send_field_space_request(owner, rez);
      wait_on.wait();
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      std::map<FieldSpace,FieldSpaceNode*>::const_iterator finder = 
        field_nodes.find(space);
      if (finder == field_nodes.end())
      {
        log_field.error("Unable to find entry for field space %x. "
                        "This is definitely a runtime bug.", space.id);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_FIELD_SPACE_ENTRY);
      }
      return finder->second;
    }

    //--------------------------------------------------------------------------
    RegionNode* RegionTreeForest::get_node(LogicalRegion handle)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, GET_NODE_CALL);
#endif
      // Check to see if the node already exists
      {
        AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
        std::map<LogicalRegion,RegionNode*>::const_iterator it = 
          region_nodes.find(handle);
        if (it != region_nodes.end())
          return it->second;
      }
      // Otherwise it hasn't been made yet, so make it
      IndexSpaceNode *index_node = get_node(handle.index_space);
      if (index_node->parent != NULL)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(index_node->parent != NULL);
#endif
        LogicalPartition parent_handle(handle.tree_id, 
                              index_node->parent->handle, handle.field_space);
        // Note this request can recursively build more nodes, but we
        // are guaranteed that the top level node exists
        PartitionNode *parent = get_node(parent_handle);
        // Now make our node and then return it
        return create_node(handle, parent);
      }
      return create_node(handle, NULL);
    }

    //--------------------------------------------------------------------------
    PartitionNode* RegionTreeForest::get_node(LogicalPartition handle)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, GET_NODE_CALL);
#endif
      // Check to see if the node already exists
      {
        AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
        std::map<LogicalPartition,PartitionNode*>::const_iterator it =
          part_nodes.find(handle);
        if (it != part_nodes.end())
          return it->second;
      }
      // Otherwise it hasn't been made yet so make it
      IndexPartNode *index_node = get_node(handle.index_partition);
#ifdef DEBUG_HIGH_LEVEL
      assert(index_node->parent != NULL);
#endif
      LogicalRegion parent_handle(handle.tree_id, index_node->parent->handle,
                                  handle.field_space);
      // Note this request can recursively build more nodes, but we
      // are guaranteed that the top level node exists
      RegionNode *parent = get_node(parent_handle);
      // Now create our node and return it
      return create_node(handle, parent);
    }

    //--------------------------------------------------------------------------
    RegionNode* RegionTreeForest::get_tree(RegionTreeID tid)
    //--------------------------------------------------------------------------
    {
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      std::map<RegionTreeID,RegionNode*>::const_iterator it = 
        tree_nodes.find(tid);
      if (it == tree_nodes.end())
      {
        log_region.error("Unable to find top-level tree entry for "
                               "region tree %d.  This is either a runtime "
                               "bug or requires Legion fences if names are "
                               "being returned out fo the context in which"
                               "they are being created.", tid);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_TREE_ENTRY);
      }
      return it->second;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_node(IndexSpace space) const
    //--------------------------------------------------------------------------
    {
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      return (index_nodes.find(space) != index_nodes.end());
    }
    
    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_node(IndexPartition part) const
    //--------------------------------------------------------------------------
    {
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      return (index_parts.find(part) != index_parts.end());
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_node(FieldSpace space) const
    //--------------------------------------------------------------------------
    {
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      return (field_nodes.find(space) != field_nodes.end());
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_node(LogicalRegion handle) const
    //--------------------------------------------------------------------------
    {
      // Reflect that we can build these nodes whenever this is true
      return (has_node(handle.index_space) && has_node(handle.field_space) &&
              has_tree(handle.tree_id));
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_node(LogicalPartition handle) const
    //--------------------------------------------------------------------------
    {
      // Reflect that we can build these nodes whenever this is true
      return (has_node(handle.index_partition) && has_node(handle.field_space)
              && has_tree(handle.tree_id));
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_tree(RegionTreeID tid) const
    //--------------------------------------------------------------------------
    {
      AutoLock l_lock(lookup_lock,1,false/*exclusive*/);
      return (tree_nodes.find(tid) != tree_nodes.end());
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::has_field(FieldSpace space, FieldID fid)
    //--------------------------------------------------------------------------
    {
      if (!has_node(space))
        return false;
      return get_node(space)->has_field(fid);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::is_subregion(LogicalRegion child, 
                                        LogicalRegion parent)
    //--------------------------------------------------------------------------
    {
      if (child == parent)
        return true;
      if (child.get_tree_id() != parent.get_tree_id())
        return false;
      std::vector<ColorPoint> path;
      return compute_index_path(parent.get_index_space(),
                                child.get_index_space(), path);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::is_disjoint(IndexPartition handle)
    //--------------------------------------------------------------------------
    {
      IndexPartNode *node = get_node(handle);
      return node->is_disjoint(true/*app query*/);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::is_disjoint(LogicalPartition handle)
    //--------------------------------------------------------------------------
    {
      return is_disjoint(handle.get_index_partition());
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::are_disjoint(IndexSpace parent, IndexSpace child)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, ARE_DISJOINT_CALL);
#endif
      std::vector<ColorPoint> path;
      if (compute_index_path(parent, child, path))
        return false;
      // Now check for a common ancestor and see if the
      // children are disjoint
      IndexSpaceNode *sp_one = get_node(parent);
      IndexSpaceNode *sp_two = get_node(child);
      // Bring them up to the same minimum depth
      unsigned depth = sp_one->depth;
      if (sp_two->depth < depth)
        depth = sp_two->depth;
      while (sp_one->depth > depth)
        sp_one = sp_one->parent->parent;
      while (sp_two->depth > depth)
        sp_two = sp_two->parent->parent;
      // Now we're at the same depth, we know they can't
      // equal or else there would have been a path
#ifdef DEBUG_HIGH_LEVEL
      assert(sp_one != sp_two);
      assert(sp_one->depth == sp_two->depth);
#endif
      while (sp_one->depth > 0)
      {
        // Check for a common partition
        if (sp_one->parent == sp_two->parent)
          return sp_one->parent->are_disjoint(sp_one->color, sp_two->color);
        // Check for a common new space
        if (sp_one->parent->parent == sp_two->parent->parent)
          return sp_one->parent->parent->are_disjoint(sp_one->parent->color,
                                                      sp_two->parent->color);
        // Otherwise advance everything
        sp_one = sp_one->parent->parent;
        sp_two = sp_two->parent->parent;
      }
      // Otherwise they're not even in the same tree
      // which guarantees disjointness
      return true;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::are_disjoint(IndexSpace parent, IndexPartition child)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, ARE_DISJOINT_CALL);
#endif
      std::vector<ColorPoint> path;
      if (compute_partition_path(parent, child, path))
        return false;
      IndexPartNode *part_node = get_node(child);
      // Do a little check for a path between the partitions
      // parent and the parent node
      if (compute_index_path(parent, part_node->parent->handle, path))
      {
        path.pop_back(); // pop off the parent node's color
        return part_node->parent->are_disjoint(part_node->color,path.back());
      }
      if (compute_index_path(part_node->parent->handle, parent, path))
      {
        path.pop_back(); // pop off the parent node's color
        return part_node->parent->are_disjoint(part_node->color,path.back());
      }
      // Now check for a common ancestor and see if the
      // children are disjoint
      IndexSpaceNode *sp_one = get_node(parent);
      IndexSpaceNode *sp_two = part_node->parent;
      // Bring them up to the same minimum depth
      unsigned depth = sp_one->depth;
      if (sp_two->depth < depth)
        depth = sp_two->depth;
      while (sp_one->depth > depth)
        sp_one = sp_one->parent->parent;
      while (sp_two->depth > depth)
        sp_two = sp_two->parent->parent;
#ifdef DEBUG_HIGH_LEVEL
      assert(sp_one != sp_two);
      assert(sp_one->depth == sp_two->depth);
#endif
      while (sp_one->depth > 0)
      {
        // Check for a common partition
        if (sp_one->parent == sp_two->parent)
          return sp_one->parent->are_disjoint(sp_one->color, sp_two->color);
        // Check for a common new space
        if (sp_one->parent->parent == sp_two->parent->parent)
          return sp_one->parent->parent->are_disjoint(sp_one->parent->color,
                                                      sp_two->parent->color);
        // Otherwise advance everything
        sp_one = sp_one->parent->parent;
        sp_two = sp_two->parent->parent;
      }
      // Otherwise they are not in the same tree
      // and therefore by definition disjoint
      return true;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::are_compatible(IndexSpace left, IndexSpace right)
    //--------------------------------------------------------------------------
    {
      IndexSpaceNode *left_node = get_node(left);
      IndexSpaceNode *right_node = get_node(right);
      const Domain &left_dom = left_node->get_domain_blocking();
      const Domain &right_dom = right_node->get_domain_blocking();
      if (left_dom.get_dim() != right_dom.get_dim())
        return false;
      else if (left_dom.get_dim() == 0)
      {
        const LowLevel::ElementMask &left_mask = 
          left_dom.get_index_space().get_valid_mask();
        const LowLevel::ElementMask &right_mask = 
          right_dom.get_index_space().get_valid_mask();
        return (left_mask.get_num_elmts() == right_mask.get_num_elmts());
      }
      return true;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::is_dominated(IndexSpace src, IndexSpace dst)
    //--------------------------------------------------------------------------
    {
      // Check to see if dst is dominated by source
#ifdef DEBUG_HIGH_LEVEL
      assert(are_compatible(src, dst));
#endif
      IndexSpaceNode *src_node = get_node(src);
      IndexSpaceNode *dst_node = get_node(dst);
      return src_node->dominates(dst_node);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::compute_index_path(IndexSpace parent, 
                               IndexSpace child, std::vector<ColorPoint> &path)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, COMPUTE_PATH_CALL);
#endif
      IndexSpaceNode *child_node = get_node(child); 
      path.push_back(child_node->color);
      if (parent == child) 
        return true; // Early out
      IndexSpaceNode *parent_node = get_node(parent);
      while (parent_node != child_node)
      {
        if (parent_node->depth >= child_node->depth)
        {
          path.clear();
          return false;
        }
        if (child_node->parent == NULL)
        {
          path.clear();
          return false;
        }
        path.push_back(child_node->parent->color);
        path.push_back(child_node->parent->parent->color);
        child_node = child_node->parent->parent;
      }
      return true;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeForest::compute_partition_path(IndexSpace parent, 
                           IndexPartition child, std::vector<ColorPoint> &path)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this, COMPUTE_PATH_CALL);
#endif
      IndexPartNode *child_node = get_node(child);
      path.push_back(child_node->color);
      if (child_node->parent == NULL)
      {
        path.clear();
        return false;
      }
      return compute_index_path(parent, child_node->parent->handle, path);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::initialize_path(IndexSpace child, IndexSpace parent,
                                           RegionTreePath &path)
    //--------------------------------------------------------------------------
    {
      initialize_path(get_node(child), get_node(parent), path);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::initialize_path(IndexPartition child, 
                                           IndexSpace parent, 
                                           RegionTreePath &path)
    //--------------------------------------------------------------------------
    {
      initialize_path(get_node(child), get_node(parent), path);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::initialize_path(IndexSpace child,
                                           IndexPartition parent,
                                           RegionTreePath &path)
    //--------------------------------------------------------------------------
    {
      initialize_path(get_node(child), get_node(parent), path);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::initialize_path(IndexPartition child,
                                           IndexPartition parent,
                                           RegionTreePath &path)
    //--------------------------------------------------------------------------
    {
      initialize_path(get_node(child), get_node(parent), path);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::initialize_path(IndexTreeNode *child,
                                           IndexTreeNode *parent,
                                           RegionTreePath &path)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(child->depth >= parent->depth);
#endif
      path.initialize(parent->depth, child->depth);
      while (child != parent)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(child->depth > 0);
#endif
        path.register_child(child->depth-1,child->color);
        child = child->get_parent();
      }
    }

    //--------------------------------------------------------------------------
    FatTreePath* RegionTreeForest::compute_fat_path(IndexSpace child,
                                                    IndexSpace parent,
                                 std::map<IndexTreeNode*,FatTreePath*> &storage,
                                                    bool test, bool &overlap)
    //--------------------------------------------------------------------------
    {
      IndexTreeNode *child_node = get_node(child);
      IndexTreeNode *parent_node = get_node(parent);
      return compute_fat_path(child_node, parent_node, storage, test, overlap);
    }

    //--------------------------------------------------------------------------
    FatTreePath* RegionTreeForest::compute_fat_path(IndexSpace child,
                                                    IndexPartition parent,
                                 std::map<IndexTreeNode*,FatTreePath*> &storage,
                                                    bool test, bool &overlap)
    //--------------------------------------------------------------------------
    {
      IndexTreeNode *child_node = get_node(child);
      IndexTreeNode *parent_node = get_node(parent);
      return compute_fat_path(child_node, parent_node, storage, test, overlap);
    }

    //--------------------------------------------------------------------------
    FatTreePath* RegionTreeForest::compute_fat_path(IndexTreeNode *child,
                                                    IndexTreeNode *parent,
                                 std::map<IndexTreeNode*,FatTreePath*> &storage,
                                               bool test_overlap, bool &overlap)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent->depth <= child->depth); // some sanity checking
#endif
      if (storage.find(child) != storage.end())
      {
        if (test_overlap)
          overlap = true;
        std::map<IndexTreeNode*,FatTreePath*>::const_iterator finder = 
          storage.find(parent);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != storage.end());
#endif
        return finder->second;
      }
      if (child == parent)
      {
        FatTreePath *result = new FatTreePath();
        storage[child] = result;
        if (test_overlap)
          overlap = false;
        return result;
      }
      IndexTreeNode *current = child;
      FatTreePath *current_path = new FatTreePath();
      // Add ourselves to the map
      storage[current] = current_path;
      while (current != parent)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(parent->depth < child->depth);
#endif
        IndexTreeNode *next = current->get_parent();
#ifdef DEBUG_HIGH_LEVEL
        assert(next != NULL);
#endif
        std::map<IndexTreeNode*,FatTreePath*>::const_iterator finder = 
          storage.find(next);
        if (finder != storage.end())
        {
          // If we found it then add it and check for disjointness
          if (test_overlap)
            overlap = finder->second->add_child(current->color, 
                                                current_path, next);
          else
            finder->second->add_child(current->color, current_path);
          if (next == parent)
            return finder->second;
          finder = storage.find(parent);
#ifdef DEBUG_HIGH_LEVEL
          assert(finder != storage.end()); 
#endif
          return finder->second;
        }
        // Otherwise the next one doesn't exist yet so make it 
        FatTreePath *next_path = new FatTreePath();
        storage[next] = next_path;
        next_path->add_child(current->color, current_path);
        current = next;
        current_path = next_path;
      }
      // We had to add the whole path so we are done
      if (test_overlap)
        overlap = false;
      return current_path;
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::issue_copy(const Domain &dom, Operation *op,
                         const std::vector<Domain::CopySrcDstField> &src_fields,
                         const std::vector<Domain::CopySrcDstField> &dst_fields,
                                       Event precondition)
    //--------------------------------------------------------------------------
    {
      if (runtime->profiler != NULL)
      {
        Realm::ProfilingRequestSet requests;
        runtime->profiler->add_copy_request(requests, op); 
        return dom.copy(src_fields, dst_fields, requests, precondition);
      }
      else
        return dom.copy(src_fields, dst_fields, precondition);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::issue_fill(const Domain &dom, UniqueID uid,
                         const std::vector<Domain::CopySrcDstField> &dst_fields,
                         const void *fill_value, size_t fill_size,
                                       Event precondition)
    //--------------------------------------------------------------------------
    {
      if (runtime->profiler != NULL)
      {
        Realm::ProfilingRequestSet requests;
        runtime->profiler->add_fill_request(requests, uid);
        return dom.fill(dst_fields, requests, 
                        fill_value, fill_size, precondition);
      }
      else
        return dom.fill(dst_fields, fill_value, fill_size, precondition);
    }
    
    //--------------------------------------------------------------------------
    Event RegionTreeForest::issue_reduction_copy(const Domain &dom, 
                        Operation *op, ReductionOpID redop, bool reduction_fold,
                         const std::vector<Domain::CopySrcDstField> &src_fields,
                         const std::vector<Domain::CopySrcDstField> &dst_fields,
                                       Event precondition)
    //--------------------------------------------------------------------------
    {
      if (runtime->profiler != NULL)
      {
        Realm::ProfilingRequestSet requests;
        runtime->profiler->add_copy_request(requests, op); 
        return dom.copy(src_fields, dst_fields, requests, 
                        precondition, redop, reduction_fold);
      }
      else
        return dom.copy(src_fields, dst_fields,
                        precondition, redop, reduction_fold);
    }

    //--------------------------------------------------------------------------
    Event RegionTreeForest::issue_indirect_copy(const Domain &dom, 
                       Operation *op, const Domain::CopySrcDstField &idx,
                       ReductionOpID redop, bool reduction_fold,
                       const std::vector<Domain::CopySrcDstField> &src_fields,
                       const std::vector<Domain::CopySrcDstField> &dst_fields,
                                                Event precondition)
    //--------------------------------------------------------------------------
    {
      // TODO: teach the low-level runtime to profile indirect copies
      return dom.copy_indirect(idx, src_fields, dst_fields, 
                               precondition, redop, reduction_fold);
    }

    //--------------------------------------------------------------------------
    PhysicalInstance RegionTreeForest::create_instance(const Domain &dom,
                               Memory target, size_t field_size, UniqueID op_id)
    //--------------------------------------------------------------------------
    {
      if (runtime->profiler != NULL)
      {
        Realm::ProfilingRequestSet requests;
        runtime->profiler->add_inst_request(requests, op_id);
        return dom.create_instance(target, field_size, requests);
      }
      else
        return dom.create_instance(target, field_size);
    }

    //--------------------------------------------------------------------------
    PhysicalInstance RegionTreeForest::create_instance(const Domain &dom,
                          Memory target, const std::vector<size_t> &field_sizes, 
                          size_t blocking_factor, UniqueID op_id)
    //--------------------------------------------------------------------------
    {
      if (runtime->profiler != NULL)
      {
        Realm::ProfilingRequestSet reqs;
        runtime->profiler->add_inst_request(reqs, op_id);
        return dom.create_instance(target, field_sizes, blocking_factor, reqs);
      }
      else
        return dom.create_instance(target, field_sizes, blocking_factor);
    }

    //--------------------------------------------------------------------------
    PhysicalInstance RegionTreeForest::create_instance(const Domain &dom,
          Memory target, size_t field_size, ReductionOpID redop, UniqueID op_id)
    //--------------------------------------------------------------------------
    {
      if (runtime->profiler != NULL)
      {
        Realm::ProfilingRequestSet requests;
        runtime->profiler->add_inst_request(requests, op_id);
        return dom.create_instance(target, field_size, requests, redop);
      }
      else
        return dom.create_instance(target, field_size, redop);
    }

    //--------------------------------------------------------------------------
    template<typename T>
    Color RegionTreeForest::generate_unique_color(
                                          const std::map<Color,T> &current_map)
    //--------------------------------------------------------------------------
    {
      if (current_map.empty())
        return runtime->get_start_color();
      unsigned stride = runtime->get_color_modulus();
      typename std::map<Color,T>::const_reverse_iterator rlast = current_map.rbegin();
      Color result = rlast->first + stride;
#ifdef DEBUG_HIGH_LEVEL
      assert(current_map.find(result) == current_map.end());
#endif
      return result;
    }

#ifdef DEBUG_HIGH_LEVEL
    //--------------------------------------------------------------------------
    void RegionTreeForest::dump_logical_state(LogicalRegion region,
                                              ContextID ctx)
    //--------------------------------------------------------------------------
    {
      TreeStateLogger dump_logger; 
      assert(region_nodes.find(region) != region_nodes.end());
      region_nodes[region]->dump_logical_context(ctx, &dump_logger,
                                                 FieldMask(FIELD_ALL_ONES));
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::dump_physical_state(LogicalRegion region,
                                               ContextID ctx)
    //--------------------------------------------------------------------------
    {
      TreeStateLogger dump_logger;
      assert(region_nodes.find(region) != region_nodes.end());
      region_nodes[region]->dump_physical_context(ctx, &dump_logger,
                                                  FieldMask(FIELD_ALL_ONES));
    }
#endif

    //--------------------------------------------------------------------------
    void RegionTreeForest::attach_semantic_information(IndexSpace handle,
                                                       SemanticTag tag,
                                                       const NodeSet &source,
                                                       const void *buffer,
                                                       size_t size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->attach_semantic_information(tag, source, buffer, size);
#ifdef LEGION_SPY
      if (NAME_SEMANTIC_TAG == tag)
        LegionSpy::log_index_space_name(handle.id,
            reinterpret_cast<const char*>(buffer));
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::attach_semantic_information(IndexPartition handle,
                                                       SemanticTag tag,
                                                       const NodeSet &source,
                                                       const void *buffer,
                                                       size_t size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->attach_semantic_information(tag, source, buffer, size);
#ifdef LEGION_SPY
      if (NAME_SEMANTIC_TAG == tag)
        LegionSpy::log_index_partition_name(handle.id,
            reinterpret_cast<const char*>(buffer));
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::attach_semantic_information(FieldSpace handle,
                                                       SemanticTag tag,
                                                       const NodeSet &source,
                                                       const void *buffer,
                                                       size_t size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->attach_semantic_information(tag, source, buffer, size);
#ifdef LEGION_SPY
      if (NAME_SEMANTIC_TAG == tag)
        LegionSpy::log_field_space_name(handle.id,
            reinterpret_cast<const char*>(buffer));
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::attach_semantic_information(FieldSpace handle,
                                                       FieldID fid,
                                                       SemanticTag tag,
                                                       const NodeSet &src,
                                                       const void *buf,
                                                       size_t size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->attach_semantic_information(fid, tag, src, buf, size);
#ifdef LEGION_SPY
      if (NAME_SEMANTIC_TAG == tag)
        LegionSpy::log_field_name(handle.id, fid,
            reinterpret_cast<const char*>(buf));
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::attach_semantic_information(LogicalRegion handle,
                                                       SemanticTag tag,
                                                       const NodeSet &source,
                                                       const void *buffer,
                                                       size_t size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->attach_semantic_information(tag, source, buffer, size);
#ifdef LEGION_SPY
      if (NAME_SEMANTIC_TAG == tag)
        LegionSpy::log_logical_region_name(handle.index_space.id,
            handle.field_space.id, handle.tree_id,
            reinterpret_cast<const char*>(buffer));
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::attach_semantic_information(LogicalPartition handle,
                                                       SemanticTag tag,
                                                       const NodeSet &source,
                                                       const void *buffer,
                                                       size_t size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->attach_semantic_information(tag, source, buffer, size);
#ifdef LEGION_SPY
      if (NAME_SEMANTIC_TAG == tag)
        LegionSpy::log_logical_partition_name(handle.index_partition.id,
            handle.field_space.id, handle.tree_id,
            reinterpret_cast<const char*>(buffer));
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::retrieve_semantic_information(IndexSpace handle,
                                                         SemanticTag tag,
                                                         const void *&result,
                                                         size_t &size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->retrieve_semantic_information(tag, result, size);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::retrieve_semantic_information(IndexPartition handle,
                                                         SemanticTag tag,
                                                         const void *&result,
                                                         size_t &size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->retrieve_semantic_information(tag, result, size);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::retrieve_semantic_information(FieldSpace handle,
                                                         SemanticTag tag,
                                                         const void *&result,
                                                         size_t &size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->retrieve_semantic_information(tag, result, size);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::retrieve_semantic_information(FieldSpace handle,
                                                         FieldID fid,
                                                         SemanticTag tag,
                                                         const void *&result,
                                                         size_t &size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->retrieve_semantic_information(fid, tag, result, size);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::retrieve_semantic_information(LogicalRegion handle,
                                                         SemanticTag tag,
                                                         const void *&result,
                                                         size_t &size)
    //--------------------------------------------------------------------------
    {
      get_node(handle)->retrieve_semantic_information(tag, result, size);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::retrieve_semantic_information(LogicalPartition part,
                                                         SemanticTag tag,
                                                         const void *&result,
                                                         size_t &size)
    //--------------------------------------------------------------------------
    {
      get_node(part)->retrieve_semantic_information(tag, result, size);
    }

    //--------------------------------------------------------------------------
    /*static*/ bool RegionTreeForest::are_disjoint(const Domain &left,
                                                   const Domain &right)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(left.get_dim() == right.get_dim());
#endif
      bool disjoint = true;
      switch (left.get_dim())
      {
        case 0:
          {
            const LowLevel::ElementMask &left_mask = 
                                       left.get_index_space().get_valid_mask();
            const LowLevel::ElementMask &right_mask = 
                                       right.get_index_space().get_valid_mask();
            LowLevel::ElementMask::OverlapResult result = 
                                            left_mask.overlaps_with(right_mask);
            if (result != LowLevel::ElementMask::OVERLAP_NO)
              disjoint = false;
            break;
          }
        case 1:
          {
            Rect<1> leftr = left.get_rect<1>();
            Rect<1> rightr = right.get_rect<1>();
            if (leftr.overlaps(rightr))
              disjoint = false;
            break;
          }
        case 2:
          {
            Rect<2> leftr = left.get_rect<2>();
            Rect<2> rightr = right.get_rect<2>();
            if (leftr.overlaps(rightr))
              disjoint = false;
            break;
          }
        case 3:
          {
            Rect<3> leftr = left.get_rect<3>();
            Rect<3> rightr = right.get_rect<3>();
            if (leftr.overlaps(rightr))
              disjoint = false;
            break;
          }
        default:
          assert(false);
      }
      return disjoint;
    }

    //--------------------------------------------------------------------------
    /*static*/ bool RegionTreeForest::are_disjoint(IndexSpaceNode *left,
                                                   IndexSpaceNode *right)
    //--------------------------------------------------------------------------
    {
      bool disjoint = true;
      if (left->has_component_domains())
      {
        const std::set<Domain> &left_domains = 
          left->get_component_domains_blocking();
        if (right->has_component_domains())
        {
          const std::set<Domain> &right_domains = 
            right->get_component_domains_blocking();
          // Double Loop
          for (std::set<Domain>::const_iterator lit = left_domains.begin();
                disjoint && (lit != left_domains.end()); lit++)
          {
            for (std::set<Domain>::const_iterator rit = right_domains.begin();
                  disjoint && (rit != right_domains.end()); rit++)
            {
              disjoint = RegionTreeForest::are_disjoint(*lit, *rit);
            }
          }
        }
        else
        {
          // Loop over left components
          for (std::set<Domain>::const_iterator it = left_domains.begin();
                disjoint && (it != left_domains.end()); it++)
          {
            disjoint = RegionTreeForest::are_disjoint(*it, 
                        right->get_domain_blocking());
          }
        }
      }
      else
      {
        if (right->has_component_domains())
        {
          const std::set<Domain> &right_domains = 
              right->get_component_domains_blocking();
          // Loop over right components
          for (std::set<Domain>::const_iterator it = right_domains.begin();
                disjoint && (it != right_domains.end()); it++)
          {
            disjoint = RegionTreeForest::are_disjoint(
                          left->get_domain_blocking(), *it);
          }
        }
        else
        {
          // No Loops
          disjoint = RegionTreeForest::are_disjoint(left->get_domain_blocking(),
                                                  right->get_domain_blocking());
        }
      }
      return disjoint;
    }

#ifdef DEBUG_PERF
    //--------------------------------------------------------------------------
    void RegionTreeForest::record_call(int kind, unsigned long long time)
    //--------------------------------------------------------------------------
    {
      Processor p = Processor::get_executing_processor();
      traces[p.local_id()].back().record_call(kind, time);
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::begin_perf_trace(int kind)
    //--------------------------------------------------------------------------
    {
      Processor p = Processor::get_executing_processor();
      unsigned long long start = TimeStamp::get_current_time_in_micros();
      assert(p.local_id() < traces.size());
      traces[p.local_id()].push_back(PerfTrace(kind, start));
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::end_perf_trace(unsigned long long tolerance)
    //--------------------------------------------------------------------------
    {
      Processor p = Processor::get_executing_processor();
      unsigned long long stop = TimeStamp::get_current_time_in_micros();
      unsigned index = p.local_id();
      PerfTrace &trace = traces[index].back();
      unsigned long long diff = stop - trace.start;
      if (diff >= tolerance)
      {
        AutoLock t_lock(perf_trace_lock);
        trace.report_trace(diff);
      }
      traces[index].pop_back();
    }

    //--------------------------------------------------------------------------
    RegionTreeForest::PerfTrace::PerfTrace(int k, unsigned long long s)
      : tracing(true), kind(k), start(s)
    //--------------------------------------------------------------------------
    {
      // Allocate space for all of the calls
      for (unsigned idx = 0; idx < NUM_CALL_KIND; idx++)
        records.push_back(CallRecord(idx));
    }

    //--------------------------------------------------------------------------
    void RegionTreeForest::PerfTrace::report_trace(unsigned long long diff)
    //--------------------------------------------------------------------------
    {
      // Print out the kind of trace 
      switch (kind)
      {
        case REGION_DEPENDENCE_ANALYSIS:
          {
            fprintf(stdout,"REGION DEPENDENCE ANALYSIS: %lld us\n",diff);
            break;
          }
        case PREMAP_PHYSICAL_REGION_ANALYSIS:
          {
            fprintf(stdout,"PREMAP PHYSICAL REGION ANALYSIS: %lld us\n",diff);
            break;
          }
        case MAP_PHYSICAL_REGION_ANALYSIS:
          {
            fprintf(stdout,"MAP PHYSICAL REGION ANALYSIS: %lld us\n",diff);
            break;
          }
        case REMAP_PHYSICAL_REGION_ANALYSIS:
          {
            fprintf(stdout,"REMAP PHYSICAL REGION ANALYSIS: %lld us\n",diff);
            break;
          }
        case REGISTER_PHYSICAL_REGION_ANALYSIS:
          {
            fprintf(stdout,"REGISTER PHYSICAL REGION ANALYSIS: %lld us\n",diff);
            break;
          }
        case COPY_ACROSS_ANALYSIS:
          {
            fprintf(stdout,"COPY ACROSS ANALYSIS: %lld us\n",diff);
            break;
          }
        case PERFORM_CLOSE_OPERATIONS_ANALYSIS:
          {
            fprintf(stdout,"PERFORM CLOSE OPERATIONS ANALYSIS: %lld us\n",diff);
            break;
          }
        default:
          assert(false);
      }
      // Record all the call records which have a non-zero call count
      // Keep them in order from largest to smallest using a simple
      // insertion sort
      std::list<unsigned> record_indexes;
      for (unsigned idx = 0; idx < records.size(); idx++)
      {
        if (records[idx].count > 0)
        {
          bool inserted = false;
          for (std::list<unsigned>::iterator it = record_indexes.begin();
                it != record_indexes.end(); it++)
          {
            if (records[idx].total_time > records[*it].total_time)
            {
              record_indexes.insert(it, idx);
              inserted = true;
              break;
            }
          }
          if (!inserted)
            record_indexes.push_back(idx);
        }
      }

      // Then print out all the records
      for (std::list<unsigned>::const_iterator it = record_indexes.begin();
            it != record_indexes.end(); it++)
      {
        // Print out the kind of call record
        const CallRecord &rec = records[*it];
        switch (rec.kind)
        {
          case CREATE_NODE_CALL:
            {
              fprintf(stdout,"  Create Node Call:\n");
              break;
            }
          case GET_NODE_CALL:
            {
              fprintf(stdout,"  Get Node Call:\n");
              break;
            }
          case ARE_DISJOINT_CALL:
            {
              fprintf(stdout,"  Are Disjoint Call:\n");
              break;
            }
          case COMPUTE_PATH_CALL:
            {
              fprintf(stdout,"  Compute Path Call:\n");
              break;
            }
          case CREATE_INSTANCE_CALL:
            {
              fprintf(stdout,"  Create Instance Call:\n");
              break;
            }
          case CREATE_REDUCTION_CALL:
            {
              fprintf(stdout,"  Create Reduction Call:\n");
              break;
            }
          case PERFORM_PREMAP_CLOSE_CALL:
            {
              fprintf(stdout,"  Perform Premap Close Call:\n");
              break;
            }
          case MAPPING_TRAVERSE_CALL:
            {
              fprintf(stdout,"  Mapping Traverse Call:\n");
              break;
            }
          case MAP_PHYSICAL_REGION_CALL:
            {
              fprintf(stdout,"  Map Physical Region Call:\n");
              break;
            }
          case MAP_REDUCTION_REGION_CALL:
            {
              fprintf(stdout,"  Map Reduction Region Call:\n");
              break;
            }
          case REGISTER_LOGICAL_NODE_CALL:
            {
              fprintf(stdout,"  Register Logical Node Call:\n");
              break;
            }
          case OPEN_LOGICAL_NODE_CALL:
            {
              fprintf(stdout,"  Open Logical Node Call:\n");
              break;
            }
          case CLOSE_LOGICAL_NODE_CALL:
            {
              fprintf(stdout,"  Close Logical Node Call:\n");
              break;
            }
          case SIPHON_LOGICAL_CHILDREN_CALL:
            {
              fprintf(stdout,"  Siphon Logical Children Call:\n");
              break;
            }
          case PERFORM_LOGICAL_CLOSE_CALL:
            {
              fprintf(stdout,"  Perform Logical Close Call:\n");
              break;
            }
          case FILTER_PREV_EPOCH_CALL:
            {
              fprintf(stdout,"  Filter Previous Epoch Call:\n");
              break;
            }
          case FILTER_CURR_EPOCH_CALL:
            {
              fprintf(stdout,"  Filter Current Epoch Call:\n");
              break;
            }
          case FILTER_CLOSE_CALL:
            {
              fprintf(stdout,"  Filter Close Call:\n");
              break;
            }
          case INITIALIZE_LOGICAL_CALL:
            {
              fprintf(stdout,"  Initialize Logical Call:\n");
              break;
            }
          case INVALIDATE_LOGICAL_CALL:
            {
              fprintf(stdout,"  Invalidate Logical Call:\n");
              break;
            }
          case REGISTER_LOGICAL_DEPS_CALL:
            {
              fprintf(stdout,"  Register Logical Dependences Call:\n");
              break;
            }
          case CLOSE_PHYSICAL_NODE_CALL:
            {
              fprintf(stdout,"  Close Physical Node Call:\n");
              break;
            }
          case SELECT_CLOSE_TARGETS_CALL:
            {
              fprintf(stdout,"  Select Close Targets Call:\n");
              break;
            }
          case SIPHON_PHYSICAL_CHILDREN_CALL:
            {
              fprintf(stdout,"  Siphon Physical Children Call:\n");
              break;
            }
          case CLOSE_PHYSICAL_CHILD_CALL:
            {
              fprintf(stdout,"  Close Physical Child Call:\n");
              break;
            }
          case FIND_VALID_INSTANCE_VIEWS_CALL:
            {
              fprintf(stdout,"  Find Valid Instance Views Call:\n");
              break;
            }
          case FIND_VALID_REDUCTION_VIEWS_CALL:
            {
              fprintf(stdout,"  Find Valid Reduction Views Call:\n");
              break;
            }
          case PULL_VALID_VIEWS_CALL:
            {
              fprintf(stdout,"  Pull Valid Views Call:\n");
              break;
            }
          case FIND_COPY_ACROSS_INSTANCES_CALL:
            {
              fprintf(stdout,"  Find Copy Across Instances Call:\n");
              break;
            }
          case ISSUE_UPDATE_COPIES_CALL:
            {
              fprintf(stdout,"  Issue Update Copies Call:\n");
              break;
            }
          case ISSUE_UPDATE_REDUCTIONS_CALL:
            {
              fprintf(stdout,"  Issue Update Reductions Call:\n");
              break;
            }
          case PERFORM_COPY_DOMAIN_CALL:
            {
              fprintf(stdout,"  Perform Copy Domain Call:\n");
              break;
            }
          case INVALIDATE_INSTANCE_VIEWS_CALL:
            {
              fprintf(stdout,"  Invalidate Instance Views Call:\n");
              break;
            }
          case INVALIDATE_REDUCTION_VIEWS_CALL:
            {
              fprintf(stdout,"  Invalidate Reduction Views Call:\n");
              break;
            }
          case UPDATE_VALID_VIEWS_CALL:
            {
              fprintf(stdout,"  Update Valid Views Call:\n");
              break;
            }
          case UPDATE_REDUCTION_VIEWS_CALL:
            {
              fprintf(stdout,"  Update Reduction Views Call:\n");
              break;
            }
          case FLUSH_REDUCTIONS_CALL:
            {
              fprintf(stdout,"  Flush Reductions Call:\n");
              break;
            }
          case INITIALIZE_PHYSICAL_STATE_CALL:
            {
              fprintf(stdout,"  Initialize Physical State Call:\n");
              break;
            }
          case INVALIDATE_PHYSICAL_STATE_CALL:
            {
              fprintf(stdout,"  Invalidate Physical State Call:\n");
              break;
            }
          case PERFORM_DEPENDENCE_CHECKS_CALL:
            {
              fprintf(stdout,"  Perform Dependence Checks Call:\n");
              break;
            }
          case PERFORM_CLOSING_CHECKS_CALL:
            {
              fprintf(stdout,"  Perform Closing Checks Call:\n");
              break;
            }
          case REMAP_REGION_CALL:
            {
              fprintf(stdout,"  Remap Region Call:\n");
              break;
            }
          case REGISTER_REGION_CALL:
            {
              fprintf(stdout,"  Register Region Call:\n");
              break;
            }
          case CLOSE_PHYSICAL_STATE_CALL:
            {
              fprintf(stdout,"  Close Physical State Call:\n");
              break;
            }
          case GARBAGE_COLLECT_CALL:
            {
              fprintf(stdout,"  Garbage Collect Call:\n");
              break;
            }
          case NOTIFY_INVALID_CALL:
            {
              fprintf(stdout,"  Notify Invalid Call:\n");
              break;
            }
          case DEFER_COLLECT_USER_CALL:
            {
              fprintf(stdout,"  Defer Collect User Call:\n");
              break;
            }
          case GET_SUBVIEW_CALL:
            {
              fprintf(stdout,"  Get Subview Call:\n");
              break;
            }
          case COPY_FIELD_CALL:
            {
              fprintf(stdout,"  Copy-Field Call:\n");
              break;
            }
          case COPY_TO_CALL:
            {
              fprintf(stdout,"  Copy-To Call:\n");
              break;
            }
          case REDUCE_TO_CALL:
            {
              fprintf(stdout,"  Reduce-To Call:\n");
              break;
            }
          case COPY_FROM_CALL:
            {
              fprintf(stdout,"  Copy-From Call:\n");
              break;
            }
          case REDUCE_FROM_CALL:
            {
              fprintf(stdout,"  Reduce-From Call:\n");
              break;
            }
          case HAS_WAR_DEPENDENCE_CALL:
            {
              fprintf(stdout,"  Has WAR Dependence Call:\n");
              break;
            }
          case ACCUMULATE_EVENTS_CALL:
            {
              fprintf(stdout,"  Accumulate Events Call:\n");
              break;
            }
          case ADD_COPY_USER_CALL:
            {
              fprintf(stdout,"  Add Copy User Call:\n");
              break;
            }
          case ADD_USER_CALL:
            {
              fprintf(stdout,"  Add User Call:\n");
              break;
            }
          case ADD_USER_ABOVE_CALL:
            {
              fprintf(stdout,"  Add User Above Call:\n");
              break;
            }
          case ADD_LOCAL_USER_CALL:
            {
              fprintf(stdout,"  Add Local User Call:\n");
              break;
            }
          case FIND_COPY_PRECONDITIONS_CALL:
            {
              fprintf(stdout,"  Find Copy Preconditions Call:\n");
              break;
            }
          case FIND_COPY_PRECONDITIONS_ABOVE_CALL:
            {
              fprintf(stdout,"  Find Copy Preconditions Above Call:\n");
              break;
            }
          case FIND_LOCAL_COPY_PRECONDITIONS_CALL:
            {
              fprintf(stdout,"  Find Local Copy Preconditions Call:\n");
              break;
            }
          case HAS_WAR_DEPENDENCE_ABOVE_CALL:
            {
              fprintf(stdout,"  Has WAR Dependence Above Call:\n");
              break;
            }
          case UPDATE_VERSIONS_CALL:
            {
              fprintf(stdout,"  Update Versions Call:\n");
              break;
            }
          case CONDENSE_USER_LIST_CALL:
            {
              fprintf(stdout,"  Condense User List Call:\n");
              break;
            }
          case PERFORM_REDUCTION_CALL:
            {
              fprintf(stdout,"  Perform Reduction Call:\n");
              break;
            }
          default:
            assert(false);
        }
        // Print out the statistics
        fprintf(stdout,"    Total calls: %d\n", rec.count);
        fprintf(stdout,"    Total time: %lld us\n", rec.total_time);
        fprintf(stdout,"    Avg time: %lld us\n", rec.total_time/rec.count);
        fprintf(stdout,"    Max time: %lld us\n", rec.max_time);
        fprintf(stdout,"    Min time: %lld us\n", rec.min_time);
      }
      fflush(stdout);
    }
#endif

    //--------------------------------------------------------------------------
    template<SemanticInfoKind KIND>
    void SendSemanticInfoFunctor<KIND>::apply(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      switch (KIND)
      {
        case INDEX_SPACE_SEMANTIC:
          {
            runtime->send_index_space_semantic_info(target, rez);
            break;
          }
        case INDEX_PARTITION_SEMANTIC:
          {
            runtime->send_index_partition_semantic_info(target, rez);
            break;
          }
        case FIELD_SPACE_SEMANTIC:
          {
            runtime->send_field_space_semantic_info(target, rez);
            break;
          }
        case FIELD_SEMANTIC:
          {
            runtime->send_field_semantic_info(target, rez);
            break;
          }
        case LOGICAL_REGION_SEMANTIC:
          {
            runtime->send_logical_region_semantic_info(target, rez);
            break;
          }
        case LOGICAL_PARTITION_SEMANTIC:
          {
            runtime->send_logical_partition_semantic_info(target, rez);
            break;
          }
        default:
          assert(false);
      }
    }
 
    /////////////////////////////////////////////////////////////
    // Index Tree Node 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    IndexTreeNode::IndexTreeNode(void)
      : depth(0), color(ColorPoint()), context(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    IndexTreeNode::IndexTreeNode(ColorPoint c, unsigned d, 
                                 RegionTreeForest *ctx)
      : depth(d), color(c), context(ctx), 
        node_lock(Reservation::create_reservation())
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(ctx != NULL);
#endif
    }

    //--------------------------------------------------------------------------
    IndexTreeNode::~IndexTreeNode(void)
    //--------------------------------------------------------------------------
    {
      node_lock.destroy_reservation();
      node_lock = Reservation::NO_RESERVATION;
      for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
            semantic_info.begin(); it != semantic_info.end(); it++)
      {
        legion_free(SEMANTIC_INFO_ALLOC, it->second.buffer, it->second.size);
      }
      for (std::map<IndexTreeNode*,IntersectInfo>::iterator it = 
            intersections.begin(); it != intersections.end(); it++)
      {
        IntersectInfo &info = it->second; 
        for (std::set<Domain>::iterator dit = info.intersections.begin();
              dit != info.intersections.end(); dit++)
        {
          LowLevel::IndexSpace space = dit->get_index_space();
          if (space.exists())
            space.destroy();
        }
      }
      intersections.clear();
    }

    //--------------------------------------------------------------------------
    void IndexTreeNode::attach_semantic_information(SemanticTag tag,
                                                    const NodeSet &mask,
                                                    const void *buffer, 
                                                    size_t size)
    //--------------------------------------------------------------------------
    {
      // Make a copy
      void *local = legion_malloc(SEMANTIC_INFO_ALLOC, size);
      memcpy(local, buffer, size);
      NodeSet diff, current;
      {
        AutoLock n_lock(node_lock); 
        // See if it already exists
        LegionMap<SemanticTag,SemanticInfo>::aligned::iterator finder = 
          semantic_info.find(tag);
        if (finder != semantic_info.end())
        {
          // Check to make sure that the bits are the same
          if (size != finder->second.size)
          {
            log_run.error("ERROR: Inconsistent Semantic Tag value "
                                "for tag %ld with different sizes of %ld"
                                " and %ld for index tree node", 
                                tag, size, finder->second.size);
#ifdef DEBUG_HIGH_LEVEL
            assert(false);
#endif
            exit(ERROR_INCONSISTENT_SEMANTIC_TAG);       
          }
          // Otherwise do a bitwise comparison
          {
            const char *orig = (const char*)finder->second.buffer;
            const char *next = (const char*)buffer;
            for (unsigned idx = 0; idx < size; idx++)
            {
              char diff = orig[idx] ^ next[idx];
              if (diff)
              {
                log_run.error("ERROR: Inconsistent Semantic Tag value "
                                    "for tag %ld with different values at"
                                    "byte %d for index tree node, %x != %x", 
                                    tag, idx, orig[idx], next[idx]);
#ifdef DEBUG_HIGH_LEVEL
                assert(false);
#endif
                exit(ERROR_INCONSISTENT_SEMANTIC_TAG);
              }
            }
          }
          finder->second.node_mask |= mask;
          diff = creation_set - finder->second.node_mask;
          current = finder->second.node_mask;
        }
        else
        {
          semantic_info[tag] = SemanticInfo(local, size, mask);
          diff = creation_set - mask;
          current = mask;
        }
      }
      if (!!diff)
        send_semantic_info(diff, tag, local, size, current);
    }

    //--------------------------------------------------------------------------
    void IndexTreeNode::retrieve_semantic_information(SemanticTag tag,
                                                      const void *&result,
                                                      size_t &size)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      LegionMap<SemanticTag,SemanticInfo>::aligned::const_iterator finder = 
        semantic_info.find(tag);
      if (finder == semantic_info.end())
      {
        log_run.error("ERROR: invalid semantic tag %ld for "
                            "index tree node", tag);   
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_SEMANTIC_TAG);
      }
      result = finder->second.buffer;
      size = finder->second.size;
    }

    //--------------------------------------------------------------------------
    /*static*/ bool IndexTreeNode::compute_intersections(
        const std::set<Domain> &left, const std::set<Domain> &right, 
        std::set<Domain> &result_domains, bool compute)
    //--------------------------------------------------------------------------
    {
      for (std::set<Domain>::const_iterator lit = left.begin();
            lit != left.end(); lit++)
      {
        for (std::set<Domain>::const_iterator rit = right.begin();
              rit != right.end(); rit++)
        {
          Domain result;
          if (compute_intersection(*lit, *rit, result, compute))
          {
            if (compute)
              result_domains.insert(result);
            else
              return true;
          }
        }
      }
      return !result_domains.empty();
    }

    //--------------------------------------------------------------------------
    /*static*/ bool IndexTreeNode::compute_intersections(
        const std::set<Domain> &left, const Domain &right, 
        std::set<Domain> &result_domains, bool compute)
    //--------------------------------------------------------------------------
    {
      for (std::set<Domain>::const_iterator it = left.begin();
            it != left.end(); it++)
      {
        Domain result;
        if (compute_intersection(*it, right, result, compute))
        {
          if (compute)
            result_domains.insert(result);
          else
            return true;
        }
      }
      return !result_domains.empty();
    }

    //--------------------------------------------------------------------------
    /*static*/ bool IndexTreeNode::compute_intersection(const Domain &left,
                                                        const Domain &right,
                                                        Domain &result,
                                                        bool compute)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(left.get_dim() == right.get_dim());
#endif
      bool non_empty = false;
      switch (left.get_dim())
      {
        case 0:
          {
            const LowLevel::ElementMask &left_mask = 
                                    left.get_index_space().get_valid_mask();
            const LowLevel::ElementMask &right_mask = 
                                    right.get_index_space().get_valid_mask();
            LowLevel::ElementMask intersection = left_mask & right_mask;
            if (!!intersection)
            {
              non_empty = true;
              if (compute)
                result = 
                 Domain(LowLevel::IndexSpace::create_index_space(intersection));
            }
            break;
          }
        case 1:
          {
            Rect<1> leftr = left.get_rect<1>();
            Rect<1> rightr = right.get_rect<1>();
            Rect<1> temp = leftr.intersection(rightr);
            if (temp.volume() > 0)
            {
              non_empty = true;
              if (compute)
                result = Domain::from_rect<1>(temp);
            }
            break;
          }
        case 2:
          {
            Rect<2> leftr = left.get_rect<2>();
            Rect<2> rightr = right.get_rect<2>();
            Rect<2> temp = leftr.intersection(rightr);
            if (temp.volume() > 0)
            {
              non_empty = true;
              if (compute)
                result = Domain::from_rect<2>(temp);
            }
            break;
          }
        case 3:
          {
            Rect<3> leftr = left.get_rect<3>();
            Rect<3> rightr = right.get_rect<3>();
            Rect<3> temp = leftr.intersection(rightr);
            if (temp.volume() > 0)
            {
              non_empty = true;
              if (compute)
                result = Domain::from_rect<3>(temp);
            }
            break;
          }
        default:
          assert(false);
      }
      return non_empty;
    }

    //--------------------------------------------------------------------------
    /*static*/ bool IndexTreeNode::compute_dominates(
            const std::set<Domain> &left_set, const std::set<Domain> &right_set)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!left_set.empty());
      assert(!right_set.empty());
#endif
      // Check to see if the left set of domains dominates the right set
      bool dominates = false;
      // Handle the easy case for dimension zero
      Domain left = *(left_set.begin());
      if (left.get_dim() == 0)
      {
        // Union left and right together and then test
        LowLevel::ElementMask left_mask, right_mask;
        bool first = true;
        for (std::set<Domain>::const_iterator it = left_set.begin();
              it != left_set.end(); it++)
        {
          if (first)
          {
            left_mask = it->get_index_space().get_valid_mask();
            first = false;
          }
          else
            left_mask |= it->get_index_space().get_valid_mask();
        }
        first = true;
        for (std::set<Domain>::const_iterator it = right_set.begin();
              it != right_set.end(); it++)
        {
          if (first)
          {
            right_mask = it->get_index_space().get_valid_mask();
            first = false;
          }
          else
            right_mask |= it->get_index_space().get_valid_mask();
        }
        LowLevel::ElementMask diff = right_mask - left_mask;
        if (!diff)
          dominates = true;
      }
      else if (left_set.size() == 1)
      {
        // This is the easy case where we only have a single domain on the left
        switch (left.get_dim())
        {
          case 1:
            {
              Rect<1> leftr = left.get_rect<1>();
              dominates = true;
              for (std::set<Domain>::const_iterator it = right_set.begin();
                    it != right_set.end(); it++)
              {
                Rect<1> right = it->get_rect<1>(); 
                if ((right.intersection(leftr)) != right)
                {
                  dominates = false;
                  break;
                }
              }
              break;
            }
          case 2:
            {
              Rect<2> leftr = left.get_rect<2>();
              dominates = true;
              for (std::set<Domain>::const_iterator it = right_set.begin();
                    it != right_set.end(); it++)
              {
                Rect<2> right = it->get_rect<2>(); 
                if ((right.intersection(leftr)) != right)
                {
                  dominates = false;
                  break;
                }
              }
              break;
            }
          case 3:
            {
              Rect<3> leftr = left.get_rect<3>();
              dominates = true;
              for (std::set<Domain>::const_iterator it = right_set.begin();
                    it != right_set.end(); it++)
              {
                Rect<3> right = it->get_rect<3>(); 
                if ((right.intersection(leftr)) != right)
                {
                  dominates = false;
                  break;
                }
              }
              break;
            }
          default:
            assert(false);
        }
      }
      else
      {
        // This is the hard case where we have multiple domains on the left
        switch (left.get_dim())
        {
          case 1:
            {
              // Construct an interval tree for the left set
              // and then check to see if all the intervals within
              // the right set are dominated by an interval in the tree
              IntervalTree<int,true/*discrete*/> intervals;
              for (std::set<Domain>::const_iterator it = left_set.begin();
                    it != left_set.end(); it++)
              {
                Rect<1> left_rect = it->get_rect<1>();
                intervals.insert(left_rect.lo[0], left_rect.hi[0]);
              }
              dominates = true;
              for (std::set<Domain>::const_iterator it = right_set.begin();
                    it != right_set.end(); it++)
              {
                Rect<1> right_rect = it->get_rect<1>();
                if (!intervals.dominates(right_rect.lo[0], right_rect.hi[0]))
                {
                  dominates = false;
                  break;
                }
              }
              break;
            }
          case 2:
            {
              RectangleSet<int,true/*discrete*/> rectangles;
              for (std::set<Domain>::const_iterator it = left_set.begin();
                    it != left_set.end(); it++)
              {
                Rect<2> left_rect = it->get_rect<2>();
                rectangles.add_rectangle(left_rect.lo[0], left_rect.lo[1],
                                         left_rect.hi[0], left_rect.hi[1]);
              }
              dominates = true;
              for (std::set<Domain>::const_iterator it = right_set.begin();
                    it != right_set.end(); it++)
              {
                Rect<2> right_rect = it->get_rect<2>();
                if (!rectangles.covers(right_rect.lo[0], right_rect.lo[1],
                                       right_rect.hi[0], right_rect.hi[1]))
                {
                  dominates = false;
                  break;
                }
              }
              break;
            }
          case 3:
            {
              // TODO: Improve this terrible approximation
              dominates = true;
              for (std::set<Domain>::const_iterator rit = right_set.begin();
                    (rit != right_set.end()) && dominates; rit++)
              {
                Rect<3> right_rect = rit->get_rect<3>();
                bool has_dominator = false;
                // See if any of the rectangles on the left dominate it
                for (std::set<Domain>::const_iterator lit = left_set.begin();
                      lit != left_set.end(); lit++)
                {
                  Rect<3> left_rect = lit->get_rect<3>();
                  if (right_rect.intersection(left_rect) == right_rect)
                  {
                    has_dominator = true;
                    break;
                  }
                }
                if (!has_dominator)
                {
                  dominates = false;
                  break;
                }
              }
              break;
            }
          default:
            assert(false); // should never get here
        }
      }
      return dominates;
    }


    /////////////////////////////////////////////////////////////
    // Index Space Node 
    /////////////////////////////////////////////////////////////
    
    //--------------------------------------------------------------------------
    IndexSpaceNode::IndexSpaceNode(IndexSpace h, const Domain &d, 
                                   IndexPartNode *par, ColorPoint c,
                                   IndexSpaceKind k, AllocateMode m,
                                   RegionTreeForest *ctx)
      : IndexTreeNode(c, (par == NULL) ? 0 : par->depth+1, ctx),
        handle(h), parent(par), kind(k), mode(m), handle_ready(Event::NO_EVENT),
        domain_ready(Event::NO_EVENT), domain(d),  allocator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode::IndexSpaceNode(IndexSpace h, const Domain &d, Event r, 
                                   IndexPartNode *par, ColorPoint c, 
                                   IndexSpaceKind k, AllocateMode m, 
                                   RegionTreeForest *ctx)
      : IndexTreeNode(c, (par == NULL) ? 0 : par->depth+1, ctx),
        handle(h), parent(par), kind(k), mode(m), handle_ready(Event::NO_EVENT),
        domain_ready(r), domain(d), allocator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode::IndexSpaceNode(IndexSpace h, Event h_ready, Event d_ready,
                                   IndexPartNode *par, ColorPoint c,
                                   IndexSpaceKind k, AllocateMode m,
                                   RegionTreeForest *ctx)
      : IndexTreeNode(c, (par == NULL) ? 0 : par->depth+1, ctx),
        handle(h), parent(par), kind(k), mode(m), handle_ready(h_ready),
        domain_ready(d_ready), allocator(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode::IndexSpaceNode(const IndexSpaceNode &rhs)
      : IndexTreeNode(), handle(IndexSpace::NO_SPACE), parent(NULL), 
        kind(rhs.kind), mode(rhs.mode), handle_ready(Event::NO_EVENT), 
        domain_ready(Event::NO_EVENT), domain(Domain::NO_DOMAIN),allocator(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode::~IndexSpaceNode(void)
    //--------------------------------------------------------------------------
    {
      if (allocator != NULL)
      {
        free(allocator);
        allocator = NULL;
      }
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode& IndexSpaceNode::operator=(const IndexSpaceNode &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* IndexSpaceNode::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<IndexSpaceNode,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::is_index_space_node(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* IndexSpaceNode::as_index_space_node(void)
    //--------------------------------------------------------------------------
    {
      return this;
    }

    //--------------------------------------------------------------------------
    IndexPartNode* IndexSpaceNode::as_index_part_node(void)
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    IndexTreeNode* IndexSpaceNode::get_parent(void) const
    //--------------------------------------------------------------------------
    {
      return parent;
    }

    //--------------------------------------------------------------------------
    size_t IndexSpaceNode::get_num_elmts(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(kind == UNSTRUCTURED_KIND);
#endif
      if (parent != NULL)
        return parent->get_num_elmts();
      const Domain &dom = get_domain_blocking();
      return dom.get_index_space().get_valid_mask().get_num_elmts();
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::send_semantic_info(const NodeSet &targets,
                                            SemanticTag tag,
                                            const void *buffer, size_t size,
                                            const NodeSet &current)
    //--------------------------------------------------------------------------
    {
      // Package up the message first
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(handle);
        rez.serialize(tag);
        rez.serialize(current);
        rez.serialize(size);
        rez.serialize(buffer, size);
      }
      // Then send the messages
      SendSemanticInfoFunctor<INDEX_SPACE_SEMANTIC> 
                                  functor(context->runtime, rez);
      targets.map(functor);
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexSpaceNode::handle_semantic_info(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      IndexSpace handle;
      derez.deserialize(handle);
      SemanticTag tag;
      derez.deserialize(tag);
      NodeSet sources;
      derez.deserialize(sources);
      size_t size;
      derez.deserialize(size);
      const void *buffer = derez.get_current_pointer();
      derez.advance_pointer(size);
      forest->attach_semantic_information(handle, tag, sources, buffer, size);
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::has_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      std::map<ColorPoint,IndexPartNode*>::const_iterator finder = 
        color_map.find(c);
      return ((finder != color_map.end()) && (finder->second != NULL));
    }

    //--------------------------------------------------------------------------
    IndexPartNode* IndexSpaceNode::get_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
#ifdef DEBUG_HIGH_LEVEL
      assert((color_map.find(c) != color_map.end()) &&
             (color_map[c] != NULL));
#endif
      return color_map[c];
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::add_child(IndexPartNode *child)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      // Can have a NULL pointer
      assert((color_map.find(child->color) == color_map.end()) ||
             (color_map[child->color] == NULL));
#endif
      color_map[child->color] = child;
      valid_map[child->color] = child;
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::remove_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      valid_map.erase(c);
    }

    //--------------------------------------------------------------------------
    size_t IndexSpaceNode::get_num_children(void) const
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      return valid_map.size();
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::get_children(
                                  std::map<ColorPoint,IndexPartNode*> &children)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclsuve*/);
      children = color_map;
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::get_child_colors(std::set<ColorPoint> &child_colors,
                                          bool only_valid)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      if (only_valid)
      {
        for (std::map<ColorPoint,IndexPartNode*>::const_iterator it = 
              valid_map.begin(); it != valid_map.end(); it++)
          child_colors.insert(it->first);
      }
      else
      {
        for (std::map<ColorPoint,IndexPartNode*>::const_iterator it = 
              color_map.begin(); it != color_map.end(); it++)
          child_colors.insert(it->first);
      }
    }

    //--------------------------------------------------------------------------
    Event IndexSpaceNode::get_domain_precondition(void)
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      return domain_ready;
    }

    //--------------------------------------------------------------------------
    const Domain& IndexSpaceNode::get_domain_blocking(void)
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      if (!domain_ready.has_triggered())
        domain_ready.wait();
      return domain;
    }

    //--------------------------------------------------------------------------
    const Domain& IndexSpaceNode::get_domain(Event &precondition)
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      precondition = domain_ready;
      return domain;
    }

    //--------------------------------------------------------------------------
    const Domain& IndexSpaceNode::get_domain_no_wait(void)
    //--------------------------------------------------------------------------
    {
      // We still need to wait for the handle to be valid
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      return domain;
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::set_domain(const Domain &dom)
    //--------------------------------------------------------------------------
    {
      domain = dom;
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::get_domains_blocking(std::vector<Domain> &domains) 
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      if (!domain_ready.has_triggered())
        domain_ready.wait();
      if (has_component_domains())
      {
        domains.insert(domains.end(), 
                       component_domains.begin(), component_domains.end());
      }
      else
        domains.push_back(domain);
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::get_domains(std::vector<Domain> &domains, 
                                     Event &precondition)
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      precondition = domain_ready;
      if (has_component_domains())
      {
        domains.insert(domains.end(), 
                       component_domains.begin(), component_domains.end());
      }
      else
        domains.push_back(domain);
    }

    //--------------------------------------------------------------------------
    size_t IndexSpaceNode::get_domain_volume(bool app_query)
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
      {
        if (app_query)
        {
          Processor current_proc = Processor::get_executing_processor();
          context->runtime->pre_wait(current_proc);
          handle_ready.wait();
          context->runtime->post_wait(current_proc);
        }
        else
          handle_ready.wait();
      }
      if (!domain_ready.has_triggered())
      {
        if (app_query)
        {
          Processor current_proc = Processor::get_executing_processor();
          context->runtime->pre_wait(current_proc);
          domain_ready.wait();
          context->runtime->post_wait(current_proc);
        }
        else
          domain_ready.wait();
      }
      if (domain.get_dim() == 0)
      {
        const LowLevel::ElementMask &mask = 
                                  domain.get_index_space().get_valid_mask();
        return mask.get_num_elmts();
      }
      else
        return domain.get_volume();
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::are_disjoint(const ColorPoint &c1, 
                                      const ColorPoint &c2)
    //--------------------------------------------------------------------------
    {
      // Quick out
      if (c1 == c2)
        return false;
      // Do the test with read-only mode first
      Event ready = Event::NO_EVENT;
      bool issue_dynamic_test = false;
      std::pair<ColorPoint,ColorPoint> key(c1,c2);
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        if (disjoint_subsets.find(key) != disjoint_subsets.end())
          return true;
        else if (aliased_subsets.find(key) != aliased_subsets.end())
          return false;
        else
        {
          std::map<std::pair<ColorPoint,ColorPoint>,Event>::const_iterator
            finder = pending_tests.find(key);
          if (finder != pending_tests.end())
            ready = finder->second;
          else
            issue_dynamic_test = true;
        }
      }
      if (issue_dynamic_test)
      {
        IndexPartNode *left = get_child(c1);
        IndexPartNode *right = get_child(c2);
        std::set<Event> preconditions; 
        left->get_subspace_domain_preconditions(preconditions);
        right->get_subspace_domain_preconditions(preconditions);
        AutoLock n_lock(node_lock);
        // Test again to make sure we didn't lose the race
        std::map<std::pair<ColorPoint,ColorPoint>,Event>::const_iterator
          finder = pending_tests.find(key);
        if (finder == pending_tests.end())
        {
          DynamicIndependenceArgs args;
          args.hlr_id = HLR_PART_INDEPENDENCE_TASK_ID;
          args.parent = this;
          args.left = left;
          args.right = right;
          // Get the preconditions for domains 
          Event pre = Event::merge_events(preconditions);
          ready = context->runtime->issue_runtime_meta_task(&args, sizeof(args),
                                      HLR_PART_INDEPENDENCE_TASK_ID, NULL, pre);
          pending_tests[key] = ready;
          pending_tests[std::pair<ColorPoint,ColorPoint>(c2,c1)] = ready;
        }
        else
          ready = finder->second;
      }
      // Wait for the ready event and then get the result
      ready.wait();
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      if (disjoint_subsets.find(key) != disjoint_subsets.end())
        return true;
      else
        return false;
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::record_disjointness(bool disjoint, 
                                     const ColorPoint &c1, const ColorPoint &c2)
    //--------------------------------------------------------------------------
    {
      if (c1 == c2)
        return;
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(color_map.find(c1) != color_map.end());
      assert(color_map.find(c2) != color_map.end());
#endif
      if (disjoint)
      {
        disjoint_subsets.insert(std::pair<ColorPoint,ColorPoint>(c1,c2));
        disjoint_subsets.insert(std::pair<ColorPoint,ColorPoint>(c2,c1));
      }
      else
      {
        aliased_subsets.insert(std::pair<ColorPoint,ColorPoint>(c1,c2));
        aliased_subsets.insert(std::pair<ColorPoint,ColorPoint>(c2,c1));
      }
      pending_tests.erase(std::pair<ColorPoint,ColorPoint>(c1,c2));
      pending_tests.erase(std::pair<ColorPoint,ColorPoint>(c2,c1));
    }

    //--------------------------------------------------------------------------
    Color IndexSpaceNode::generate_color(void)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      Color result;
      if (!color_map.empty())
      {
        unsigned stride = context->runtime->get_color_modulus();
        std::map<ColorPoint,IndexPartNode*>::const_reverse_iterator rlast = 
                                                        color_map.rbegin();
#ifdef DEBUG_HIGH_LEVEL
        assert(rlast->first.get_dim() == 1);
#endif
        // We know all colors for index spaces are 0-D
        result = rlast->first[0] + stride;
      }
      else
        result = context->runtime->get_start_color();
      ColorPoint color(result);
#ifdef DEBUG_HIGH_LEVEL
      assert(color_map.find(color) == color_map.end());
#endif
      // We have to put ourselves in the map to be sound for other parallel
      // allocations of colors which may come later
      color_map[color] = NULL; /* just put in a NULL pointer for now */
      return result;
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::get_colors(std::set<ColorPoint> &colors)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::map<ColorPoint,IndexPartNode*>::const_iterator it = 
            valid_map.begin(); it != valid_map.end(); it++)
      {
        // Can be NULL in some cases of parallel partitioning
        if (it->second != NULL)
          colors.insert(it->first);
      }
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::add_instance(RegionNode *inst)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(logical_nodes.find(inst) == logical_nodes.end());
#endif
      logical_nodes.insert(inst);
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::has_instance(RegionTreeID tid)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::set<RegionNode*>::const_iterator it = logical_nodes.begin();
            it != logical_nodes.end(); it++)
      {
        if ((*it)->handle.get_tree_id() == tid)
          return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::add_creation_source(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      creation_set.add(source);
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::destroy_node(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        parent->remove_child(color);
      AutoLock n_lock(node_lock);
      destruction_set.add(source);
      for (std::set<RegionNode*>::const_iterator it = logical_nodes.begin();
            it != logical_nodes.end(); it++)
      {
        (*it)->destroy_node(source);
      }
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::has_component_domains(void) const
    //--------------------------------------------------------------------------
    {
      return !component_domains.empty();
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::update_component_domains(const std::set<Domain> &doms)
    //--------------------------------------------------------------------------
    {
      component_domains.insert(doms.begin(), doms.end());
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& IndexSpaceNode::get_component_domains_blocking(
                                                                     void) const
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      if (!domain_ready.has_triggered())
        domain_ready.wait();
      return component_domains;
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& IndexSpaceNode::get_component_domains(
                                                             Event &ready) const
    //--------------------------------------------------------------------------
    {
      if (!handle_ready.has_triggered())
        handle_ready.wait();
      ready = domain_ready;
      return component_domains;
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::intersects_with(IndexSpaceNode *other, bool compute)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        // Only return the value if we either didn't want to compute
        // or we already have valid intersections
        if ((finder != intersections.end()) && 
            (!compute || finder->second.intersections_valid))
          return finder->second.has_intersects;
      }
      std::set<Domain> intersect;
      bool result;
      if (component_domains.empty())
      { 
        if (other->has_component_domains())
          result = compute_intersections(
                                   other->get_component_domains_blocking(),
                                   get_domain_blocking(), intersect, compute);
        else
        {
          Domain inter;
          result = compute_intersection(get_domain_blocking(), 
                                        other->get_domain_blocking(),
                                        inter, compute);
          if (result)
            intersect.insert(inter);
        }
      }
      else
      {
        if (other->has_component_domains())
          result = compute_intersections(component_domains,
                  other->get_component_domains_blocking(), intersect, compute);
        else
          result = compute_intersections(component_domains,
                                         other->get_domain_blocking(), 
                                         intersect, compute); 
      }
      AutoLock n_lock(node_lock);
      if (result)
      {
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        // Check to make sure we didn't lose the race
        if ((finder == intersections.end()) || 
            (compute && !finder->second.intersections_valid))
        {
          if (compute)
            intersections[other] = IntersectInfo(intersect);
          else
            intersections[other] = IntersectInfo(true/*result*/);
        }
      }
      else
        intersections[other] = IntersectInfo(false/*result*/);
      return result;
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::intersects_with(IndexPartNode *other, bool compute)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        // Only return the value if we know we are valid and we didn't
        // want to compute anything or we already did compute it
        if ((finder != intersections.end()) &&
            (!compute || finder->second.intersections_valid))
          return finder->second.has_intersects;
      }
      // Build up the set of domains for the partition
      std::set<Domain> other_domains, intersect;
      other->get_subspace_domains(other_domains);
      bool result;
      if (component_domains.empty())
      {
        result = compute_intersections(other_domains, get_domain_blocking(), 
                                       intersect, compute);
      }
      else
      {
        result = compute_intersections(component_domains, other_domains,
                                       intersect, compute);
      }
      AutoLock n_lock(node_lock);
      if (result)
      {
        // Check to make sure we didn't lose the race
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder == intersections.end()) ||
            (compute && !finder->second.intersections_valid))
        {
          if (compute)
            intersections[other] = IntersectInfo(intersect);
          else
            intersections[other] = IntersectInfo(true/*result*/);
        }
      }
      else
        intersections[other] = IntersectInfo(false/*result*/);
      return result;
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& IndexSpaceNode::get_intersection_domains(
                                                          IndexSpaceNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder != intersections.end()) &&
            finder->second.intersections_valid)
          return finder->second.intersections;
      }
      std::set<Domain> intersect;
      bool result;
      if (component_domains.empty())
      { 
        if (other->has_component_domains())
          result = compute_intersections(
                    other->get_component_domains_blocking(),
                    get_domain_blocking(), intersect, true/*compute*/);
        else
        {
          Domain inter;
          result = compute_intersection(get_domain_blocking(), 
                                        other->get_domain_blocking(), 
                                        inter, true/*compute*/);
          if (result)
            intersect.insert(inter);
        }
      }
      else
      {
        if (other->has_component_domains())
          result = compute_intersections(component_domains,
                  other->get_component_domains_blocking(), 
                  intersect, true/*compute*/);
        else
          result = compute_intersections(component_domains,
                                         other->get_domain_blocking(), 
                                         intersect, true/*compute*/); 
      }
      AutoLock n_lock(node_lock);
      if (result)
      {
        // Check again to make sure we didn't lose the race
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder == intersections.end()) ||
            !finder->second.intersections_valid)
          intersections[other] = IntersectInfo(intersect);
      }
      else
        intersections[other] = IntersectInfo(false/*result*/);
      return intersections[other].intersections;
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& IndexSpaceNode::get_intersection_domains(
                                                           IndexPartNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder != intersections.end()) &&
            finder->second.intersections_valid)
          return finder->second.intersections;
      }
      // Build up the set of domains for the partition
      std::set<Domain> other_domains, intersect;
      other->get_subspace_domains(other_domains);
      bool result;
      if (component_domains.empty())
      {
        result = compute_intersections(other_domains, get_domain_blocking(), 
                                       intersect, true/*compute*/);
      }
      else
      {
        result = compute_intersections(component_domains, other_domains,
                                       intersect, true/*compute*/);
      }
      AutoLock n_lock(node_lock);
      if (result)
      {
        // Check again to make sure we didn't lose the race
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder == intersections.end()) ||
            !finder->second.intersections_valid)
          intersections[other] = IntersectInfo(intersect);
      }
      else
        intersections[other] = IntersectInfo(false/*result*/);
      return intersections[other].intersections;
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::dominates(IndexSpaceNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,bool>::const_iterator finder = 
          dominators.find(other);
        if (finder != dominators.end())
          return finder->second;
      }
      bool result;
      if (component_domains.empty())
      {
        if (other->has_component_domains())
        {
          std::set<Domain> local;
          local.insert(get_domain_blocking());
          result = compute_dominates(local, 
                                     other->get_component_domains_blocking());
        }
        else
        {
          std::set<Domain> left, right;
          left.insert(get_domain_blocking());
          right.insert(other->get_domain_blocking());
          result = compute_dominates(left, right);
        }
      }
      else
      {
        if (other->has_component_domains())
          result = compute_dominates(component_domains,   
                                     other->get_component_domains_blocking()); 
        else
        {
          std::set<Domain> other_doms;
          other_doms.insert(other->get_domain_blocking());
          result = compute_dominates(component_domains, other_doms);
        }
      }
      AutoLock n_lock(node_lock);
      dominators[other] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    bool IndexSpaceNode::dominates(IndexPartNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,bool>::const_iterator finder = 
          dominators.find(other);
        if (finder != dominators.end())
          return finder->second;
      }
      bool result;
      std::set<Domain> other_doms;
      other->get_subspace_domains(other_doms);
      if (component_domains.empty())
      {
        std::set<Domain> local;
        local.insert(get_domain_blocking());
        result = compute_dominates(local, other_doms);
      }
      else
        result = compute_dominates(component_domains, other_doms);
      AutoLock n_lock(node_lock);
      dominators[other] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    Event IndexSpaceNode::create_subspaces_by_field(
                        const std::vector<FieldDataDescriptor> &field_data,
                        std::map<DomainPoint, LowLevel::IndexSpace> &subspaces,
                        bool mutable_results, Event precondition)
    //--------------------------------------------------------------------------
    {
      Event dom_precondition;
      const Domain &dom = get_domain(dom_precondition);
      return dom.get_index_space().create_subspaces_by_field(field_data,
                                     subspaces, mutable_results, 
                                     Event::merge_events(precondition,
                                                     dom_precondition));
    }

    //--------------------------------------------------------------------------
    Event IndexSpaceNode::create_subspaces_by_image(
                const std::vector<FieldDataDescriptor> &field_data,
                std::map<LowLevel::IndexSpace, LowLevel::IndexSpace> &subspaces,
                bool mutable_results, Event precondition)
    //--------------------------------------------------------------------------
    {
      Event dom_precondition;
      const Domain &dom = get_domain(dom_precondition);
      return dom.get_index_space().create_subspaces_by_image(field_data,
                                     subspaces, mutable_results, 
                                     Event::merge_events(precondition,
                                                     dom_precondition));
    }

    //--------------------------------------------------------------------------
    Event IndexSpaceNode::create_subspaces_by_preimage(
                const std::vector<FieldDataDescriptor> &field_data,
                std::map<LowLevel::IndexSpace, LowLevel::IndexSpace> &subspaces,
                bool mutable_results, Event precondition)
    //--------------------------------------------------------------------------
    {
      Event dom_precondition;
      const Domain &dom = get_domain(dom_precondition);
      return dom.get_index_space().create_subspaces_by_preimage(field_data,
                                     subspaces, mutable_results, 
                                     Event::merge_events(precondition,
                                                     dom_precondition));
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexSpaceNode::handle_disjointness_test(
              IndexSpaceNode *parent, IndexPartNode *left, IndexPartNode *right)
    //--------------------------------------------------------------------------
    {
      std::map<ColorPoint,IndexSpaceNode*> left_spaces, right_spaces;    
      left->get_children(left_spaces);
      right->get_children(right_spaces);
      bool disjoint = true;
      for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator lit = 
            left_spaces.begin(); disjoint && (lit != left_spaces.end()); lit++)
      {
        for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator rit = 
              right_spaces.begin(); disjoint && 
              (rit != right_spaces.end()); rit++)
        {
          if (!RegionTreeForest::are_disjoint(lit->second, rit->second))
            disjoint = false;
        }
      }
      parent->record_disjointness(disjoint, left->color, right->color);
    }

    //--------------------------------------------------------------------------
    void IndexSpaceNode::send_node(AddressSpaceID target, bool up, bool down)
    //--------------------------------------------------------------------------
    {
      // Go up first so we know those nodes will be there
      if (up && (parent != NULL))
        parent->send_node(target, true/*up*/, false/*down*/);
      // Check to see if we need to wait for the handle event to be ready
      if (!handle_ready.has_triggered())
          handle_ready.wait();
      // Check to see if our creation set includes the target
      std::map<ColorPoint,IndexPartNode*> valid_copy;
      {
        AutoLock n_lock(node_lock);
        if (!creation_set.contains(target))
        {
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(handle);
            rez.serialize(domain);
            rez.serialize(domain_ready);
            rez.serialize(kind);
            rez.serialize(mode);
            if (parent != NULL)
              rez.serialize(parent->handle);
            else
              rez.serialize(IndexPartition::NO_PART);
            rez.serialize(color);
            rez.serialize(component_domains.size());
            for (std::set<Domain>::const_iterator it = 
                  component_domains.begin(); it != 
                  component_domains.end(); it++)
            {
              rez.serialize(*it);
            }
            rez.serialize<size_t>(semantic_info.size());
            for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
                  semantic_info.begin(); it != semantic_info.end(); it++)
            {
              it->second.node_mask.add(target);
              rez.serialize(it->first);
              rez.serialize(it->second.node_mask);
              rez.serialize(it->second.size);
              rez.serialize(it->second.buffer, it->second.size);
            }
          }
          context->runtime->send_index_space_node(target, rez); 
          creation_set.add(target);
        }
        // Also check to see if we need to go down
        if (down && child_creation.contains(target))
          down = false;
        if (!destruction_set.contains(target))
        {
          // Now we need to send a destruction
          context->runtime->send_index_space_destruction(handle, target);
          destruction_set.add(target);
        }
        // If we need to go down, make a copy of the valid children
        if (down)
          valid_copy = valid_map;
      }
      if (down)
      {
        for (std::map<ColorPoint,IndexPartNode*>::const_iterator it = 
              valid_copy.begin(); it != valid_copy.end(); it++)
        {
          it->second->send_node(target, false/*up*/, true/*down*/);
        }
        // If we sent all our children, then we can record it
        AutoLock n_lock(node_lock);
        child_creation.add(target);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexSpaceNode::handle_node_creation(
        RegionTreeForest *context, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      IndexSpace handle;
      derez.deserialize(handle);
      Domain domain;
      derez.deserialize(domain);
      Event ready_event;
      derez.deserialize(ready_event);
      IndexSpaceKind kind;
      derez.deserialize(kind);
      AllocateMode mode;
      derez.deserialize(mode);
      IndexPartition parent;
      derez.deserialize(parent);
      ColorPoint color;
      derez.deserialize(color);
      size_t components;
      derez.deserialize(components);
      std::set<Domain> component_domains;
      for (unsigned idx = 0; idx < components; idx++)
      {
        Domain component;
        derez.deserialize(component);
        component_domains.insert(component);
      }
      IndexPartNode *parent_node = NULL;
      if (parent != IndexPartition::NO_PART)
      {
        parent_node = context->get_node(parent);
#ifdef DEBUG_HIGH_LEVEL
        assert(parent_node != NULL);
#endif
      }
      IndexSpaceNode *node = 
                  context->create_node(handle, domain, ready_event, 
                                       parent_node, color,
                                       kind, mode);
#ifdef DEBUG_HIGH_LEVEL
      assert(node != NULL);
#endif
      node->add_creation_source(source);
      if (components > 0)
        node->update_component_domains(component_domains);
      size_t num_semantic;
      derez.deserialize(num_semantic);
      for (unsigned idx = 0; idx < num_semantic; idx++)
      {
        SemanticTag tag;
        derez.deserialize(tag);
        NodeSet source_mask;
        derez.deserialize(source_mask);
        size_t buffer_size;
        derez.deserialize(buffer_size);
        const void *buffer = derez.get_current_pointer();
        derez.advance_pointer(buffer_size);
        node->attach_semantic_information(tag, source_mask, 
                                          buffer, buffer_size);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexSpaceNode::handle_node_request(
           RegionTreeForest *forest, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      IndexSpace handle;
      derez.deserialize(handle);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      IndexSpaceNode *target = forest->get_node(handle);
      target->send_node(source, true/*up*/, true/*down*/);
      // Then send back the flush
      Serializer rez;
      rez.serialize(to_trigger);
      forest->runtime->send_index_space_return(source, rez);
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexSpaceNode::handle_node_return(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      to_trigger.trigger();
    }

    //--------------------------------------------------------------------------
    IndexSpaceAllocator* IndexSpaceNode::get_allocator(void)
    //--------------------------------------------------------------------------
    {
      if (kind != UNSTRUCTURED_KIND)
      {
        log_run.error("Illegal request for an allocator on a structured "
                      "index space! Only unstructured index spaces are "
                      "permitted to have allocators.");
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_ILLEGAL_ALLOCATOR_REQUEST);
      }
      if (allocator == NULL)
      {
        AutoLock n_lock(node_lock);
        if (allocator == NULL)
        {
          allocator = (IndexSpaceAllocator*)malloc(sizeof(IndexSpaceAllocator));
          const Domain &dom = get_domain_blocking();
          *allocator = dom.get_index_space().create_allocator();
        }
      }
      return allocator;
    }

    /////////////////////////////////////////////////////////////
    // Index Partition Node 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    IndexPartNode::IndexPartNode(IndexPartition p, IndexSpaceNode *par,
                                 ColorPoint c, Domain cspace,
                                 bool dis, AllocateMode m,
                                 RegionTreeForest *ctx)
      : IndexTreeNode(c, par->depth+1, ctx), handle(p), color_space(cspace),
        mode(m), parent(par), disjoint(dis), disjoint_ready(Event::NO_EVENT), 
        has_complete(false)
    //--------------------------------------------------------------------------
    { 
    }

    //--------------------------------------------------------------------------
    IndexPartNode::IndexPartNode(IndexPartition p, IndexSpaceNode *par,
                                 ColorPoint c, Domain cspace,
                                 Event ready, AllocateMode m,
                                 RegionTreeForest *ctx)
      : IndexTreeNode(c, par->depth+1, ctx), handle(p), color_space(cspace),
        mode(m), parent(par), disjoint(false), disjoint_ready(ready), 
        has_complete(false)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    IndexPartNode::IndexPartNode(const IndexPartNode &rhs)
      : IndexTreeNode(), handle(IndexPartition::NO_PART), 
        color_space(Domain::NO_DOMAIN), mode(NO_MEMORY), 
        parent(NULL), disjoint(false), has_complete(false)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    IndexPartNode::~IndexPartNode(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    IndexPartNode& IndexPartNode::operator=(const IndexPartNode &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* IndexPartNode::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<IndexPartNode,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::is_index_space_node(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* IndexPartNode::as_index_space_node(void)
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    IndexPartNode* IndexPartNode::as_index_part_node(void)
    //--------------------------------------------------------------------------
    {
      return this;
    }

    //--------------------------------------------------------------------------
    IndexTreeNode* IndexPartNode::get_parent(void) const
    //--------------------------------------------------------------------------
    {
      return parent;
    }

    //--------------------------------------------------------------------------
    size_t IndexPartNode::get_num_elmts(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return parent->get_num_elmts();
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::send_semantic_info(const NodeSet &targets, 
                                           SemanticTag tag, const void *buffer,
                                           size_t size, const NodeSet &current)
    //--------------------------------------------------------------------------
    {
      // Package up the message first
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(handle);
        rez.serialize(tag);
        rez.serialize(current);
        rez.serialize(size);
        rez.serialize(buffer, size);
      }
      // Then send the messages
      SendSemanticInfoFunctor<INDEX_PARTITION_SEMANTIC>
                                    functor(context->runtime, rez);
      targets.map(functor);
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexPartNode::handle_semantic_info(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      IndexPartition handle;
      derez.deserialize(handle);
      SemanticTag tag;
      derez.deserialize(tag);
      NodeSet sources;
      derez.deserialize(sources);
      size_t size;
      derez.deserialize(size);
      const void *buffer = derez.get_current_pointer();
      derez.advance_pointer(size);
      forest->attach_semantic_information(handle, tag, sources, buffer, size);
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::has_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      return (color_map.find(c) != color_map.end());
    }

    //--------------------------------------------------------------------------
    IndexSpaceNode* IndexPartNode::get_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // First check to see if we can find it
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/); 
        std::map<ColorPoint,IndexSpaceNode*>::const_iterator finder = 
          color_map.find(c);
        if (finder != color_map.end())
          return finder->second;
      }
#ifdef DEBUG_HIGH_LEVEL
      if (!color_space.contains(c.get_point()))
      {
        log_index.error("Invalid color for index subspace!");
        assert(false);
        exit(ERROR_INVALID_INDEX_PART_COLOR);
      }
#endif
      // Didn't find it so now we try to make it.
      // Make a unique handle name
      IndexSpace is(context->runtime->get_unique_index_space_id(),
                    handle.get_tree_id());
      if (parent->kind == UNSTRUCTURED_KIND)
      {
        // Make a new sub-index space first based on the 
        // parent. Determine if it is allocable based on the
        // properties of this partition object.
        const Domain &parent_dom = parent->get_domain_no_wait();
        LowLevel::IndexSpace parent_space = parent_dom.get_index_space();
        LowLevel::ElementMask new_mask(get_num_elmts());
        LowLevel::IndexSpace new_space =  
          LowLevel::IndexSpace::create_index_space(parent_space, new_mask,
                                                   (mode & ALLOCABLE));
        IndexSpaceNode *result = context->create_node(is, Domain(new_space),
            this, c, UNSTRUCTURED_KIND, mode);
        return result;
      }
      else
      {
        // Easy case just make an empty domain and use that
        Domain empty = Domain::NO_DOMAIN;
        IndexSpaceNode *result = context->create_node(is, empty,
            this, c, DENSE_ARRAY_KIND, parent->mode);
        return result;
      }
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::add_child(IndexSpaceNode *child)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(color_map.find(child->color) == color_map.end());
#endif
      color_map[child->color] = child;
      valid_map[child->color] = child;
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::remove_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      valid_map.erase(c);
      // Mark that any completeness computations we've done are no longer valid
      has_complete = false;
    }

    //--------------------------------------------------------------------------
    size_t IndexPartNode::get_num_children(void) const
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      return valid_map.size();
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::get_children(
                                 std::map<ColorPoint,IndexSpaceNode*> &children)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      children = color_map;
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::compute_disjointness(UserEvent ready_event)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(disjoint_ready.exists() && !disjoint_ready.has_triggered());
      assert(ready_event == disjoint_ready);
#endif
      // Make a copy of our color map 
      std::set<ColorPoint> current_colors;
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator it = 
              color_map.begin(); it != color_map.end(); it++)
          current_colors.insert(it->first);
      }
      // Now do the pairwise disjointness tests
      disjoint = true;
      for (std::set<ColorPoint>::const_iterator it1 = current_colors.begin();
            disjoint && (it1 != current_colors.end()); it1++)
      {
        for (std::set<ColorPoint>::const_iterator it2 = it1;
              disjoint && (it2 != current_colors.end()); it2++)
        {
          if ((*it1) == (*it2))
            continue;
          if (!are_disjoint(*it1, *it2, true/*force compute*/))
            disjoint = false;
        }
      }
      // Once we get here, we know the disjointness result so we can
      // trigger the event saying when the disjointness value is ready
      ready_event.trigger();
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::is_disjoint(bool app_query)
    //--------------------------------------------------------------------------
    {
      if (!disjoint_ready.has_triggered())
      {
        if (app_query)
        {
          Processor current_proc = Processor::get_executing_processor();
          context->runtime->pre_wait(current_proc);
          disjoint_ready.wait();
          context->runtime->post_wait(current_proc);
        }
        else
          disjoint_ready.wait();
      }
      return disjoint;
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::are_disjoint(const ColorPoint &c1, const ColorPoint &c2,
                                     bool force_compute)
    //--------------------------------------------------------------------------
    {
      if (c1 == c2)
        return false;
      if (!force_compute && is_disjoint(false/*appy query*/))
        return true;
      bool issue_dynamic_test = false;
      std::pair<ColorPoint,ColorPoint> key(c1,c2);
      Event ready_event = Event::NO_EVENT;
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        if (disjoint_subspaces.find(key) != disjoint_subspaces.end())
          return true;
        else if (aliased_subspaces.find(key) != aliased_subspaces.end())
          return false;
        else
        {
          std::map<std::pair<ColorPoint,ColorPoint>,Event>::const_iterator
            finder = pending_tests.find(key);
          if (finder != pending_tests.end())
            ready_event = finder->second;
          else
            issue_dynamic_test = true;
        }
      }
      if (issue_dynamic_test)
      {
        IndexSpaceNode *left = get_child(c1);
        IndexSpaceNode *right = get_child(c2);
        Event left_pre = left->get_domain_precondition();
        Event right_pre = right->get_domain_precondition();
        AutoLock n_lock(node_lock);
        // Test again to see if we lost the race
        std::map<std::pair<ColorPoint,ColorPoint>,Event>::const_iterator
          finder = pending_tests.find(key);
        if (finder == pending_tests.end())
        {
          DynamicIndependenceArgs args;
          args.hlr_id = HLR_SPACE_INDEPENDENCE_TASK_ID;
          args.parent = this;
          args.left = left;
          args.right = right;
          Event pre = Event::merge_events(left_pre, right_pre);
          ready_event = context->runtime->issue_runtime_meta_task(&args, 
                    sizeof(args), HLR_SPACE_INDEPENDENCE_TASK_ID, NULL, pre);
          pending_tests[key] = ready_event;
          pending_tests[std::pair<ColorPoint,ColorPoint>(c2,c1)] = ready_event;
        }
        else
          ready_event = finder->second;
      }
      ready_event.wait();
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      if (disjoint_subspaces.find(key) != disjoint_subspaces.end())
        return true;
      else
        return false;
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::record_disjointness(bool result,
                                     const ColorPoint &c1, const ColorPoint &c2)
    //--------------------------------------------------------------------------
    {
      if (c1 == c2)
        return;
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(color_map.find(c1) != color_map.end());
      assert(color_map.find(c2) != color_map.end());
#endif
      if (result)
      {
        disjoint_subspaces.insert(std::pair<ColorPoint,ColorPoint>(c1,c2));
        disjoint_subspaces.insert(std::pair<ColorPoint,ColorPoint>(c2,c1));
      }
      else
      {
        aliased_subspaces.insert(std::pair<ColorPoint,ColorPoint>(c1,c2));
        aliased_subspaces.insert(std::pair<ColorPoint,ColorPoint>(c2,c1));
      }
      pending_tests.erase(std::pair<ColorPoint,ColorPoint>(c1,c2));
      pending_tests.erase(std::pair<ColorPoint,ColorPoint>(c2,c1));
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::is_complete(void)
    //--------------------------------------------------------------------------
    {
      // If we've cached the value then we are good to go
      {
        AutoLock n_lock(node_lock, 1, false/*exclusive*/);
        if (has_complete)
          return complete;
      }
      // Otherwise compute it 
      std::set<Domain> parent_domains, child_domains;
      bool can_cache = false;
      if (parent->has_component_domains())
        parent_domains = parent->get_component_domains_blocking();
      else
      {
        const Domain &dom = parent->get_domain_blocking();
        parent_domains.insert(dom);
        // We can cache the result if we know the domains
        // has dimension greater than zero indicating we have
        // a structured index space
        can_cache = (dom.get_dim() > 0);
      }
      for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator it = 
            color_map.begin(); it != color_map.end(); it++)
      {
        if (it->second->has_component_domains())
        {
          const std::set<Domain> &child_doms = 
                            it->second->get_component_domains_blocking();
          child_domains.insert(child_doms.begin(), child_doms.end());
        }
        else
          child_domains.insert(it->second->get_domain_blocking());
      }
      bool result = compute_dominates(child_domains, parent_domains);
      if (can_cache)
      {
        AutoLock n_lock(node_lock);
        complete = result;
        has_complete = true;
      }
      return result;
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::get_colors(std::set<ColorPoint> &colors)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator it = 
            valid_map.begin(); it != valid_map.end(); it++)
      {
        colors.insert(it->first);
      }
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::add_instance(PartitionNode *inst)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(logical_nodes.find(inst) == logical_nodes.end());
#endif
      logical_nodes.insert(inst);
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::has_instance(RegionTreeID tid)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::set<PartitionNode*>::const_iterator it = logical_nodes.begin();
            it != logical_nodes.end(); it++)
      {
        if ((*it)->handle.get_tree_id() == tid)
          return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::add_creation_source(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      creation_set.add(source);
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::destroy_node(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        parent->remove_child(color);
      AutoLock n_lock(node_lock);
      destruction_set.add(source);
      for (std::set<PartitionNode*>::const_iterator it = logical_nodes.begin();
             it != logical_nodes.end(); it++)
      {
        (*it)->destroy_node(source);
      }
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::add_pending_child(const ColorPoint &child_color,
                                          UserEvent handle_ready, 
                                          UserEvent domain_ready)
    //--------------------------------------------------------------------------
    {
      bool launch_remove = false;
      {
        AutoLock n_lock(node_lock);
        // Duplicate insertions can happen legally so avoid them
        if (pending_children.find(child_color) == pending_children.end())
        {
          pending_children[child_color] = 
            std::pair<UserEvent,UserEvent>(handle_ready, domain_ready);
          launch_remove = true;
        }
      }
      if (launch_remove)
      {
        PendingChildArgs args;
        args.hlr_id = HLR_PENDING_CHILD_TASK_ID;
        args.parent = this;
        args.pending_child = child_color;
        // Don't remove the pending child until the handle is ready
        context->runtime->issue_runtime_meta_task(&args, sizeof(args),
                                                  HLR_PENDING_CHILD_TASK_ID,
                                                  NULL, handle_ready);
      }
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::get_pending_child(const ColorPoint &child_color,
                                          UserEvent &handle_ready,
                                          UserEvent &domain_ready)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock, 1, false/*exclusive*/);
      std::map<ColorPoint,std::pair<UserEvent,UserEvent> >::const_iterator
        finder = pending_children.find(child_color);
      if (finder != pending_children.end())
      {
        handle_ready = finder->second.first;
        domain_ready = finder->second.second;
        return true;
      }
      return false;
    }
    
    //--------------------------------------------------------------------------
    void IndexPartNode::remove_pending_child(const ColorPoint &child_color)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      pending_children.erase(child_color);
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexPartNode::handle_pending_child_task(const void *args)
    //--------------------------------------------------------------------------
    {
      const PendingChildArgs *pargs = (const PendingChildArgs*)args;  
      pargs->parent->remove_pending_child(pargs->pending_child);
    }

    //--------------------------------------------------------------------------
    Event IndexPartNode::create_equal_children(size_t granularity)
    //--------------------------------------------------------------------------
    {
      if (parent->kind == UNSTRUCTURED_KIND)
      {
        size_t num_subspaces = color_space.get_volume();
        std::vector<LowLevel::IndexSpace> subspaces(num_subspaces);
        Event precondition;
        const Domain &parent_dom = parent->get_domain(precondition);
        // Launch the operation down to the low-level runtime
        Event ready_event = 
          parent_dom.get_index_space().create_equal_subspaces(num_subspaces,
                                                            granularity,
                                                            subspaces,
                                                            (mode & ALLOCABLE),
                                                            precondition);
        // Fill in all the subspaces
        unsigned idx = 0;
        for (Domain::DomainPointIterator itr(color_space); itr; itr++, idx++)
        {
          ColorPoint is_color(itr.p);
          IndexSpaceNode *child_node = get_child(is_color);
#ifdef DEBUG_HIGH_LEVEL
          assert(subspaces[idx].exists());
#endif
          child_node->set_domain(Domain(subspaces[idx]));
        }
        return ready_event;
      }
      else
      {
        // TODO: Implement structured kinds
        assert(false);
        return Event::NO_EVENT;
      }
    }

    //--------------------------------------------------------------------------
    Event IndexPartNode::create_weighted_children(
                   const std::map<DomainPoint,int> &weights, size_t granularity)
    //--------------------------------------------------------------------------
    {
      if (parent->kind == UNSTRUCTURED_KIND)
      {
        size_t num_subspaces = weights.size();
        std::vector<int> local_weights(num_subspaces);
        unsigned idx = 0;
        for (std::map<DomainPoint,int>::const_iterator it = weights.begin();
              it != weights.end(); it++, idx++)
        {
          local_weights[idx] = it->second;
        }
        std::vector<LowLevel::IndexSpace> subspaces(num_subspaces);
        Event precondition;
        const Domain &parent_dom = parent->get_domain(precondition);
        // Launch the operation down to the low-level runtime
        Event ready_event = 
          parent_dom.get_index_space().create_weighted_subspaces(num_subspaces,
                                                             granularity,
                                                             local_weights,
                                                             subspaces,
                                                             (mode & ALLOCABLE),
                                                             precondition);
        // Now create each of the sub-spaces
        idx = 0; 
        for (std::map<DomainPoint,int>::const_iterator it = weights.begin();
              it != weights.end(); it++, idx++)
        {
          ColorPoint is_color(it->first);
          IndexSpaceNode *child_node = get_child(is_color);
#ifdef DEBUG_HIGH_LEVEL
          assert(subspaces[idx].exists());
#endif
          child_node->set_domain(Domain(subspaces[idx]));
        }
        return ready_event;
      }
      else
      {
        // TODO: Implement structured kinds
        assert(false);
        return Event::NO_EVENT;
      }
    }

    //--------------------------------------------------------------------------
    Event IndexPartNode::create_by_operation(IndexPartNode *left, 
                                             IndexPartNode *right,
                                   LowLevel::IndexSpace::IndexSpaceOperation op)
    //--------------------------------------------------------------------------
    {
      if (parent->kind == UNSTRUCTURED_KIND)
      {
        size_t num_subspaces = color_space.get_volume();  
        std::vector<LowLevel::IndexSpace::BinaryOpDescriptor> 
                                                    operations(num_subspaces);
        std::set<Event> preconditions;
        Event parent_pre;
        const Domain parent_dom = parent->get_domain(parent_pre); 
        if (parent_pre.exists())
          preconditions.insert(parent_pre);
        unsigned idx = 0;
        for (Domain::DomainPointIterator itr(color_space); itr; itr++, idx++)
        {
          ColorPoint child_color(itr.p);
          IndexSpaceNode *left_child = left->get_child(child_color);
          IndexSpaceNode *right_child = right->get_child(child_color);
          Event left_pre, right_pre;
          const Domain &left_dom = left_child->get_domain(left_pre);
          if (left_pre.exists())
            preconditions.insert(left_pre);
          const Domain &right_dom = right_child->get_domain(right_pre);
          if (right_pre.exists())
            preconditions.insert(right_pre);
          operations[idx].op = op;
          operations[idx].parent = parent_dom.get_index_space();
          operations[idx].left_operand = left_dom.get_index_space();
          operations[idx].right_operand = right_dom.get_index_space();
        }
        // Merge all the preconditions and issue to the low-level runtime
        Event precondition = Event::merge_events(preconditions);
        Event result = LowLevel::IndexSpace::compute_index_spaces(operations,
                                                            (mode & ALLOCABLE),
                                                            precondition);
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, result);
#endif
        // Now set the domains for all the nodes
        idx = 0;
        for (Domain::DomainPointIterator itr(color_space); itr; itr++, idx++)
        {
          ColorPoint is_color(itr.p);
#ifdef DEBUG_HIGH_LEVEL
          assert(operations[idx].result.exists());
#endif
          IndexSpaceNode *child_node = get_child(is_color);
          child_node->set_domain(Domain(operations[idx].result));
        }
        return result;
      }
      else
      {
        // TODO: implement structured kinds
        assert(false);
        return Event::NO_EVENT;
      }
    }

    //--------------------------------------------------------------------------
    Event IndexPartNode::create_by_operation(IndexSpaceNode *left,
                                             IndexPartNode *right,
                                   LowLevel::IndexSpace::IndexSpaceOperation op)
    //--------------------------------------------------------------------------
    {
      if (parent->kind == UNSTRUCTURED_KIND)
      {
        size_t num_subspaces = color_space.get_volume();  
        std::vector<LowLevel::IndexSpace::BinaryOpDescriptor> 
                                                    operations(num_subspaces);
        std::set<Event> preconditions;
        Event parent_pre;
        const Domain parent_dom = parent->get_domain(parent_pre); 
        if (parent_pre.exists())
          preconditions.insert(parent_pre);
        Event left_pre;
        const Domain left_dom = left->get_domain(left_pre);
        if (left_pre.exists())
          preconditions.insert(left_pre);
        unsigned idx = 0;
        for (Domain::DomainPointIterator itr(color_space); itr; itr++, idx++)
        {
          ColorPoint child_color(itr.p);
          IndexSpaceNode *child = right->get_child(child_color);
          Event child_pre;
          const Domain child_dom = child->get_domain(child_pre);
          if (child_pre.exists())
            preconditions.insert(child_pre);
          operations[idx].op = op;
          operations[idx].parent = parent_dom.get_index_space();
          operations[idx].left_operand = left_dom.get_index_space();
          operations[idx].right_operand = child_dom.get_index_space();
        }
        // Merge all the preconditions and issue to the low-level runimte
        Event precondition = Event::merge_events(preconditions);
        Event result = LowLevel::IndexSpace::compute_index_spaces(operations,
                                                            (mode & ALLOCABLE),
                                                            precondition);
#ifdef LEGION_SPY
        LegionSpy::log_event_dependence(precondition, result);
#endif
        // Now set the domains for the nodes
        idx = 0;
        for (Domain::DomainPointIterator itr(color_space); itr; itr++, idx++)
        {
          ColorPoint is_color(itr.p);
#ifdef DEBUG_HIGH_LEVEL
          assert(operations[idx].result.exists());
#endif
          IndexSpaceNode *child = get_child(is_color);
          child->set_domain(Domain(operations[idx].result));
        }
        return result;
      }
      else
      {
        // TODO: implement structured kinds
        assert(false);
        return Event::NO_EVENT;
      }
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::get_subspace_domain_preconditions(
                                                 std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock, 1, false/*exclusive*/);
      for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator it = 
            color_map.begin(); it != color_map.end(); it++)
      {
        preconditions.insert(it->second->get_domain_precondition());
      }
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::get_subspace_domains(std::set<Domain> &subspaces)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock, 1, false/*exclusive*/);
      for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator it = 
            color_map.begin(); it != color_map.end(); it++)
      {
        if (it->second->has_component_domains())
        {
          const std::set<Domain> &components = 
                                it->second->get_component_domains_blocking();
          subspaces.insert(components.begin(), components.end());
        }
        else
          subspaces.insert(it->second->get_domain_blocking());
      }
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::intersects_with(IndexSpaceNode *other, bool compute)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder != intersections.end()) &&
            (!compute || finder->second.intersections_valid))
          return finder->second.has_intersects;
      }
      std::set<Domain> local_domains, intersect;
      bool result;
      get_subspace_domains(local_domains);
      if (other->has_component_domains())
      {
        result = compute_intersections(local_domains, 
                   other->get_component_domains_blocking(), intersect, compute);
      }
      else
      {
        result = compute_intersections(local_domains, 
                                       other->get_domain_blocking(), 
                                       intersect, compute);
      }
      AutoLock n_lock(node_lock);
      if (result)
      {
        // Check to make sure we didn't lose the race
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder == intersections.end()) ||
            (compute && !finder->second.intersections_valid))
        {
          if (compute)
            intersections[other] = IntersectInfo(intersect);
          else
            intersections[other] = IntersectInfo(true/*result*/);
        }
      }
      else
        intersections[other] = IntersectInfo(false/*result*/);
      return result;
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::intersects_with(IndexPartNode *other, bool compute)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        // Only return the value if we know we are valid and we didn't
        // want to compute anything or we already did compute it
        if ((finder != intersections.end()) &&
            (!compute || finder->second.intersections_valid))
          return finder->second.has_intersects;
      }
      std::set<Domain> local_domains, other_domains, intersect;
      get_subspace_domains(local_domains);
      other->get_subspace_domains(other_domains);
      bool result = compute_intersections(local_domains, other_domains, 
                                          intersect, compute);
      AutoLock n_lock(node_lock);
      if (result)
      {
        // Check to make sure we didn't lose the race
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder == intersections.end()) ||
            (compute && !finder->second.intersections_valid))
        {
          if (compute)
            intersections[other] = IntersectInfo(intersect);
          else
            intersections[other] = IntersectInfo(false/*result*/);
        }
      }
      else
        intersections[other] = IntersectInfo(false/*result*/);
      return result;
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& IndexPartNode::get_intersection_domains(
                                                          IndexSpaceNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder != intersections.end()) &&
            finder->second.intersections_valid)
          return finder->second.intersections;
      }
      std::set<Domain> local_domains, intersect;
      bool result;
      get_subspace_domains(local_domains);
      if (other->has_component_domains())
      {
        result = compute_intersections(local_domains, 
           other->get_component_domains_blocking(), intersect, true/*compute*/);
      }
      else
      {
        result = compute_intersections(local_domains, 
                                       other->get_domain_blocking(), 
                                       intersect, true/*compute*/);
      }
      AutoLock n_lock(node_lock);
      if (result)
      {
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder == intersections.end()) ||
            !finder->second.intersections_valid)
          intersections[other] = IntersectInfo(intersect);
      }
      else
        intersections[other] = IntersectInfo(false/*false*/);
      return intersections[other].intersections;
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& IndexPartNode::get_intersection_domains(
                                                           IndexPartNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder != intersections.end()) &&
            finder->second.intersections_valid)
          return finder->second.intersections;
      }
      std::set<Domain> local_domains, other_domains, intersect;
      get_subspace_domains(local_domains);
      other->get_subspace_domains(other_domains);
      bool result = compute_intersections(local_domains, other_domains, 
                                          intersect, true/*compute*/);
      AutoLock n_lock(node_lock);
      if (result)
      {
        std::map<IndexTreeNode*,IntersectInfo>::const_iterator finder = 
          intersections.find(other);
        if ((finder == intersections.end()) ||
            !finder->second.intersections_valid)
          intersections[other] = IntersectInfo(intersect);
      }
      else
        intersections[other] = IntersectInfo(false/*result*/);
      return intersections[other].intersections;
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::dominates(IndexSpaceNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,bool>::const_iterator finder = 
          dominators.find(other);
        if (finder != dominators.end())
          return finder->second;
      }
      std::set<Domain> local;
      get_subspace_domains(local);
      bool result;
      if (other->has_component_domains())
        result = compute_dominates(local, 
                  other->get_component_domains_blocking()); 
      else
      {
        std::set<Domain> other_doms;
        other_doms.insert(other->get_domain_blocking());
        result = compute_dominates(local, other_doms);
      }
      AutoLock n_lock(node_lock);
      dominators[other] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    bool IndexPartNode::dominates(IndexPartNode *other)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<IndexTreeNode*,bool>::const_iterator finder = 
          dominators.find(other);
        if (finder != dominators.end())
          return finder->second;
      }
      std::set<Domain> local, other_doms;
      get_subspace_domains(local);
      other->get_subspace_domains(other_doms);
      bool result = compute_dominates(local, other_doms);
      AutoLock n_lock(node_lock);
      dominators[other] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    /*static*/void IndexPartNode::handle_disjointness_test(
             IndexPartNode *parent, IndexSpaceNode *left, IndexSpaceNode *right)
    //--------------------------------------------------------------------------
    {
      bool disjoint = RegionTreeForest::are_disjoint(left, right);
      parent->record_disjointness(disjoint, left->color, right->color);
    }

    //--------------------------------------------------------------------------
    void IndexPartNode::send_node(AddressSpaceID target, bool up, bool down)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      if (up)
        parent->send_node(target, true/*up*/, false/*down*/);
      std::map<ColorPoint,IndexSpaceNode*> valid_copy;
      {
        // Make sure we know if this is disjoint or not yet
        bool disjoint_result = is_disjoint();
        AutoLock n_lock(node_lock);
        if (!creation_set.contains(target))
        {
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(handle);
            rez.serialize(color_space);
            rez.serialize(mode);
            rez.serialize(parent->handle); 
            rez.serialize(color);
            rez.serialize<bool>(disjoint_result);
            rez.serialize<size_t>(semantic_info.size());
            for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
                  semantic_info.begin(); it != semantic_info.end(); it++)
            {
              it->second.node_mask.add(target);
              rez.serialize(it->first);
              rez.serialize(it->second.node_mask);
              rez.serialize(it->second.size);
              rez.serialize(it->second.buffer, it->second.size);
            }
            rez.serialize<size_t>(pending_children.size());
            for (std::map<ColorPoint,std::pair<UserEvent,UserEvent> >
                  ::const_iterator it = pending_children.begin();
                  it != pending_children.end(); it++)
            {
              rez.serialize(it->first);
              rez.serialize(it->second.first);
              rez.serialize(it->second.second);
            }
          }
          context->runtime->send_index_partition_node(target, rez);
          creation_set.add(target);
        }
        // See if we need to go down
        if (down && child_creation.contains(target))
          down = false;
        if (!destruction_set.contains(target))
        {
          // Send the deletion notification
          context->runtime->send_index_partition_destruction(handle, target);
          destruction_set.add(target);
        }
        if (down)
          valid_copy = valid_map;
      }
      if (down)
      {
        for (std::map<ColorPoint,IndexSpaceNode*>::const_iterator it = 
              valid_copy.begin(); it != valid_copy.end(); it++)
        {
          it->second->send_node(target, false/*up*/, true/*down*/);
        }
        AutoLock n_lock(node_lock);
        child_creation.add(target);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexPartNode::handle_node_creation(
        RegionTreeForest *context, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      IndexPartition handle;
      derez.deserialize(handle);
      Domain color_space;
      derez.deserialize(color_space);
      AllocateMode mode;
      derez.deserialize(mode);
      IndexSpace parent;
      derez.deserialize(parent);
      ColorPoint color;
      derez.deserialize(color);
      bool disjoint;
      derez.deserialize(disjoint);
      IndexSpaceNode *parent_node = context->get_node(parent);
#ifdef DEBUG_HIGH_LEVEL
      assert(parent_node != NULL);
#endif
      IndexPartNode *node = context->create_node(handle, parent_node, color,
                                color_space, disjoint, mode);
#ifdef DEBUG_HIGH_LEVEL
      assert(node != NULL);
#endif
      node->add_creation_source(source);
      size_t num_semantic;
      derez.deserialize(num_semantic);
      for (unsigned idx = 0; idx < num_semantic; idx++)
      {
        SemanticTag tag;
        derez.deserialize(tag);
        NodeSet source_mask;
        derez.deserialize(source_mask);
        size_t buffer_size;
        derez.deserialize(buffer_size);
        const void *buffer = derez.get_current_pointer();
        derez.advance_pointer(buffer_size);
        node->attach_semantic_information(tag, source_mask,
                                          buffer, buffer_size);
      }
      size_t num_pending;
      derez.deserialize(num_pending);
      for (unsigned idx = 0; idx < num_pending; idx++)
      {
        ColorPoint child_color;
        derez.deserialize(child_color);
        UserEvent handle_ready;
        derez.deserialize(handle_ready);
        UserEvent domain_ready;
        derez.deserialize(domain_ready);
        node->add_pending_child(child_color, handle_ready, domain_ready);
      }
    } 

    //--------------------------------------------------------------------------
    /*static*/ void IndexPartNode::handle_node_request(
           RegionTreeForest *forest, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      IndexPartition handle;
      derez.deserialize(handle);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      IndexPartNode *target = forest->get_node(handle);
      target->send_node(source, true/*up*/, true/*down*/);
      Serializer rez;
      rez.serialize(to_trigger);
      forest->runtime->send_index_partition_return(source, rez);
    }

    //--------------------------------------------------------------------------
    /*static*/ void IndexPartNode::handle_node_return(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      to_trigger.trigger();
    }

    /////////////////////////////////////////////////////////////
    // Field Space Node 
    /////////////////////////////////////////////////////////////
    
    //--------------------------------------------------------------------------
    FieldSpaceNode::FieldSpaceNode(FieldSpace sp, Event dist_alloc,
                                   RegionTreeForest *ctx)
      : handle(sp), is_owner((sp.id % ctx->runtime->runtime_stride) ==
          ctx->runtime->address_space), context(ctx), 
        next_allocation_index(-1), distributed_allocation(dist_alloc)
    //--------------------------------------------------------------------------
    {
      this->node_lock = Reservation::create_reservation();
      this->allocated_indexes.clear();
    }

    //--------------------------------------------------------------------------
    FieldSpaceNode::FieldSpaceNode(const FieldSpaceNode &rhs)
      : handle(FieldSpace::NO_SPACE), is_owner(false), context(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    FieldSpaceNode::~FieldSpaceNode(void)
    //--------------------------------------------------------------------------
    {
      node_lock.destroy_reservation();
      node_lock = Reservation::NO_RESERVATION;
      for (std::map<FIELD_TYPE,LegionList<LayoutDescription*,
            LAYOUT_DESCRIPTION_ALLOC>::tracked>::iterator it =
            layouts.begin(); it != layouts.end(); it++)
      {
        LegionList<LayoutDescription*,LAYOUT_DESCRIPTION_ALLOC>::tracked
          &descs = it->second;
        for (LegionList<LayoutDescription*,LAYOUT_DESCRIPTION_ALLOC>::
              tracked::iterator it = descs.begin(); it != descs.end(); it++)
        {
          if ((*it)->remove_reference())
            delete (*it);
        }
      }
      layouts.clear();
      for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
            semantic_info.begin(); it != semantic_info.end(); it++)
      {
        legion_free(SEMANTIC_INFO_ALLOC, it->second.buffer, it->second.size);
      }
      for (LegionMap<std::pair<FieldID,SemanticTag>,
            SemanticInfo>::aligned::iterator it = semantic_field_info.begin(); 
            it != semantic_field_info.end(); it++)
      {
        legion_free(SEMANTIC_INFO_ALLOC, it->second.buffer, it->second.size);
      }
    }

    //--------------------------------------------------------------------------
    FieldSpaceNode& FieldSpaceNode::operator=(const FieldSpaceNode &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* FieldSpaceNode::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<FieldSpaceNode,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::attach_semantic_information(SemanticTag tag,
                                                     const NodeSet &sources,
                                                     const void *buffer, 
                                                     size_t size)
    //--------------------------------------------------------------------------
    {
      void *local = legion_malloc(SEMANTIC_INFO_ALLOC, size);
      memcpy(local, buffer, size);
      NodeSet diff, current;
      {
        AutoLock n_lock(node_lock); 
        // See if it already exists
        LegionMap<SemanticTag,SemanticInfo>::aligned::iterator finder = 
          semantic_info.find(tag);
        if (finder != semantic_info.end())
        {
          // Check to make sure that the bits are the same
          if (size != finder->second.size)
          {
            log_run.error("ERROR: Inconsistent Semantic Tag value "
                                "for tag %ld with different sizes of %ld"
                                " and %ld for index tree node", 
                                tag, size, finder->second.size);
#ifdef DEBUG_HIGH_LEVEL
            assert(false);
#endif
            exit(ERROR_INCONSISTENT_SEMANTIC_TAG);       
          }
          // Otherwise do a bitwise comparison
          {
            const char *orig = (const char*)finder->second.buffer;
            const char *next = (const char*)buffer;
            for (unsigned idx = 0; idx < size; idx++)
            {
              char diff = orig[idx] ^ next[idx];
              if (diff)
              {
                log_run.error("ERROR: Inconsistent Semantic Tag value "
                                    "for tag %ld with different values at"
                                    "byte %d for index tree node, %x != %x", 
                                    tag, idx, orig[idx], next[idx]);
#ifdef DEBUG_HIGH_LEVEL
                assert(false);
#endif
                exit(ERROR_INCONSISTENT_SEMANTIC_TAG);
              }
            }
          }
          finder->second.node_mask |= sources;
          diff = creation_set - finder->second.node_mask;
          current = finder->second.node_mask;
        }
        else
        {
          semantic_info[tag] = SemanticInfo(local, size, sources);
          diff = creation_set - sources;
          current = sources;
        }
      }
      if (!!diff)
      {
        // Package up the message and then send it out
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(handle);
          rez.serialize(tag);
          rez.serialize(current);
          rez.serialize(size);
          rez.serialize(buffer, size);
        }
        SendSemanticInfoFunctor<FIELD_SPACE_SEMANTIC>
                                    functor(context->runtime, rez);
        diff.map(functor);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::attach_semantic_information(FieldID fid,
                                                     SemanticTag tag,
                                                     const NodeSet &sources,
                                                     const void *buffer,
                                                     size_t size)
    //--------------------------------------------------------------------------
    {
      void *local = legion_malloc(SEMANTIC_INFO_ALLOC, size);
      memcpy(local, buffer, size);
      NodeSet diff, current;
      {
        AutoLock n_lock(node_lock); 
        // See if it already exists
        LegionMap<std::pair<FieldID,SemanticTag>,
            SemanticInfo>::aligned::iterator finder =
          semantic_field_info.find(std::pair<FieldID,SemanticTag>(fid,tag));
        if (finder != semantic_field_info.end())
        {
          // Check to make sure that the bits are the same
          if (size != finder->second.size)
          {
            log_run.error("ERROR: Inconsistent Semantic Tag value "
                                "for tag %ld with different sizes of %ld"
                                " and %ld for index tree node", 
                                tag, size, finder->second.size);
#ifdef DEBUG_HIGH_LEVEL
            assert(false);
#endif
            exit(ERROR_INCONSISTENT_SEMANTIC_TAG);       
          }
          // Otherwise do a bitwise comparison
          {
            const char *orig = (const char*)finder->second.buffer;
            const char *next = (const char*)buffer;
            for (unsigned idx = 0; idx < size; idx++)
            {
              char diff = orig[idx] ^ next[idx];
              if (diff)
              {
                log_run.error("ERROR: Inconsistent Semantic Tag value "
                                    "for tag %ld with different values at"
                                    "byte %d for index tree node, %x != %x", 
                                    tag, idx, orig[idx], next[idx]);
#ifdef DEBUG_HIGH_LEVEL
                assert(false);
#endif
                exit(ERROR_INCONSISTENT_SEMANTIC_TAG);
              }
            }
          }
          finder->second.node_mask |= sources;
          diff = creation_set - finder->second.node_mask;
          current = finder->second.node_mask;
        }
        else
        {
          semantic_field_info[std::pair<FieldID,SemanticTag>(fid,tag)] = 
            SemanticInfo(local, size, sources);
          diff = creation_set - sources;
          current = sources;
        }
      }
      if (!!diff)
      {
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(handle);
          rez.serialize(fid);
          rez.serialize(tag);
          rez.serialize(current);
          rez.serialize(size);
          rez.serialize(buffer, size);
        }
        SendSemanticInfoFunctor<FIELD_SEMANTIC> functor(context->runtime, rez);
        diff.map(functor);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::retrieve_semantic_information(SemanticTag tag,
                                              const void *&result, size_t &size)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/); 
      LegionMap<SemanticTag,SemanticInfo>::aligned::const_iterator finder = 
        semantic_info.find(tag);
      if (finder == semantic_info.end())
      {
        log_run.error("ERROR: invalid semantic tag %ld for "
                            "field space %d", tag, handle.id);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_SEMANTIC_TAG);
      }
      result = finder->second.buffer;
      size = finder->second.size;
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::retrieve_semantic_information(FieldID fid,
                             SemanticTag tag, const void *&result, size_t &size)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/); 
      LegionMap<std::pair<FieldID,SemanticTag>,
        SemanticInfo>::aligned::const_iterator finder = 
          semantic_field_info.find(std::pair<FieldID,SemanticTag>(fid,tag));
      if (finder == semantic_field_info.end())
      {
        log_run.error("ERROR: invalid semantic tag %ld for field %d "
                            "of field space %d", tag, fid, handle.id);
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_SEMANTIC_TAG);
      }
      result = finder->second.buffer;
      size = finder->second.size;
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_semantic_info(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      FieldSpace handle;
      derez.deserialize(handle);
      SemanticTag tag;
      derez.deserialize(tag);
      NodeSet sources;
      derez.deserialize(sources);
      size_t size;
      derez.deserialize(size);
      const void *buffer = derez.get_current_pointer();
      derez.advance_pointer(size);
      forest->attach_semantic_information(handle, tag, sources, buffer, size);
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_field_semantic_info(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      FieldSpace handle;
      derez.deserialize(handle);
      FieldID fid;
      derez.deserialize(fid);
      SemanticTag tag;
      derez.deserialize(tag);
      NodeSet sources;
      derez.deserialize(sources);
      size_t size;
      derez.deserialize(size);
      const void *buffer = derez.get_current_pointer();
      derez.advance_pointer(size);
      forest->attach_semantic_information(handle, fid, tag, 
                                          sources, buffer, size);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::SendFieldAllocationFunctor::apply(
                                                          AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      runtime->send_field_allocation(handle, field, size, index, target); 
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::allocate_field(FieldID fid, size_t size, bool local)
    //--------------------------------------------------------------------------
    {
      if (!is_owner && !distributed_allocation.exists())
      {
        // Send a request to the owner node to convert to 
        // distributed allocation mode
        UserEvent wait_on = UserEvent::create_user_event();
        AddressSpace owner = handle.id % context->runtime->runtime_stride;
        Serializer rez;
        rez.serialize(handle);
        rez.serialize(wait_on);
        context->runtime->send_distributed_alloc_request(owner, rez);
        wait_on.wait();
      }
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(fields.find(fid) == fields.end());
#endif
      // Find an index in which to allocate this field  
      unsigned index = allocate_index(local);
#ifdef DEBUG_HIGH_LEVEL
      for (std::map<FieldID,FieldInfo>::const_iterator it = fields.begin();
            it != fields.end(); it++)
      {
        assert(it->second.destroyed || (it->second.idx != index));
      }
#endif
      fields[fid] = FieldInfo(size, index, local);
      // Send messages to all our subscribers telling them about the allocation
      // as long as it is not local.  Local fields get sent by the task contexts
      if (!local && !!creation_set)
      {
        SendFieldAllocationFunctor functor(handle, fid, size, 
                                           index, context->runtime);
        creation_set.map(functor);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::allocate_field_index(FieldID fid, size_t size,
                                          AddressSpaceID source, unsigned index)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      // First check to see if we have already allocated this field.
      // If not do our own allocation
      unsigned our_index;
      std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(fid);
      if (finder == fields.end())
      {
        // Try to allocate in the same place
        our_index = allocate_index(false/*local*/, index);
#ifdef DEBUG_HIGH_LEVEL
        for (std::map<FieldID,FieldInfo>::const_iterator it = fields.begin();
              it != fields.end(); it++)
        {
          assert(it->second.destroyed || (it->second.idx != index));
        }
#endif
        fields[fid] = FieldInfo(size, our_index, false/*local*/);
        // If we haven't done the allocation already send updates to
        // all our subscribers telling them where we allocated the field
        // Note this includes sending it back to the source which sent
        // us the allocation in the first place
        if (!!creation_set)
        {
          SendFieldAllocationFunctor functor(handle, fid, size,
                                             our_index, context->runtime);
          creation_set.map(functor);
        }
      }
      else
      {
        our_index = finder->second.idx;
      }
      // Update our permutation transformer. Note we do this
      // no matter what and let the permutation transformer
      // keep track of whether or not it is an identity or not.
      LegionMap<AddressSpaceID,FieldPermutation>::aligned::iterator 
        trans_it = transformers.find(source);
      // Create the transformer if we need to
      if (trans_it == transformers.end())
      {
        transformers[source] = FieldPermutation();
        trans_it = transformers.find(source);
#ifdef DEBUG_HIGH_LEVEL
        assert(trans_it != transformers.end());
#endif
      }
      trans_it->second.send_to(index, our_index);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::SendFieldDestructionFunctor::apply(
                                                          AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      runtime->send_field_destruction(handle, field, target);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::free_field(FieldID fid, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      unsigned field_idx;
      std::set<RegionNode*> cached_nodes;
      {
        AutoLock n_lock(node_lock, 1, false/*exclusive*/);
        std::map<FieldID,FieldInfo>::iterator finder = fields.find(fid);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != fields.end());
#endif
        // If we already destroyed the field then we are done
        if (finder->second.destroyed)
          return;
        field_idx = finder->second.idx;
        cached_nodes = logical_nodes;
      }
      // Invalidate all the contexts on this node
      context->invalidate_field_index(cached_nodes, field_idx);
      // Send any remote free invalidations
      AutoLock n_lock(node_lock);
      std::map<FieldID,FieldInfo>::iterator finder = fields.find(fid);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != fields.end());
#endif
      // If we already destroyed the field then we are done
      if (finder->second.destroyed)
        return;
      // Tell all our subscribers that we've destroyed the field
      if (!!creation_set)
      {
        SendFieldDestructionFunctor functor(handle, fid, context->runtime);
        creation_set.map(functor);
      }
      // Free the index
      free_index(finder->second.idx);
      // Mark the field destroyed
      finder->second.destroyed = true;
    }

    //--------------------------------------------------------------------------
    bool FieldSpaceNode::has_field(FieldID fid)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(fid);
      if (finder == fields.end())
        return false;
      // Make sure we haven't destroyed this field
      return (!finder->second.destroyed);
    }

    //--------------------------------------------------------------------------
    size_t FieldSpaceNode::get_field_size(FieldID fid)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(fid);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != fields.end());
#endif
      return finder->second.field_size;
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::get_all_fields(std::set<FieldID> &to_set)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::map<FieldID,FieldInfo>::const_iterator it = fields.begin();
            it != fields.end(); it++)
      {
        if (!it->second.destroyed)
          to_set.insert(it->first);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::get_all_regions(std::set<LogicalRegion> &regions)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::set<RegionNode*>::const_iterator it = logical_nodes.begin();
            it != logical_nodes.end(); it++)
      {
        if (!(*it)->destruction_set)
          regions.insert((*it)->handle);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::get_field_set(const FieldMask &mask,
                                       std::set<FieldID> &to_set)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::map<FieldID,FieldInfo>::const_iterator it = fields.begin();
            it != fields.end(); it++)
      {
        if (it->second.destroyed)
          continue;
        if (mask.is_set(it->second.idx))
          to_set.insert(it->first);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::get_field_set(const FieldMask &mask, 
                                       const std::set<FieldID> &basis,
                                       std::set<FieldID> &to_set)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      // Only iterate over the basis fields here
      for (std::set<FieldID>::const_iterator it = basis.begin();
            it != basis.end(); it++)
      {
        std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(*it);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != fields.end());
#endif
        if (finder->second.destroyed)
          continue;
        if (mask.is_set(finder->second.idx))
          to_set.insert(finder->first);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::add_instance(RegionNode *inst)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(logical_nodes.find(inst) == logical_nodes.end());
#endif
      logical_nodes.insert(inst);
    }

    //--------------------------------------------------------------------------
    bool FieldSpaceNode::has_instance(RegionTreeID tid)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::set<RegionNode*>::const_iterator it = logical_nodes.begin();
            it != logical_nodes.end(); it++)
      {
        if ((*it)->handle.get_tree_id() == tid)
          return true;
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::add_creation_source(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      creation_set.add(source);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::destroy_node(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      destruction_set.add(source);
      for (std::set<RegionNode*>::const_iterator it = logical_nodes.begin();
            it != logical_nodes.end(); it++)
      {
        (*it)->destroy_node(source);
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::transform_field_mask(FieldMask &mask, 
                                              AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      // If we don't have an event for distributed allocation, then we
      // don't have to do this
      if (!distributed_allocation.exists())
        return;
      {
        // Need an exclusive lock since we might change the state
        // of the transformer.
        AutoLock n_lock(node_lock);
        LegionMap<AddressSpaceID,FieldPermutation>::aligned::iterator finder =
          transformers.find(source);
        if (finder != transformers.end())
        {
          finder->second.permute(mask);
          return;
        }
      }
      // If we didn't find it, we have to send a request
      // so we can get all the updated field placements and have
      // a valid transformer
      UserEvent wait_on = UserEvent::create_user_event();
      Serializer rez;
      rez.serialize(handle);
      rez.serialize(wait_on);
      context->runtime->send_field_space_request(source, rez);
      wait_on.wait();
      AutoLock n_lock(node_lock);
      LegionMap<AddressSpaceID,FieldPermutation>::aligned::iterator finder =
        transformers.find(source);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != transformers.end());
#endif
      finder->second.permute(mask);
    }

    //--------------------------------------------------------------------------
    FieldMask FieldSpaceNode::get_field_mask(
                              const std::set<FieldID> &privilege_fields) const
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      FieldMask result;
      for (std::set<FieldID>::const_iterator it = privilege_fields.begin();
            it != privilege_fields.end(); it++)
      {
        std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(*it);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != fields.end());
#endif
        result.set_bit(finder->second.idx);
      }
#ifdef DEBUG_HIGH_LEVEL
      // Have a little bit of code for logging bit masks when requested
      if (Runtime::bit_mask_logging)
      {
        char *bit_string = result.to_string();
        fprintf(stderr,"%s\n",bit_string);
        free(bit_string);
      }
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    unsigned FieldSpaceNode::get_field_index(FieldID fid) const
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(fid);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != fields.end());
#endif
      return finder->second.idx;
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::get_field_indexes(const std::set<FieldID> &needed,
                                     std::map<unsigned,FieldID> &indexes) const
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      for (std::set<FieldID>::const_iterator it = needed.begin();
            it != needed.end(); it++)
      {
        std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(*it);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != fields.end());
#endif
        indexes[finder->second.idx] = *it;
      }
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::compute_create_offsets(
                                        const std::set<FieldID> &create_fields, 
                                        std::vector<size_t> &field_sizes,
                                        std::vector<unsigned> &indexes)
    //--------------------------------------------------------------------------
    {
      // Need to hold the lock when accessing field infos
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      unsigned idx = 0;
      for (std::set<FieldID>::const_iterator it = 
            create_fields.begin(); it != create_fields.end(); it++,idx++)
      {
        std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(*it);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != fields.end());
#endif
        field_sizes[idx] = finder->second.field_size;
        indexes[idx] = finder->second.idx;
      }
    }

    //--------------------------------------------------------------------------
    InstanceManager* FieldSpaceNode::create_instance(Memory location,
                                                     Domain domain,
                                       const std::set<FieldID> &create_fields,
                                                     size_t blocking_factor,
                                                     unsigned depth,
                                                     RegionNode *node,
                                                     DistributedID result_did,
                                                     UniqueID op_id)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this->context, CREATE_INSTANCE_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(!create_fields.empty());
#endif
      // First check to see if the memory is even local, if not we
      // need to send a message to the local address space because 
      // the low-level has a nasty habit of blocking for remote 
      // instance creation
      AddressSpaceID local_space = 
        context->runtime->find_address_space(location);
      if (local_space != context->runtime->address_space)
      {
        // Create a wait event and then send the message 
        UserEvent wait_on = UserEvent::create_user_event();
        Serializer rez; 
        {
          RezCheck z(rez);
          rez.serialize(location);
          rez.serialize(domain);
          rez.serialize<size_t>(create_fields.size());
          for (std::set<FieldID>::const_iterator it = create_fields.begin();
                it != create_fields.end(); it++)
          {
            rez.serialize(*it);
          }
          rez.serialize(blocking_factor);
          rez.serialize(depth);
          rez.serialize(node->handle);
          rez.serialize(result_did);
          rez.serialize(op_id);
          rez.serialize(wait_on);
        }
        context->runtime->send_remote_instance_creation_request(local_space, 
                                                                rez);
        wait_on.wait();
        // When we wake up, see if we can find the distributed ID
        // if not then we failed, otherwise we succeeded
        DistributedCollectable *dc = 
          context->runtime->weak_find_distributed_collectable(result_did);
        if (dc == NULL)
          return NULL;
#ifdef DEBUG_HIGH_LEVEL
        InstanceManager *result = dynamic_cast<InstanceManager*>(dc);
        assert(result != NULL);
#else
        InstanceManager *result = static_cast<InstanceManager*>(dc);
#endif
        return result;
      }
      InstanceManager *result = NULL;
      if (create_fields.size() == 1)
      {
        FieldID fid = *create_fields.begin();
        size_t field_size;
        unsigned field_index;
        {
          // Need to hold the field lock when accessing field infos
          AutoLock n_lock(node_lock,1,false/*exclusive*/);
          std::map<FieldID,FieldInfo>::const_iterator finder = fields.find(fid);
#ifdef DEBUG_HIGH_LEVEL
          assert(finder != fields.end());
#endif
          field_size = finder->second.field_size;
          field_index = finder->second.idx;
        }
        // First see if we can recycle a physical instance
        Event use_event = Event::NO_EVENT;
        PhysicalInstance inst = 
          context->create_instance(domain, location, field_size, op_id);
        if (inst.exists())
        {
          FieldMask inst_mask = get_field_mask(create_fields);
          // See if we can find a layout description object
          LayoutDescription *layout = 
            find_layout_description(inst_mask, domain, blocking_factor);
          if (layout == NULL)
          {
            // Now we need to make a layout
            std::vector<size_t> field_sizes(1);
            std::vector<unsigned> indexes(1);
            field_sizes[0] = field_size;
            indexes[0] = field_index;
            layout = create_layout_description(inst_mask, domain,
                                               blocking_factor, 
                                               create_fields,
                                               field_sizes,
                                               indexes);
          }
#ifdef DEBUG_HIGH_LEVEL
          assert(layout != NULL);
#endif
          result = legion_new<InstanceManager>(context, result_did, 
                                       context->runtime->address_space,
                                       context->runtime->address_space,
                                       location, inst, node, layout, 
                                       use_event, depth, true/*reg now*/);
#ifdef DEBUG_HIGH_LEVEL
          assert(result != NULL);
#endif
#ifdef OLD_LEGION_PROF
          if (!use_event.exists())
          {
            std::map<FieldID,size_t> inst_fields;
            inst_fields[fid] = field_size;
            LegionProf::register_instance_creation(inst.id,
                location.id, 0/*redop*/, blocking_factor,
                inst_fields);
          }
#endif
        }
      }
      else
      {
        std::vector<size_t> field_sizes(create_fields.size());
        std::vector<unsigned> indexes(create_fields.size());
        compute_create_offsets(create_fields, field_sizes, indexes);
        // First see if we can recycle a physical instance
        Event use_event = Event::NO_EVENT;
        PhysicalInstance inst = 
          context->create_instance(domain, location, field_sizes, 
                                   blocking_factor, op_id);
        if (inst.exists())
        {
          FieldMask inst_mask = get_field_mask(create_fields);
          LayoutDescription *layout = 
            find_layout_description(inst_mask, domain, blocking_factor);
          if (layout == NULL)
          {
            // We couldn't find one so make one
            layout = create_layout_description(inst_mask, domain,
                                               blocking_factor,
                                               create_fields,
                                               field_sizes,
                                               indexes);
          }
#ifdef DEBUG_HIGH_LEVEL
          assert(layout != NULL);
#endif
          result = legion_new<InstanceManager>(context, result_did,
                                       context->runtime->address_space,
                                       context->runtime->address_space,
                                       location, inst, node, layout, 
                                       use_event, depth, true/*reg now*/);
#ifdef DEBUG_HIGH_LEVEL
          assert(result != NULL);
#endif
#ifdef OLD_LEGION_PROF
          if (!use_event.exists())
          {
            std::map<unsigned,size_t> inst_fields;
            for (std::set<FieldID>::const_iterator it = 
                  create_fields.begin(); it != create_fields.end(); it++)
            {
              std::map<FieldID,FieldInfo>::const_iterator finder = 
                fields.find(*it);
#ifdef DEBUG_HIGH_LEVEL
              assert(finder != fields.end());
#endif
              inst_fields[*it] = finder->second.field_size;
            }
            LegionProf::register_instance_creation(inst.id, location.id,
                                0/*redop*/, blocking_factor, inst_fields);
          }
#endif
        }
      }
      return result;
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_remote_instance_creation(
           RegionTreeForest *forest, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      Memory location;
      derez.deserialize(location);
      Domain domain;
      derez.deserialize(domain);
      size_t num_fields;
      derez.deserialize(num_fields);
      std::set<FieldID> fields;
      for (unsigned idx = 0; idx < num_fields; idx++)
      {
        FieldID fid;
        derez.deserialize(fid);
        fields.insert(fid);
      }
      size_t blocking_factor;
      derez.deserialize(blocking_factor);
      unsigned depth;
      derez.deserialize(depth);
      LogicalRegion handle;
      derez.deserialize(handle);
      DistributedID did;
      derez.deserialize(did);
      UniqueID op_id;
      derez.deserialize(op_id);
      UserEvent done_event;
      derez.deserialize(done_event);

      RegionNode *region_node = forest->get_node(handle);
      FieldSpaceNode *target = region_node->column_source;

      // Try to make the manager
      InstanceManager *result = target->create_instance(location, domain,
                                          fields, blocking_factor, depth,
                                          region_node, did, op_id);
      // If we succeeded, send the manager back
      if (result != NULL)
        result->send_manager(source);
      // No matter what send the notification
      Serializer rez;
      rez.serialize(done_event);
      forest->runtime->send_remote_creation_response(source, rez);
    }

    //--------------------------------------------------------------------------
    ReductionManager* FieldSpaceNode::create_reduction(Memory location,
                                                       Domain domain,
                                                       FieldID fid,
                                                       bool reduction_list,
                                                       RegionNode *node,
                                                       ReductionOpID redop,
                                                       DistributedID result_did,
                                                       UniqueID op_id)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(this->context, CREATE_REDUCTION_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(redop > 0);
#endif
      // First check to see if the memory is even local, if not we
      // need to send a message to the local address space because 
      // the low-level has a nasty habit of blocking for remote 
      // instance creation
      AddressSpaceID local_space = 
        context->runtime->find_address_space(location);
      if (local_space != context->runtime->address_space)
      {
        // Create a wait event and then send the message 
        UserEvent wait_on = UserEvent::create_user_event();
        Serializer rez; 
        {
          RezCheck z(rez);
          rez.serialize(location);
          rez.serialize(domain);
          rez.serialize(fid);
          rez.serialize(reduction_list);
          rez.serialize(node->handle);
          rez.serialize(redop);
          rez.serialize(result_did);
          rez.serialize(op_id);
          rez.serialize(wait_on);
        }
        context->runtime->send_remote_reduction_creation_request(local_space, 
                                                                 rez);
        wait_on.wait();
        // When we wake up, see if we can find the distributed ID
        // if not then we failed, otherwise we succeeded
        DistributedCollectable *dc = 
          context->runtime->weak_find_distributed_collectable(result_did);
        if (dc == NULL)
          return NULL;
#ifdef DEBUG_HIGH_LEVEL
        ReductionManager *result = dynamic_cast<ReductionManager*>(dc);
        assert(result != NULL);
#else
        ReductionManager *result = static_cast<ReductionManager*>(dc);
#endif
        return result;
      }
      ReductionManager *result = NULL;
      // Find the reduction operation for this instance
      const ReductionOp *reduction_op = Runtime::get_reduction_op(redop);
#ifdef DEBUG_HIGH_LEVEL
      std::map<FieldID,FieldInfo>::const_iterator finder =
#endif
        fields.find(fid);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != fields.end());
#endif
      if (reduction_list)
      {
        // We need a new index space for handling the sparse reductions
        // TODO: allow users to specify the max number of reductions.  Otherwise
        // for right now we'll just over approximate with the number of elements
        // in the handle index space since ideally reduction lists are sparse
        // and will have less than one reduction per point.
        const size_t num_elmts = node->row_source->get_num_elmts();
        Domain ptr_space = 
          Domain(LowLevel::IndexSpace::create_index_space(num_elmts));
        std::vector<size_t> element_sizes;
        element_sizes.push_back(sizeof(ptr_t)); // pointer types
        element_sizes.push_back(reduction_op->sizeof_rhs);
        // Don't give the reduction op here since this is a list instance and we
        // don't want to initialize any of the fields
        PhysicalInstance inst = context->create_instance(ptr_space, location,
                                          element_sizes, 1/*true list*/, op_id);
        if (inst.exists())
        {
          result = legion_new<ListReductionManager>(context, result_did,
                                            context->runtime->address_space,
                                            context->runtime->address_space, 
                                            location, inst, node, 
                                            redop, reduction_op, 
                                            ptr_space, true/*reg now*/);
#ifdef DEBUG_HIGH_LEVEL
          assert(result != NULL);
#endif
#ifdef OLD_LEGION_PROF
          {
            std::map<FieldID,size_t> inst_fields;
            inst_fields[fid] = reduction_op->sizeof_rhs;
            LegionProf::register_instance_creation(inst.id, location.id,
                redop, 1/*blocking factor*/, inst_fields);
          }
#endif
        }
      }
      else
      {
        // Easy case of making a foldable reduction
        PhysicalInstance inst = context->create_instance(domain, location,
                                        reduction_op->sizeof_rhs, redop, op_id);
        if (inst.exists())
        {
          // Issue the fill operation to fill in the init values for the field
          std::vector<Domain::CopySrcDstField> init(1);
          Domain::CopySrcDstField &dst = init[0];
          dst.inst = inst;
          dst.offset = 0;
          dst.size = reduction_op->sizeof_rhs;
          // Get the initial value
          void *init_value = malloc(reduction_op->sizeof_rhs);
          reduction_op->init(init_value, 1);
          Event ready_event = context->issue_fill(domain, op_id, init, 
                                      init_value, reduction_op->sizeof_rhs);
          free(init_value);
          result = legion_new<FoldReductionManager>(context, result_did,
                                            context->runtime->address_space,
                                            context->runtime->address_space, 
                                            location, inst, node, redop, 
                                            reduction_op, ready_event, 
                                            true/*register now*/);
#ifdef DEBUG_HIGH_LEVEL
          assert(result != NULL);
#endif
#ifdef OLD_LEGION_PROF
          {
            std::map<FieldID,size_t> inst_fields;
            inst_fields[fid] = reduction_op->sizeof_rhs;
            LegionProf::register_instance_creation(inst.id, location.id,
                redop, 0/*blocking factor*/, inst_fields);
          }
#endif
        }
      }
      return result;
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_remote_reduction_creation(
           RegionTreeForest *forest, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      Memory location;
      derez.deserialize(location);
      Domain domain;
      derez.deserialize(domain);
      FieldID fid;
      derez.deserialize(fid);
      bool reduction_list;
      derez.deserialize(reduction_list);
      LogicalRegion handle;
      derez.deserialize(handle);
      ReductionOpID redop;
      derez.deserialize(redop);
      DistributedID did;
      derez.deserialize(did);
      UniqueID op_id;
      derez.deserialize(op_id);
      UserEvent done_event;
      derez.deserialize(done_event);

      RegionNode *region_node = forest->get_node(handle);
      FieldSpaceNode *target = region_node->column_source;

      ReductionManager *result = target->create_reduction(location, domain,
                                          fid, reduction_list, region_node,
                                          redop, did, op_id);
      if (result != NULL)
        result->send_manager(source);
      // No matter what send the notification
      Serializer rez;
      rez.serialize(done_event);
      forest->runtime->send_remote_creation_response(source, rez);
    }

    //--------------------------------------------------------------------------
    InstanceManager* FieldSpaceNode::create_file_instance(
                                         const std::set<FieldID> &create_fields, 
                                         const FieldMask &attach_mask,
                                         RegionNode *node, AttachOp *attach_op)
    //--------------------------------------------------------------------------
    {
      std::vector<size_t> field_sizes(create_fields.size());
      std::vector<unsigned> indexes(create_fields.size());
      compute_create_offsets(create_fields, field_sizes, indexes);
      // Now make the instance, this should always succeed
      const Domain &dom = node->get_domain_blocking();
      PhysicalInstance inst = attach_op->create_instance(dom, field_sizes);
      // Assume that everything is SOA for files right now
      size_t blocking_factor = dom.get_volume();
      // Get the layout
      LayoutDescription *layout = 
        find_layout_description(attach_mask, dom, blocking_factor);
      if (layout == NULL)
        layout = create_layout_description(attach_mask, dom,
                                           blocking_factor,
                                           create_fields,
                                           field_sizes,
                                           indexes);
#ifdef DEBUG_HIGH_LEVEL
      assert(layout != NULL);
#endif
      DistributedID did = context->runtime->get_available_distributed_id(false);
      Memory location = inst.get_location();
      InstanceManager *result = legion_new<InstanceManager>(context, did, 
                                         context->runtime->address_space,
                                         context->runtime->address_space,
                                         location, inst, node, layout,
                                         Event::NO_EVENT, node->get_depth(),
                                         true/*register now*/,
                                         InstanceManager::ATTACH_FILE_FLAG);
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
#ifdef OLD_LEGION_PROF
      {
        std::map<FieldID,size_t> inst_fields;
        for (std::set<FieldID>::const_iterator it =
            create_fields.begin(); it != create_fields.end(); it++)
        {
          std::map<FieldID,FieldInfo>::const_iterator finder =
            fields.find(*it);
#ifdef DEBUG_HIGH_LEVEL
          assert(finder != fields.end());
#endif
          inst_fields[*it] = finder->second.field_size;
        }
        LegionProf::register_instance_creation(inst.id, location.id,
            0, blocking_factor, inst_fields);
      }
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    LayoutDescription* FieldSpaceNode::find_layout_description(
        const FieldMask &mask, const Domain &domain, size_t blocking_factor)
    //--------------------------------------------------------------------------
    {
      uint64_t hash_key = mask.get_hash_key();
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      std::map<FIELD_TYPE,LegionList<LayoutDescription*,
        LAYOUT_DESCRIPTION_ALLOC>::tracked>::const_iterator finder = 
                                                    layouts.find(hash_key);
      if (finder == layouts.end())
        return NULL;
      // First go through the existing descriptions and see if we find
      // one that matches the existing layout
      for (std::list<LayoutDescription*>::const_iterator it = 
            finder->second.begin(); it != finder->second.end(); it++)
      {
        if ((*it)->match_layout(mask, domain, blocking_factor))
          return (*it);
      }
      return NULL;
    }

    //--------------------------------------------------------------------------
    LayoutDescription* FieldSpaceNode::create_layout_description(
        const FieldMask &mask, const Domain &domain, size_t blocking_factor,
                                     const std::set<FieldID> &create_fields,
                                     const std::vector<size_t> &field_sizes, 
                                     const std::vector<unsigned> &indexes)
    //--------------------------------------------------------------------------
    {
      // Make the new field description and then register it
      LayoutDescription *result = new LayoutDescription(mask, domain,
                                                        blocking_factor, this);
      unsigned idx = 0;
      size_t accum_offset = 0;
      for (std::set<FieldID>::const_iterator it = create_fields.begin();
            it != create_fields.end(); it++, idx++)
      {
        result->add_field_info((*it), indexes[idx],
                               accum_offset, field_sizes[idx]);
        accum_offset += field_sizes[idx];
      }
      // Now we can register it
      return register_layout_description(result);
    }

    //--------------------------------------------------------------------------
    LayoutDescription* FieldSpaceNode::register_layout_description(
                                                      LayoutDescription *layout)
    //--------------------------------------------------------------------------
    {
      uint64_t hash_key = layout->allocated_fields.get_hash_key();
      AutoLock n_lock(node_lock);
      LegionList<LayoutDescription*,LAYOUT_DESCRIPTION_ALLOC>::tracked
        &descs = layouts[hash_key];
      if (!descs.empty())
      {
        for (LegionList<LayoutDescription*,LAYOUT_DESCRIPTION_ALLOC>::tracked
              ::const_iterator it = descs.begin(); it != descs.end(); it++)
        {
          if (layout->match_layout(*it))
          {
            // Delete the layout we are trying to register
            // and return the matching one
            delete layout;
            return (*it);
          }
        }
      }
      // Otherwise we successfully registered it
      descs.push_back(layout);
      layout->add_reference();
      return layout;
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::UpgradeFunctor::apply(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      UserEvent to_trigger = UserEvent::create_user_event();
      to_send[target] = to_trigger;
      preconditions.insert(to_trigger);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::upgrade_distributed_alloc(UserEvent to_trigger)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(is_owner);
#endif
      std::map<AddressSpaceID,UserEvent> to_send;
      {
        AutoLock n_lock(node_lock);
        if (!distributed_allocation.exists())
        {
          std::set<Event> preconditions;
          UpgradeFunctor functor(to_send, preconditions);
          creation_set.map(functor);
          distributed_allocation = Event::merge_events(preconditions);
        }
      }
      // Send the messages
      if (!to_send.empty())
      {
        for (std::map<AddressSpaceID,UserEvent>::const_iterator it = 
              to_send.begin(); it != to_send.end(); it++)
        {
          Serializer rez;
          rez.serialize(handle);
          rez.serialize(it->second);
          rez.serialize(distributed_allocation);
          context->runtime->send_distributed_alloc_upgrade(it->first, rez);
        }
      }
      // Trigger the result;
      to_trigger.trigger(distributed_allocation);
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::process_upgrade(UserEvent to_trigger, Event ready)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!is_owner);
      assert(!distributed_allocation.exists());
#endif
      distributed_allocation = ready;
      to_trigger.trigger();
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::send_node(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      // See if this is in our creation set, if not, send it and all the fields
      AutoLock n_lock(node_lock);
      if (!creation_set.contains(target))
      {
        // First send the node info and then send all the fields
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(handle);
          rez.serialize(distributed_allocation);
          rez.serialize<size_t>(semantic_info.size());
          for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
                semantic_info.begin(); it != semantic_info.end(); it++)
          {
            it->second.node_mask.add(target);
            rez.serialize(it->first);
            rez.serialize(it->second.node_mask);
            rez.serialize(it->second.size);
            rez.serialize(it->second.buffer, it->second.size);
          }
          rez.serialize<size_t>(semantic_field_info.size());
          for (LegionMap<std::pair<FieldID,SemanticTag>,
                SemanticInfo>::aligned::iterator
                it = semantic_field_info.begin(); 
                it != semantic_field_info.end(); it++)
          {
            it->second.node_mask.add(target);
            rez.serialize(it->first.first);
            rez.serialize(it->first.second);
            rez.serialize(it->second.node_mask);
            rez.serialize(it->second.size);
            rez.serialize(it->second.buffer, it->second.size);
          }
        }
        context->runtime->send_field_space_node(target, rez);
        // Send all the field allocations
        for (std::map<FieldID,FieldInfo>::const_iterator it = 
              fields.begin(); it != fields.end(); it++)
        {
          // No need to send it if it has been destroyed
          if (!it->second.destroyed)
          {
            context->runtime->send_field_allocation(handle, it->first,
                it->second.field_size, it->second.idx, target);
          }
        }
        // Finally add it to the creation set
        creation_set.add(target);
      }
      // Send any deletions if necessary
      if (!destruction_set.contains(target))
      {
        context->runtime->send_field_space_destruction(handle, target);
        destruction_set.add(target);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_node_creation(
          RegionTreeForest *context, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      FieldSpace handle;
      derez.deserialize(handle);
      Event dist_allocation;
      derez.deserialize(dist_allocation);
      FieldSpaceNode *node = context->create_node(handle, dist_allocation);
#ifdef DEBUG_HIGH_LEVEL
      assert(node != NULL);
#endif
      node->add_creation_source(source);
      size_t num_semantic;
      derez.deserialize(num_semantic);
      for (unsigned idx = 0; idx < num_semantic; idx++)
      {
        SemanticTag tag;
        derez.deserialize(tag);
        NodeSet source_mask;
        derez.deserialize(source_mask);
        size_t buffer_size;
        derez.deserialize(buffer_size);
        const void *buffer = derez.get_current_pointer();
        derez.advance_pointer(buffer_size);
        node->attach_semantic_information(tag, source_mask, 
                                          buffer, buffer_size);
      }
      size_t num_field_semantic;
      derez.deserialize(num_field_semantic);
      for (unsigned idx = 0; idx < num_field_semantic; idx++)
      {
        FieldID fid;
        derez.deserialize(fid);
        SemanticTag tag;
        derez.deserialize(tag);
        NodeSet source_mask;
        derez.deserialize(source_mask);
        size_t buffer_size;
        derez.deserialize(buffer_size);
        const void *buffer = derez.get_current_pointer();
        derez.advance_pointer(buffer_size);
        node->attach_semantic_information(fid, tag, source_mask,
                                          buffer, buffer_size);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_node_request(
           RegionTreeForest *forest, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      FieldSpace handle;
      derez.deserialize(handle);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      FieldSpaceNode *target = forest->get_node(handle);
      target->send_node(source);
      Serializer rez;
      rez.serialize(to_trigger);
      forest->runtime->send_field_space_return(source, rez);
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_node_return(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      to_trigger.trigger();
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_distributed_alloc_request(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      FieldSpace handle;
      derez.deserialize(handle);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      FieldSpaceNode *target = forest->get_node(handle);
      target->upgrade_distributed_alloc(to_trigger);
    }

    //--------------------------------------------------------------------------
    /*static*/ void FieldSpaceNode::handle_distributed_alloc_upgrade(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      FieldSpace handle;
      derez.deserialize(handle);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      Event ready_event;
      derez.deserialize(ready_event);
      FieldSpaceNode *target = forest->get_node(handle);
      target->process_upgrade(to_trigger, ready_event);
    }

    //--------------------------------------------------------------------------
    char* FieldSpaceNode::to_string(const FieldMask &mask) const
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
#ifdef DEBUG_HIGH_LEVEL
      assert(!!mask);
#endif
      char *result = (char*)malloc(MAX_FIELDS*4); 
      bool first = true;
      for (std::map<FieldID,FieldInfo>::const_iterator it = fields.begin();
            it != fields.end(); it++)
      {
        if (mask.is_set(it->second.idx))
        {
          if (first)
          {
            sprintf(result,"%d",it->first);
            first = false;
          }
          else
          {
            char temp[8];
            sprintf(temp,",%d",it->first);
            strcat(result, temp);
          }
        }
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(!first); // we should have written something
#endif
      return result;
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::to_field_set(const FieldMask &mask,
                                      std::set<FieldID> &field_set) const
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
#ifdef DEBUG_HIGH_LEVEL
      assert(!!mask);
#endif
      for (std::map<FieldID,FieldInfo>::const_iterator it = fields.begin();
            it != fields.end(); it++)
      {
        if (mask.is_set(it->second.idx))
        {
          field_set.insert(it->first);
        }
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(!field_set.empty()); // we should have found something
#endif
    }

    //--------------------------------------------------------------------------
    unsigned FieldSpaceNode::allocate_index(bool local, int goal /*= -1*/)
    //--------------------------------------------------------------------------
    {
      // Assume we are already holding the node lock 

      // We do something intelligent here to try and maintain
      // identity permutations as long as possible.  First, if there
      // is a target goal try and allocate it there since it has already
      // been assigned there on a remote node.
      if ((goal >= 0) && !allocated_indexes.is_set(goal))
      {
        unsigned result = goal;
        allocated_indexes.set_bit(result);
        return result;
      }
      // Two cases here. If we have more fields than nodes try to
      // evenly distribute allocations, otherwise just give up and
      // fall back to the random case.
      if (context->runtime->runtime_stride <= MAX_FIELDS)
      {
        // Otherwise, try picking out an index along the stripe corresponding
        // to our runtime instance.
        unsigned tests = 0;
        unsigned offset = context->runtime->address_space;
        for (unsigned idx = offset;
              idx < MAX_FIELDS; idx += context->runtime->runtime_stride)
        {
          if (!allocated_indexes.is_set(idx))
          {
            allocated_indexes.set_bit(idx);
            return idx;
          }
          tests++;
        }
        offset++;
        if (offset == context->runtime->runtime_stride)
          offset = 0;
        // If our strip is full, go onto the next runtime from ours
        // continue doing this until we have tested all the points.
        // Walk these points backwards to avoid conflicts with remote
        // nodes doing their own allocation on their stipes.
        while (tests < MAX_FIELDS)
        {
          int target = MAX_FIELDS-1;
          // Find our stripe
          while (offset != unsigned(target%context->runtime->runtime_stride))
            target--;
          // Now we're on our stripe, so start searching
          while (target >= 0)
          {
            if (!allocated_indexes.is_set(target))
            {
              unsigned result = target;
              allocated_indexes.set_bit(result);
              return result;
            }
            target -= context->runtime->runtime_stride;
            tests++;
          }
          // Didn't find anything on this stripe, go to the next one
          offset++;
          if (offset == context->runtime->runtime_stride)
            offset = 0;
        }
      }
      else
      {
        // Random case, we have more runtime objects than fields
        // in our field space, so pick a random location and
        // allocate sequentially from that point.

        // If we haven't picked a starting point yet, do that now
        if (next_allocation_index < 0)
          next_allocation_index = 
            context->runtime->generate_random_integer() % MAX_FIELDS;
        for (int tests = 0; tests < MAX_FIELDS; tests++)
        {
          // Handle the wrap-around case
          if (next_allocation_index == MAX_FIELDS)
            next_allocation_index = 0;
          // Always increment
          unsigned target = next_allocation_index++;
          // Check to see if it has been allocated
          if (!allocated_indexes.is_set(target))
          {
            allocated_indexes.set_bit(target);
            return target;
          }
        }
      }
      // If we make it here, the mask is full and we are out of allocations
      log_field.error("Exceeded maximum number of allocated fields for "
                            "field space %x.  Change MAX_FIELDS from %d and "
                            "related macros at the top of legion_config.h and "
                            "recompile.", handle.id, MAX_FIELDS);
#ifdef DEBUG_HIGH_LEVEL
      assert(false);
#endif
      exit(ERROR_MAX_FIELD_OVERFLOW);
      return 0;
    }

    //--------------------------------------------------------------------------
    void FieldSpaceNode::free_index(unsigned index)
    //--------------------------------------------------------------------------
    {
      // Assume we are already holding the node lock
      allocated_indexes.unset_bit(index);
      // We also need to invalidate all our layout descriptions
      // that contain this field
      std::vector<FIELD_TYPE> to_delete;
      for (std::map<FIELD_TYPE,LegionList<LayoutDescription*,
                  LAYOUT_DESCRIPTION_ALLOC>::tracked>::iterator lit = 
            layouts.begin(); lit != layouts.end(); lit++)
      {
        // If the bit is set, remove the layout descriptions
        if (lit->first & (1ULL << index))
        {
          LegionList<LayoutDescription*,LAYOUT_DESCRIPTION_ALLOC>::tracked
            &descs = lit->second;
          bool perform_delete = true;
          for (LegionList<LayoutDescription*,LAYOUT_DESCRIPTION_ALLOC>::
                tracked::iterator it = descs.begin(); 
                it != descs.end(); /*nothing*/)
          {
            if ((*it)->allocated_fields.is_set(index))
            {
              if ((*it)->remove_reference())
                delete (*it);
              it = descs.erase(it);
            }
            else 
            {
              it++;
              perform_delete = false;
            }
          }
          if (perform_delete)
            to_delete.push_back(lit->first);
        }
      }
      for (std::vector<FIELD_TYPE>::const_iterator it = to_delete.begin();
            it != to_delete.end(); it++)
      {
        layouts.erase(*it);
      }
    }

    /////////////////////////////////////////////////////////////
    // Users and Info 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalUser::LogicalUser(void)
      : GenericUser(), op(NULL), idx(0), gen(0), timeout(TIMEOUT)
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        , uid(0)
#endif
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalUser::LogicalUser(Operation *o, unsigned id, const RegionUsage &u,
                             const FieldMask &m)
      : GenericUser(u, m), op(o), idx(id), 
        gen(o->get_generation()), timeout(TIMEOUT)
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        , uid(o->get_unique_op_id())
#endif
    //--------------------------------------------------------------------------
    {
    }
    
    //--------------------------------------------------------------------------
    PhysicalUser::PhysicalUser(const RegionUsage &u, const ColorPoint &c,
                               FieldVersions *v)
      : usage(u), child(c), versions(v)
    //--------------------------------------------------------------------------
    {
      // Can be NULL in some cases
      if (versions != NULL)
        versions->add_reference();
#ifdef DEBUG_HIGH_LEVEL
      if (usage.redop > 0) // Use this property in pack and unpack
        assert(versions == NULL);
#endif
    }

    //--------------------------------------------------------------------------
    PhysicalUser::PhysicalUser(const PhysicalUser &rhs)
      : versions(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PhysicalUser::~PhysicalUser(void)
    //--------------------------------------------------------------------------
    {
      if ((versions != NULL) && versions->remove_reference())
        delete versions;
    }

    //--------------------------------------------------------------------------
    PhysicalUser& PhysicalUser::operator=(const PhysicalUser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool PhysicalUser::same_versions(const FieldMask &test_mask,
                                     const FieldVersions *other) const
    //--------------------------------------------------------------------------
    {
      if ((other == NULL) || (versions == NULL))
        return false;
      const LegionMap<VersionID,FieldMask>::aligned &local_versions = 
        versions->get_field_versions();
      const LegionMap<VersionID,FieldMask>::aligned &other_versions = 
        other->get_field_versions();
      for (LegionMap<VersionID,FieldMask>::aligned::const_iterator vit = 
            local_versions.begin(); vit != local_versions.end(); vit++)
      {
        FieldMask overlap = vit->second & test_mask;
        if (!overlap)
          continue;
        for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
              other_versions.begin(); it != other_versions.end(); it++)
        {
          FieldMask overlap2 = overlap & it->second;
          if (!overlap2)
            continue;
          if (vit->first != it->first)
            return false;
        }
      }
      return true;
    }

    //--------------------------------------------------------------------------
    void PhysicalUser::pack_user(Serializer &rez)
    //--------------------------------------------------------------------------
    {
      rez.serialize(child);
      rez.serialize(usage.privilege);
      rez.serialize(usage.prop);
      if (versions != NULL)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(usage.redop == 0);
#endif
        const LegionMap<VersionID,FieldMask>::aligned &field_versions =
          versions->get_field_versions();
        int count = field_versions.size();
        count = -count; // negate for disambiguation
        rez.serialize(count);
        for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
              field_versions.begin(); it != field_versions.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
      }
      else
      {
        int redop = usage.redop;
        rez.serialize(redop);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ PhysicalUser* PhysicalUser::unpack_user(Deserializer &derez,
                                                       FieldSpaceNode *node,
                                                       AddressSpaceID source,
                                                       bool add_reference)
    //--------------------------------------------------------------------------
    {
      ColorPoint child;
      derez.deserialize(child);
      RegionUsage usage;
      derez.deserialize(usage.privilege);
      derez.deserialize(usage.prop);
      int redop;
      derez.deserialize(redop);
      PhysicalUser *result = NULL;
      if (redop <= 0)
      {
        usage.redop = 0;
        FieldVersions *versions = NULL;
        if (redop < 0)
        {
          int count = -redop;
          versions = new FieldVersions();
          for (int idx = 0; idx < count; idx++)
          {
            VersionID vid;
            derez.deserialize(vid);
            FieldMask version_mask;
            derez.deserialize(version_mask);
            node->transform_field_mask(version_mask, source);
            versions->add_field_version(vid, version_mask);
          }
        }
        result = legion_new<PhysicalUser>(usage, child, versions);
      }
      else
      {
        usage.redop = redop;
        result = legion_new<PhysicalUser>(usage, child);
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      if (add_reference)
        result->add_reference();
      return result;
    }

    //--------------------------------------------------------------------------
    MappableInfo::MappableInfo(ContextID c, Operation *o, Processor p,
                               RegionRequirement &r, VersionInfo &info,
                               const FieldMask &k)
      : ctx(c), op(o), local_proc(p), req(r), 
        version_info(info), traversal_mask(k)
    //--------------------------------------------------------------------------
    {
    }

    /////////////////////////////////////////////////////////////
    // FieldVersions 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    FieldVersions::FieldVersions(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    FieldVersions::FieldVersions(const FieldVersions &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    FieldVersions::~FieldVersions(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    FieldVersions& FieldVersions::operator=(const FieldVersions &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void FieldVersions::add_field_version(VersionID vid, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      LegionMap<VersionID,FieldMask>::aligned::iterator finder = 
        field_versions.find(vid);
      if (finder == field_versions.end())
        field_versions[vid] = mask;
      else
        finder->second |= mask;
    }

    /////////////////////////////////////////////////////////////
    // VersionInfo 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    VersionInfo::NodeInfo::NodeInfo(const NodeInfo &rhs)
      : physical_state((rhs.physical_state == NULL) ? NULL : 
                        rhs.physical_state->clone(!rhs.needs_capture())),
        field_versions(rhs.field_versions), bit_mask(rhs.bit_mask) 
    //--------------------------------------------------------------------------
    {
      if (field_versions != NULL)
        field_versions->add_reference();
    }

    //--------------------------------------------------------------------------
    VersionInfo::NodeInfo::~NodeInfo(void)
    //--------------------------------------------------------------------------
    {
      if ((field_versions != NULL) && (field_versions->remove_reference()))
        delete field_versions;
      if (physical_state != NULL)
        delete physical_state;
    }

    //--------------------------------------------------------------------------
    VersionInfo::NodeInfo& VersionInfo::NodeInfo::operator=(const NodeInfo &rhs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(physical_state == NULL);
#endif
      if (rhs.physical_state != NULL)
      {
        if (physical_state != NULL)
          delete physical_state;
        physical_state = rhs.physical_state->clone(!rhs.needs_capture());
      }
      if ((field_versions != NULL) && (field_versions->remove_reference()))
        delete field_versions;
      field_versions = rhs.field_versions;
      if (field_versions != NULL)
        field_versions->add_reference();
      bit_mask = rhs.bit_mask;
      return *this;
    }

    //--------------------------------------------------------------------------
    VersionInfo::VersionInfo(void)
      : upper_bound_node(NULL),
        packed(false), packed_buffer(NULL), packed_size(0)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    VersionInfo::VersionInfo(const VersionInfo &rhs)
      : node_infos(rhs.node_infos), upper_bound_node(rhs.upper_bound_node), 
        packed(false), packed_buffer(NULL), packed_size(0)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!rhs.packed); // This shouldn't be called when packed
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        assert(it->second.physical_state == NULL);
      }
#endif
    }

    //--------------------------------------------------------------------------
    VersionInfo::~VersionInfo(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        assert(it->second.physical_state == NULL);
      }
#endif
      // If we still have a buffer, then free it now
      if (packed_buffer != NULL)
        free(packed_buffer);
    }

    //--------------------------------------------------------------------------
    VersionInfo& VersionInfo::operator=(const VersionInfo &rhs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        assert(it->second.physical_state == NULL);
      }
      // shouldn't be called when packed
      assert(!packed);
#endif
      node_infos = rhs.node_infos;
      upper_bound_node = rhs.upper_bound_node;
      return *this;
    }

    //--------------------------------------------------------------------------
    void VersionInfo::set_upper_bound_node(RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(upper_bound_node == NULL);
      assert(!packed);
#endif
      upper_bound_node = node;
    }

    //--------------------------------------------------------------------------
    void VersionInfo::merge(const VersionInfo &rhs, const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
      assert(!rhs.packed);
#endif
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator vit = 
            rhs.node_infos.begin(); vit != 
            rhs.node_infos.end(); vit++)
      {
        const LegionMap<VersionID,FieldMask>::aligned &rhs_versions = 
          vit->second.field_versions->get_field_versions();
        FieldVersions *&entry = node_infos[vit->first].field_versions;
        for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
              rhs_versions.begin(); it != rhs_versions.end(); it++)
        {
          FieldMask overlap = it->second & mask;
          if (!overlap)
            continue;
          if (entry == NULL)
          {
            entry = new FieldVersions();
            entry->add_reference();
          }
          entry->add_field_version(it->first, it->second);
        }
        if (entry != NULL)
        {
          NodeInfo &next = node_infos[vit->first];
          if (vit->second.premap_only())
            next.set_premap_only();
          if (vit->second.path_only())
            next.set_path_only();
          if (vit->second.needs_final())
            next.set_needs_final();
          if (vit->second.advance())
            next.set_advance();
          if (vit->second.close_top())
            next.set_close_top();
        }
      }
      if (upper_bound_node == NULL)
        upper_bound_node = rhs.upper_bound_node;
      else if (rhs.upper_bound_node != NULL)
      {
        unsigned current_depth = upper_bound_node->get_depth();
        unsigned rhs_depth = rhs.upper_bound_node->get_depth();
        if (current_depth < rhs_depth)
          upper_bound_node = rhs.upper_bound_node;
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::apply_premapping(ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
#endif
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        // Apply the update, but don't delete it yet because 
        // we still might be rolled back. Note we only advance if
        // we we're premapping only since the actual advance hasn't
        // been done for the other nodes yet.
        if (it->second.physical_state != NULL)
        {
          if (it->second.path_only())
            it->second.physical_state->apply_premapping_state(
                                                it->second.advance());
          else
            it->second.physical_state->apply_state(false/*advance*/);
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::apply_mapping(ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
#endif
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        // Skip anything that was premapping only
        if (it->second.premap_only())
          continue;
        if (it->second.physical_state != NULL)
        {
          it->second.physical_state->apply_state(true/*apply advance*/);
          // Don't delete it because we need to hold onto the 
          // version manager references in case this operation
          // fails to complete
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::apply_close(ContextID ctx, bool permit_leave_open)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
#endif
      if (permit_leave_open)
      {
        for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
              node_infos.begin(); it != node_infos.end(); it++)
        {
          // Skip anything that was premapping only
          if (it->second.premap_only())
            continue;
          if (it->second.physical_state != NULL)
          {
            it->second.physical_state->filter_and_apply(it->second.close_top(),
                                                 false/*filter children*/);
          }
        }
      }
      else
      {
        for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
              node_infos.begin(); it != node_infos.end(); it++)
        {
          // Skip anything that was premapping only
          if (it->second.premap_only())
            continue;
          // We can also skip anything that isn't the top node
          if (!it->second.close_top())
            continue;
          if (it->second.physical_state != NULL)
          {
            it->second.physical_state->filter_and_apply(true/*top*/,
                                           true/*filter children*/);
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::reset(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
#endif
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it =
            node_infos.begin(); it != node_infos.end(); it++)
      {
        if (it->second.physical_state != NULL)
        {
          // First reset the physical state
          it->second.physical_state->reset();
          it->second.set_needs_capture();
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::release(void)
    //--------------------------------------------------------------------------
    {
      // Might be called when packed, in which case this is a no-op
      // Now it is safe to go through and delete all the physical states
      // which will free up all the references on all the version managers
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        if (it->second.physical_state != NULL)
        {
          legion_delete(it->second.physical_state);
          it->second.physical_state = NULL;
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::clear(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        assert(it->second.physical_state == NULL);
      }
      assert(!packed);
#endif
      node_infos.clear();
      upper_bound_node = NULL;
    }

    //--------------------------------------------------------------------------
    void VersionInfo::sanity_check(RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
      assert(!packed);
      LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator finder = 
        node_infos.find(node);
      if (finder == node_infos.end())
        return;
      FieldMask previous_fields;
      assert(finder->second.field_versions != NULL);
      const LegionMap<VersionID,FieldMask>::aligned &field_versions = 
        finder->second.field_versions->get_field_versions();
      for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
            field_versions.begin(); it != field_versions.end(); it++)
      {
        assert(previous_fields * it->second);
        previous_fields |= it->second;
      }
    }

    //--------------------------------------------------------------------------
    PhysicalState* VersionInfo::find_physical_state(RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
#endif
      LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator finder = 
        node_infos.find(node);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != node_infos.end());
#endif
      // Check to see if we need a reset
      if ((finder->second.physical_state != NULL) &&
           finder->second.needs_capture())
      { 
#ifdef DEBUG_HIGH_LEVEL
        assert(finder->second.field_versions != NULL);
#endif
        // Recapture the state if we had to be reset
        finder->second.physical_state->capture_state(
                           finder->second.premap_only(),
                           finder->second.close_top(),
                           finder->second.field_versions->get_field_versions());
        finder->second.unset_needs_capture();
      }
      return finder->second.physical_state;
    }

    //--------------------------------------------------------------------------
    PhysicalState* VersionInfo::create_physical_state(RegionTreeNode *node, 
                         VersionManager *manager, bool initialize, bool capture)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
#endif
      LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator finder = 
        node_infos.find(node);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != node_infos.end());
      assert(finder->second.physical_state == NULL);
      assert(finder->second.field_versions != NULL);
#endif
      const LegionMap<VersionID,FieldMask>::aligned &field_versions = 
        finder->second.field_versions->get_field_versions();
      // Ask the manager to make the physical state
      PhysicalState *result = manager->construct_state(node, field_versions, 
                                                     finder->second.path_only(),
                                                     finder->second.close_top(),
                                                     finder->second.advance(),
                                                     initialize, capture);
      // Save the resulting physical state
      finder->second.physical_state = result;
      finder->second.unset_needs_capture();
      return result;
    } 

    //--------------------------------------------------------------------------
    FieldVersions* VersionInfo::get_versions(RegionTreeNode *node) const
    //--------------------------------------------------------------------------
    {
      LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator finder = 
        node_infos.find(node);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != node_infos.end());
#endif
      // It's alright for this to return NULL
      return finder->second.field_versions;
    }

    //--------------------------------------------------------------------------
    void VersionInfo::pack_version_info(Serializer &rez, 
                                        AddressSpaceID local, ContextID ctx)
    //--------------------------------------------------------------------------
    {
      if (packed_buffer != NULL)
      {
        // If we are already packed this is easy
        rez.serialize(packed_size);
        rez.serialize(packed_buffer, packed_size);
      }
      else
      {
        // Otherwise, make our own local serializer so we
        // can record how many bytes we need
        Serializer local_rez;
        pack_buffer(local_rez, local, ctx);
        size_t total_size = local_rez.get_used_bytes();
        rez.serialize(total_size);
        rez.serialize(local_rez.get_buffer(), total_size);
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::unpack_version_info(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
      assert(packed_buffer == NULL);
#endif
      derez.deserialize(packed_size);
      packed_buffer = malloc(packed_size);
      derez.deserialize(packed_buffer, packed_size);
      packed = true;
    }

    //--------------------------------------------------------------------------
    void VersionInfo::make_local(std::set<Event> &preconditions, 
                                 RegionTreeForest *forest,
                                 ContextID ctx, bool path_only)
    //--------------------------------------------------------------------------
    {
      if (packed)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(!path_only); // shouldn't be doing this if we are unpacking
#endif
        unpack_buffer(forest, ctx);
      }
      // Iterate over all version state infos and build physical states
      // without actually capturing any data
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        NodeInfo &info = it->second;
        // Skip any unnecessary entries if we are doing path only
        if (path_only && !info.path_only())
          continue;
        if (info.physical_state == NULL)
        {
          info.physical_state = it->first->get_physical_state(ctx, *this,
                                                       false/*initialize*/,
                                                       false/*capture*/);
          info.set_needs_capture();
        }
        // Now get the preconditions for the state
        info.physical_state->make_local(preconditions, info.needs_final());
      }
    } 

    //--------------------------------------------------------------------------
    void VersionInfo::clone_from(const VersionInfo &rhs)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
      assert(!rhs.packed);
#endif
      upper_bound_node = rhs.upper_bound_node;
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator nit = 
            rhs.node_infos.begin(); nit != rhs.node_infos.end(); nit++)
      {
        const NodeInfo &current = nit->second;
#ifdef DEBUG_HIGH_LEVEL
        assert(current.field_versions != NULL);
#endif
        NodeInfo &next = node_infos[nit->first];
#ifdef DEBUG_HIGH_LEVEL
        assert(next.physical_state == NULL);
#endif
        // Don't capture the physical states, we don't need them
        // If we do capture them, we'll create a cycle in the 
        // garbage collection scheme that will hold onto all
        // version states from the beginning of time
        next.field_versions = current.field_versions;
        next.field_versions->add_reference();
        next.bit_mask = current.bit_mask & 15;
        // Needs capture is already set
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::clone_from(const VersionInfo &rhs,CompositeCloser &closer)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
      assert(!rhs.packed);
#endif
      upper_bound_node = rhs.upper_bound_node;
      // Capture all region tree nodes that have not already been
      // captured by the closer
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator nit = 
            rhs.node_infos.begin(); nit != rhs.node_infos.end(); nit++)
      {
        const NodeInfo &current = nit->second;
#ifdef DEBUG_HIGH_LEVEL
        assert(current.field_versions != NULL);
#endif
        FieldMask clone_mask;         
        const LegionMap<VersionID,FieldMask>::aligned &field_versions = 
          current.field_versions->get_field_versions();
        for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
              field_versions.begin(); it != field_versions.end(); it++) 
        {
          clone_mask |= it->second;
        }
        // Filter this node from the closer
        closer.filter_capture_mask(nit->first, clone_mask);
        if (!clone_mask)
          continue;
        NodeInfo &next = node_infos[nit->first];
#ifdef DEBUG_HIGH_LEVEL
        assert(next.physical_state == NULL); 
#endif
        // Don't capture the physical states, we don't need them
        // If we do capture them, we'll create a cycle in the 
        // garbage collection scheme that will hold onto all
        // version states from the beginning of time
        // Just copy the reference, we'll still have all the necessary
        // version numbers for each fields and maybe a few more
        next.field_versions = current.field_versions;
        next.field_versions->add_reference();
        next.bit_mask = current.bit_mask & 15;
        // Needs capture is already set
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(node_infos.find(upper_bound_node) != node_infos.end());
#endif
    }

    //--------------------------------------------------------------------------
    void VersionInfo::pack_buffer(Serializer &rez, 
                                  AddressSpaceID local_space, ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!packed);
      assert(upper_bound_node != NULL);
#endif
      if (upper_bound_node->is_region())
      {
        rez.serialize<bool>(true);
        rez.serialize(upper_bound_node->as_region_node()->handle);
      }
      else
      {
        rez.serialize<bool>(false);
        rez.serialize(upper_bound_node->as_partition_node()->handle);
      }
      rez.serialize(local_space);
      size_t total_regions = 0;
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::const_iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        if (it->first->is_region())
          total_regions++;
      }
      rez.serialize(total_regions);
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        if (it->first->is_region())
        {
          rez.serialize(it->first->as_region_node()->handle);
          pack_node_info(rez, it->second, it->first, ctx);
        }
      }
      size_t total_partitions = node_infos.size() - total_regions;
      rez.serialize(total_partitions);
      for (LegionMap<RegionTreeNode*,NodeInfo>::aligned::iterator it = 
            node_infos.begin(); it != node_infos.end(); it++)
      {
        if (!it->first->is_region())
        {
          rez.serialize(it->first->as_partition_node()->handle);
          pack_node_info(rez, it->second, it->first, ctx);
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionInfo::unpack_buffer(RegionTreeForest *forest, ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(packed);
      assert(packed_buffer != NULL);
#endif
      Deserializer derez(packed_buffer, packed_size);
      // Unpack the upper bound node
      {
        bool is_region;
        derez.deserialize(is_region);
        if (is_region)
        {
          LogicalRegion handle;
          derez.deserialize(handle);
          upper_bound_node = forest->get_node(handle);
        }
        else
        {
          LogicalPartition handle;
          derez.deserialize(handle);
          upper_bound_node = forest->get_node(handle);
        }
      }
      // Unpack the source
      AddressSpaceID source;
      derez.deserialize(source);
      // Unpack the node infos
      size_t num_regions;
      derez.deserialize(num_regions);
      for (unsigned idx = 0; idx < num_regions; idx++)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        RegionTreeNode *node = forest->get_node(handle);
        unpack_node_info(node, ctx, derez, source);
      }
      size_t num_partitions;
      derez.deserialize(num_partitions);
      for (unsigned idx = 0; idx < num_partitions; idx++)
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        RegionTreeNode *node = forest->get_node(handle);
        unpack_node_info(node, ctx, derez, source);
      }
      // Keep the buffer around for now in case we need
      // to pack it again later (e.g. for composite views)
      packed = false;
    }

    //--------------------------------------------------------------------------
    void VersionInfo::pack_node_info(Serializer &rez, NodeInfo &info,
                                     RegionTreeNode *node, ContextID ctx)
    //--------------------------------------------------------------------------
    {
      rez.serialize(info.bit_mask);
      if (info.physical_state == NULL)
        info.physical_state = node->get_physical_state(ctx, *this,
                                                       false/*capture*/);
      PhysicalState *state = info.physical_state;
#ifdef DEBUG_HIGH_LEVEL
      if (!info.advance())
        assert(state->advance_states.empty());
#endif
      size_t total_version_states = 0;
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator it1 = 
            state->version_states.begin(); it1 != 
            state->version_states.end(); it1++)
      {
        const VersionStateInfo &state_info = it1->second;
        total_version_states += state_info.states.size();
      }
      rez.serialize(total_version_states);
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator it1 = 
            state->version_states.begin(); it1 != 
            state->version_states.end(); it1++)
      {
        const VersionStateInfo &state_info = it1->second;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              state_info.states.begin(); it != state_info.states.end(); it++)
        {
          rez.serialize(it1->first);
          rez.serialize(it->first->did);
          rez.serialize(it->second);
        }
      }
      size_t total_advance_states = 0;
      if (!state->advance_states.empty())
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              it1 = state->advance_states.begin(); it1 != 
              state->advance_states.end(); it1++)
        {
          const VersionStateInfo &state_info = it1->second;
          total_advance_states += state_info.states.size();
        }
        rez.serialize(total_advance_states);
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              it1 = state->advance_states.begin(); it1 != 
              state->advance_states.end(); it1++)
        {
          const VersionStateInfo &state_info = it1->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it =
                state_info.states.begin(); it != state_info.states.end(); it++)
          {
            rez.serialize(it1->first);
            rez.serialize(it->first->did);
            rez.serialize(it->second);
          }
        }
      }
      else
        rez.serialize(total_advance_states);
    }

    //--------------------------------------------------------------------------
    void VersionInfo::unpack_node_info(RegionTreeNode *node, ContextID ctx,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      NodeInfo &info = node_infos[node];
      VersionManager *manager = node->get_version_manager(ctx);
#ifdef DEBUG_HIGH_LEVEL
      info.physical_state = legion_new<PhysicalState>(manager, node);
#else
      info.physical_state = legion_new<PhysicalState>(manager);
#endif
      // Don't need premap
      derez.deserialize(info.bit_mask);
      info.set_needs_capture();
      // Unpack the version states
      size_t num_states;
      derez.deserialize(num_states);
      if (num_states > 0)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(info.field_versions == NULL);
#endif
        info.field_versions = new FieldVersions();
        info.field_versions->add_reference();
      }
      FieldSpaceNode *field_node = node->column_source;
      for (unsigned idx = 0; idx < num_states; idx++)
      {
        VersionID vid;
        derez.deserialize(vid);
        DistributedID did;
        derez.deserialize(did);
        FieldMask mask;
        derez.deserialize(mask);
        // Transform the field mask
        field_node->transform_field_mask(mask, source);
        VersionState *state = node->find_remote_version_state(ctx, vid, 
                                        did, source, mask, false/*initialize*/);
        info.physical_state->add_version_state(state, mask);
        // Also add this to the version numbers
        // Only need to do this for the non-advance states
        info.field_versions->add_field_version(vid, mask);
      }
      // Unpack the advance states
      derez.deserialize(num_states);
      for (unsigned idx = 0; idx < num_states; idx++)
      {
        VersionID vid;
        derez.deserialize(vid);
        DistributedID did;
        derez.deserialize(did);
        FieldMask mask;
        derez.deserialize(mask);
        // Transform the field mask
        field_node->transform_field_mask(mask, source);
        VersionState *state = node->find_remote_version_state(ctx, vid, 
                                        did, source, mask, true/*initialize*/);
        // No point in adding this to the version state infos
        // since we already know we just use that to build the PhysicalState
        info.physical_state->add_advance_state(state, mask);
      }
    }
    
    /////////////////////////////////////////////////////////////
    // RestrictInfo 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RestrictInfo::RestrictInfo(void)
      : perform_check(false)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RestrictInfo::RestrictInfo(const RestrictInfo &rhs)
      : perform_check(rhs.perform_check), restrictions(rhs.restrictions)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RestrictInfo::~RestrictInfo(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RestrictInfo& RestrictInfo::operator=(const RestrictInfo &rhs)
    //--------------------------------------------------------------------------
    {
      // Only need to copy over perform_check and restrictions
      perform_check = rhs.perform_check;
      restrictions = rhs.restrictions;
      return *this;
    }

    //--------------------------------------------------------------------------
    bool RestrictInfo::has_restrictions(LogicalRegion handle, RegionNode *node,
                                        const std::set<FieldID> &fields) const
    //--------------------------------------------------------------------------
    {
      LegionMap<LogicalRegion,FieldMask>::aligned::const_iterator finder = 
        restrictions.find(handle);
      if (finder != restrictions.end())
      {
        FieldMask mask = node->column_source->get_field_mask(fields);
        return (!(mask * finder->second));
      }
      return false;
    }

    //--------------------------------------------------------------------------
    void RestrictInfo::pack_info(Serializer &rez)
    //--------------------------------------------------------------------------
    {
      rez.serialize<size_t>(restrictions.size());
      for (LegionMap<LogicalRegion,FieldMask>::aligned::const_iterator it = 
            restrictions.begin(); it != restrictions.end(); it++)
      {
        rez.serialize(it->first);
        rez.serialize(it->second);
      }
    }

    //--------------------------------------------------------------------------
    void RestrictInfo::unpack_info(Deserializer &derez, AddressSpaceID source,
                                   RegionTreeForest *forest)
    //--------------------------------------------------------------------------
    {
      size_t num_restrictions;
      derez.deserialize(num_restrictions);
      FieldSpaceNode *field_node = NULL;
      for (unsigned idx = 0; idx < num_restrictions; idx++)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        FieldMask &mask = restrictions[handle];
        derez.deserialize(mask);
        if (field_node == NULL)
          field_node = forest->get_node(handle)->column_source;
        field_node->transform_field_mask(mask, source);
      }
    }

    /////////////////////////////////////////////////////////////
    // PathTraverser 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PathTraverser::PathTraverser(RegionTreePath &p)
      : path(p)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PathTraverser::PathTraverser(const PathTraverser &rhs)
      : path(rhs.path)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PathTraverser::~PathTraverser(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PathTraverser& PathTraverser::operator=(const PathTraverser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool PathTraverser::traverse(RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
      // Continue visiting nodes and then finding their children
      // until we have traversed the entire path.
      while (true)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(node != NULL);
#endif
        depth = node->get_depth();
        has_child = path.has_child(depth);
        if (has_child)
          next_child = path.get_child(depth);
        bool continue_traversal = node->visit_node(this);
        if (!continue_traversal)
          return false;
        if (!has_child)
          break;
        node = node->get_tree_child(next_child);
      }
      return true;
    }

    /////////////////////////////////////////////////////////////
    // LogicalPathRegistrar
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalPathRegistrar::LogicalPathRegistrar(ContextID c, Operation *o,
                                       const FieldMask &m, RegionTreePath &p)
      : PathTraverser(p), ctx(c), field_mask(m), op(o)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalPathRegistrar::LogicalPathRegistrar(const LogicalPathRegistrar&rhs)
      : PathTraverser(rhs.path), ctx(0), field_mask(FieldMask()), op(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    LogicalPathRegistrar::~LogicalPathRegistrar(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalPathRegistrar& LogicalPathRegistrar::operator=(
                                                const LogicalPathRegistrar &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool LogicalPathRegistrar::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->register_logical_dependences<false>(ctx, op, field_mask);
      if (!has_child)
      {
        // If we're at the bottom, fan out and do all the children
        LogicalRegistrar<false> registrar(ctx, op, field_mask);
        return node->visit_node(&registrar);
      }
      return true;
    }

    //--------------------------------------------------------------------------
    bool LogicalPathRegistrar::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->register_logical_dependences<false>(ctx, op, field_mask);
      if (!has_child)
      {
        // If we're at the bottom, fan out and do all the children
        LogicalRegistrar<false> registrar(ctx, op, field_mask);
        return node->visit_node(&registrar);
      }
      return true;
    }


    /////////////////////////////////////////////////////////////
    // LogicalRegistrar
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    LogicalRegistrar<DOMINATE>::LogicalRegistrar(ContextID c, Operation *o,
                                                 const FieldMask &m)
      : ctx(c), field_mask(m), op(o)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    LogicalRegistrar<DOMINATE>::LogicalRegistrar(const LogicalRegistrar &rhs)
      : ctx(0), field_mask(FieldMask()), op(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    LogicalRegistrar<DOMINATE>::~LogicalRegistrar(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    LogicalRegistrar<DOMINATE>& LogicalRegistrar<DOMINATE>::operator=(
                                                    const LogicalRegistrar &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    bool LogicalRegistrar<DOMINATE>::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    bool LogicalRegistrar<DOMINATE>::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->register_logical_dependences<DOMINATE>(ctx, op, field_mask);
      return true;
    }

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    bool LogicalRegistrar<DOMINATE>::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->register_logical_dependences<DOMINATE>(ctx, op, field_mask);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // LogicalInitializer 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalInitializer::LogicalInitializer(ContextID c)
      : ctx(c)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalInitializer::LogicalInitializer(const LogicalInitializer &rhs)
      : ctx(0)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    LogicalInitializer::~LogicalInitializer(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalInitializer& LogicalInitializer::operator=(
                                                  const LogicalInitializer &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool LogicalInitializer::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool LogicalInitializer::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->initialize_logical_state(ctx); 
      return true;
    }

    //--------------------------------------------------------------------------
    bool LogicalInitializer::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->initialize_logical_state(ctx);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // LogicalInvalidator
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalInvalidator::LogicalInvalidator(ContextID c)
      : ctx(c)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalInvalidator::LogicalInvalidator(const LogicalInvalidator &rhs)
      : ctx(0)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    LogicalInvalidator::~LogicalInvalidator(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalInvalidator& LogicalInvalidator::operator=(
                                                  const LogicalInvalidator &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool LogicalInvalidator::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool LogicalInvalidator::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_logical_state(ctx); 
      return true;
    }

    //--------------------------------------------------------------------------
    bool LogicalInvalidator::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_logical_state(ctx);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // RestrictionMutator
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    template<bool ADD_RESTRICT>
    RestrictionMutator<ADD_RESTRICT>::RestrictionMutator(ContextID c,
                                                         const FieldMask &mask)
      : ctx(c), restrict_mask(mask)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    template<bool ADD_RESTRICT>
    bool RestrictionMutator<ADD_RESTRICT>::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    template<bool ADD_RESTRICT>
    bool RestrictionMutator<ADD_RESTRICT>::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      if (ADD_RESTRICT)
        node->add_restriction(ctx, restrict_mask);
      else
        node->release_restriction(ctx, restrict_mask);
      return true;
    }

    //--------------------------------------------------------------------------
    template<bool ADD_RESTRICT>
    bool RestrictionMutator<ADD_RESTRICT>::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      if (ADD_RESTRICT)
        node->add_restriction(ctx, restrict_mask);
      else
        node->release_restriction(ctx, restrict_mask);
      return true;
    } 

    /////////////////////////////////////////////////////////////
    // PhysicalInitializer 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PhysicalInitializer::PhysicalInitializer(ContextID c)
      : ctx(c)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalInitializer::PhysicalInitializer(const PhysicalInitializer &rhs)
      : ctx(0)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PhysicalInitializer::~PhysicalInitializer(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalInitializer& PhysicalInitializer::operator=(
                                                const PhysicalInitializer &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool PhysicalInitializer::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    bool PhysicalInitializer::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->initialize_physical_state(ctx);
      return true;
    }

    //--------------------------------------------------------------------------
    bool PhysicalInitializer::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->initialize_physical_state(ctx);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // PhysicalInvalidator
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PhysicalInvalidator::PhysicalInvalidator(ContextID c)
      : ctx(c)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalInvalidator::PhysicalInvalidator(const PhysicalInvalidator &rhs)
      : ctx(0)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PhysicalInvalidator::~PhysicalInvalidator(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalInvalidator& PhysicalInvalidator::operator=(
                                                const PhysicalInvalidator &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool PhysicalInvalidator::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool PhysicalInvalidator::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_physical_state(ctx);
      return true;
    }

    //--------------------------------------------------------------------------
    bool PhysicalInvalidator::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->invalidate_physical_state(ctx);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // PhysicalDetacher 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PhysicalDetacher::PhysicalDetacher(ContextID c, const FieldMask &m,
                                       PhysicalManager *t)
      : ctx(c), detach_mask(m), target(t)
    //--------------------------------------------------------------------------
    {
    }
    
    //--------------------------------------------------------------------------
    bool PhysicalDetacher::visit_only_valid(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool PhysicalDetacher::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      node->detach_instance_views(ctx, detach_mask, target);
      return true;
    }

    //--------------------------------------------------------------------------
    bool PhysicalDetacher::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      node->detach_instance_views(ctx, detach_mask, target);
      return true;
    }

    /////////////////////////////////////////////////////////////
    // ReductionCloser 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ReductionCloser::ReductionCloser(ContextID c, ReductionView *t,
                                     const FieldMask &m, VersionInfo &info, 
                                     Processor local, Operation *o)
      : ctx(c), target(t), close_mask(m), version_info(info), 
        local_proc(local), op(o)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ReductionCloser::ReductionCloser(const ReductionCloser &rhs)
      : ctx(0), target(NULL), close_mask(FieldMask()), 
        version_info(rhs.version_info), local_proc(Processor::NO_PROC), op(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    ReductionCloser::~ReductionCloser(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    ReductionCloser& ReductionCloser::operator=(const ReductionCloser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void ReductionCloser::issue_close_reductions(RegionTreeNode *node,
                                                 PhysicalState *state)
    //--------------------------------------------------------------------------
    {
      LegionMap<ReductionView*,FieldMask>::aligned valid_reductions;
      for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it = 
            state->reduction_views.begin(); it != 
            state->reduction_views.end(); it++)
      {
        // Skip our target
        if (it->first == target)
          continue;
        // If we already issued it then we don't need to do it again
        // because reduction views only work from a single level
        if (issued_reductions.find(it->first) != issued_reductions.end())
          continue;
        FieldMask overlap = it->second & close_mask;
        if (!!overlap)
          valid_reductions[it->first] = overlap;
        issued_reductions.insert(it->first);
      }
      if (!valid_reductions.empty())
        node->issue_update_reductions(target, close_mask, version_info,
                                      local_proc, valid_reductions, op);
    }

    /////////////////////////////////////////////////////////////
    // PremapTraverser 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PremapTraverser::PremapTraverser(RegionTreePath &p, const MappableInfo &i)
      : PathTraverser(p), info(i)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PremapTraverser::PremapTraverser(const PremapTraverser &rhs)
      : PathTraverser(rhs.path), info(rhs.info)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PremapTraverser::~PremapTraverser(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PremapTraverser& PremapTraverser::operator=(const PremapTraverser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool PremapTraverser::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      return premap_node(node, node->handle);
    }

    //--------------------------------------------------------------------------
    bool PremapTraverser::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      return premap_node(node, node->parent->handle);
    }

    //--------------------------------------------------------------------------
    bool PremapTraverser::premap_node(RegionTreeNode *node, 
                                                   LogicalRegion closing_handle)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(node->context, PERFORM_PREMAP_CLOSE_CALL);
#endif
      PhysicalState *state = node->get_physical_state(info.ctx, 
                                                      info.version_info);
      // Update our physical state to indicate which child
      // we are opening and in which fields
      if (has_child)
      {
        state->children.valid_fields |= info.traversal_mask;
        LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
                            state->children.open_children.find(next_child);
        if (finder == state->children.open_children.end())
          state->children.open_children[next_child] = info.traversal_mask;
        else
          finder->second |= info.traversal_mask;
      }
      // Finally check to see if we arrived at our destination node
      // in which case we should pull down the valid instance views
      // to our node
      else if (!IS_REDUCE(info.req))
      {
        // If we're not doing a reduction, pull down all the valid views
        // and then record the valid physical instances unless we're
        // doing a reductions in which case it doesn't matter
        node->pull_valid_instance_views(info.ctx, state, info.traversal_mask, 
                                        true/*need space*/, info.version_info);
        // Find the memories for all the instances and report
        // which memories have full instances and which ones
        // only have partial instances
        for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
              state->valid_views.begin(); it != state->valid_views.end(); it++)
        {
          if (it->first->is_deferred_view())
            continue;
#ifdef DEBUG_HIGH_LEVEL
          assert(it->first->as_instance_view()->is_materialized_view());
#endif
          MaterializedView *cur_view = 
            it->first->as_instance_view()->as_materialized_view();
          Memory mem = cur_view->get_location();
          std::map<Memory,bool>::iterator finder = 
            info.req.current_instances.find(mem);
          if ((finder == info.req.current_instances.end()) 
              || !finder->second)
          {
            bool full_instance = !(info.traversal_mask - it->second);
            if (finder == info.req.current_instances.end())
              info.req.current_instances[mem] = full_instance;
            else
              finder->second = full_instance;
          }
        }
        // Also set the maximum blocking factor for this region
        Domain node_domain = node->get_domain_blocking();
        if (node_domain.get_dim() == 0)
        {
          const LowLevel::ElementMask &mask = 
            node_domain.get_index_space().get_valid_mask();
          info.req.max_blocking_factor = mask.get_num_elmts();
        }
        else
          info.req.max_blocking_factor = node_domain.get_volume();
      }
      return true;
    }

    /////////////////////////////////////////////////////////////
    // MappingTraverser 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    MappingTraverser<RESTRICTED>::MappingTraverser(RegionTreePath &p, 
                                                   const MappableInfo &i,
                                       const RegionUsage &u, const FieldMask &m,
                                       Processor proc, unsigned idx)
      : PathTraverser(p), info(i), usage(u), user_mask(m), 
        target_proc(proc), index(idx)
    //--------------------------------------------------------------------------
    {
    }
    
    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    MappingTraverser<RESTRICTED>::MappingTraverser(const MappingTraverser &rhs)
      : PathTraverser(rhs.path), info(rhs.info), usage(RegionUsage()),
        user_mask(FieldMask()), target_proc(rhs.target_proc), index(rhs.index)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    MappingTraverser<RESTRICTED>::~MappingTraverser(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    MappingTraverser<RESTRICTED>& MappingTraverser<RESTRICTED>::operator=(  
                                                    const MappingTraverser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    bool MappingTraverser<RESTRICTED>::visit_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      if (!has_child)
      {
        // Now we're ready to map this instance
        // Separate paths for reductions and non-reductions
        if (!IS_REDUCE(info.req))
        {
          // See if we can get or make a physical instance
          // that we can use
          if (RESTRICTED)
            return map_restricted_physical(node);
          else
            return map_physical_region(node);
        }
        else
        {
          // See if we can make or use an existing reduction instance
          if (RESTRICTED)
            return map_restricted_reduction(node);
          else
            return map_reduction_region(node);
        }
      }
      else
      {
        // Still not there yet, traverse the node
        traverse_node(node);
        return true;
      } 
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    bool MappingTraverser<RESTRICTED>::visit_partition(PartitionNode *node)
    //--------------------------------------------------------------------------
    {
      // Since we know we're mapping we know we won't ever stop
      // on a partition node
#ifdef DEBUG_HIGH_LEVEL
      assert(has_child);
#endif
      traverse_node(node);
      return true;
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    const MappingRef& MappingTraverser<RESTRICTED>::get_instance_ref(void) const
    //--------------------------------------------------------------------------
    {
      return result;
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    void MappingTraverser<RESTRICTED>::traverse_node(RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(node->context, MAPPING_TRAVERSE_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(has_child);
#endif
      PhysicalState *state = node->get_physical_state(info.ctx,
                                                      info.version_info);
      state->children.valid_fields |= info.traversal_mask;
      LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
                              state->children.open_children.find(next_child);
      if (finder == state->children.open_children.end())
        state->children.open_children[next_child] = info.traversal_mask;
      else
        finder->second |= info.traversal_mask;
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    bool MappingTraverser<RESTRICTED>::map_physical_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(node->context, MAP_PHYSICAL_REGION_CALL);
#endif
      std::vector<Memory> &chosen_order = info.req.target_ranking;
      const std::set<FieldID> &additional_fields = info.req.additional_fields;
      // Clamp the selected blocking factor
      const size_t blocking_factor = 
        (info.req.blocking_factor <= info.req.max_blocking_factor) ? 
        info.req.blocking_factor : info.req.max_blocking_factor;
      // Filter out any memories that are not visible from 
      // the target processor if there is a processor that 
      // we're targeting (e.g. never do this for premaps)
      // We can also skip this if region requirement has a NO_ACCESS_FLAG
      if (!chosen_order.empty() && target_proc.exists() &&
          !(info.req.flags & NO_ACCESS_FLAG))
      {
        Machine machine = Machine::get_machine();
        std::set<Memory> visible_memories;
	machine.get_visible_memories(target_proc, visible_memories);
        if (visible_memories.empty() && 
            (target_proc.kind() == Processor::PROC_GROUP))
        {
          log_run.warning("Comment on github that you've encountered "
                          "issue #35");
        }
        else
        {
          std::vector<Memory> filtered_memories;
          filtered_memories.reserve(chosen_order.size());
          for (std::vector<Memory>::const_iterator it = chosen_order.begin();
                it != chosen_order.end(); it++)
          {
            if (visible_memories.find(*it) == visible_memories.end())
            {
              log_region.warning("WARNING: Mapper specified memory " IDFMT 
                                       " which is not visible from processor "
                                       "" IDFMT " when mapping region %d of "
                                       "mappable (ID %lld)!  Removing memory "
                                       "from the chosen ordering!", it->id, 
                                       target_proc.id, index, 
                           info.op->get_mappable()->get_unique_mappable_id());
              continue;
            }
            // Otherwise we can add it to the list of filtered memories
            filtered_memories.push_back(*it);
          }
          chosen_order = filtered_memories;
        }
      }
      // Get the set of currently valid instances
      LegionMap<LogicalView*,FieldMask>::aligned valid_instances;
      // Check to see if the mapper requested any additional fields in this
      // instance.  If it did, then re-run the computation to get the list
      // of valid instances with the right set of fields
      std::set<FieldID> new_fields = info.req.privilege_fields;
      PhysicalState *state = node->get_physical_state(info.ctx,
                                                      info.version_info);
      if (!additional_fields.empty())
      {
        new_fields.insert(additional_fields.begin(),
                             additional_fields.end());
        FieldMask additional_mask = 
          node->column_source->get_field_mask(new_fields);
        node->find_valid_instance_views(info.ctx, state, additional_mask,
                                        additional_mask, info.version_info,
                                        true/*space*/, valid_instances);
      }
      else
      {
        node->find_valid_instance_views(info.ctx, state, user_mask,
                                        user_mask, info.version_info,
                                        true/*space*/, valid_instances);
      }
      // Compute the set of valid memories and filter out instance which
      // do not have the proper blocking factor in the process
      std::map<Memory,bool> valid_memories;
      {
        std::vector<LogicalView*> to_erase;
        for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
              valid_instances.begin(); it != valid_instances.end(); it++)
        {
          // Remove any deferred instances
          if (it->first->is_deferred_view())
          {
            to_erase.push_back(it->first);
            continue;
          }
#ifdef DEBUG_HIGH_LEVEL
          assert(it->first->as_instance_view()->is_materialized_view());
#endif
          MaterializedView *current_view = 
            it->first->as_instance_view()->as_materialized_view();
          // For right now allow blocking factors that are greater
          // than or equal to the requested blocking factor
          size_t bf = current_view->get_blocking_factor();
          if (bf >= blocking_factor)
          {
            Memory m = current_view->get_location();
            if (valid_memories.find(m) == valid_memories.end())
              valid_memories[m] = !(user_mask - it->second);
            else if (!valid_memories[m])
              valid_memories[m] = !(user_mask - it->second);
            // Otherwise we already have an instance in this memory that
            // dominates all the fields in which case we don't care
          }
          else
          {
            to_erase.push_back(it->first);
          }
        }
        for (std::vector<LogicalView*>::const_iterator it = to_erase.begin();
              it != to_erase.end(); it++)
          valid_instances.erase(*it);  
        to_erase.clear();
      }

      MaterializedView *chosen_inst = NULL;
      FieldMask needed_fields; 
      // Go through each of the memories provided by the mapper
      for (std::vector<Memory>::const_iterator mit = chosen_order.begin();
            mit != chosen_order.end(); mit++)
      {
        // See if it has any valid instances
        if (valid_memories.find(*mit) != valid_memories.end())
        {
          // Already have a valid instance with at least a 
          // few valid fields, figure out if it has all or 
          // some of the fields valid
          if (valid_memories[*mit])
          {
            // We've got an instance with all the valid fields, go find it
            for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator
                  it = valid_instances.begin(); 
                  it != valid_instances.end(); it++)
            {
              // At this point we know everything is a materialized view
#ifdef DEBUG_HIGH_LEVEL
              assert(it->first->is_instance_view());
              assert(it->first->as_instance_view()->is_materialized_view());
#endif
              MaterializedView *cur_view = 
                it->first->as_instance_view()->as_materialized_view();
              if (cur_view->get_location() != (*mit))
                continue;
              if (!(user_mask - it->second))
              {
                // Check to see if have any WAR dependences
                // in which case we'll skip it for a something better
                if (info.req.enable_WAR_optimization && HAS_WRITE(info.req) 
                    && cur_view->has_war_dependence(usage, user_mask))
                  continue;
                // No WAR problems, so it it is good
                chosen_inst = cur_view;
                // No need to set needed fields since everything is valid
                break;
              }
            }
            // If we found a good instance break, otherwise go onto
            // the partial instances
            if (chosen_inst != NULL)
            {
#ifdef DEBUG_HIGH_LEVEL
              assert(!needed_fields);
#endif
              break;
            }
          }
          // Do this if we couldn't find a better choice
          // Note we can't do this in the read-only case because we might 
          // end up issuing multiple copies to the same location.
          // On second thought this might be ok since they are both 
          // reading and anybody else who mutates this instance will
          // see both copies because of mapping dependences.
          // if (!IS_READ_ONLY(usage))
          {
            // These are instances which have space for all the required fields
            // but only a subset of those fields contain valid data.
            // Find the valid instance with the most valid fields to use.
            int covered_fields = -1;
            for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator 
                  it = valid_instances.begin(); 
                  it != valid_instances.end(); it++)
            {
              // At this point we know everything is a materialized view
#ifdef DEBUG_HIGH_LEVEL
              assert(it->first->is_instance_view());
              assert(it->first->as_instance_view()->is_materialized_view());
#endif
              MaterializedView *cur_view = 
                it->first->as_instance_view()->as_materialized_view();
              if (cur_view->get_location() != (*mit))
                continue;
              int cf = FieldMask::pop_count(it->second);
              if (cf > covered_fields)
              {
                // Check to see if we have any WAR dependences 
                // which might disqualify us
                if (info.req.enable_WAR_optimization && HAS_WRITE(info.req) 
                    && cur_view->has_war_dependence(usage, user_mask))
                  continue;
                covered_fields = cf;
                chosen_inst = cur_view; 
                needed_fields = user_mask - it->second; 
              }
            }
            // If we got a good one break out, otherwise we'll try 
            // to make a new instance
            if (chosen_inst != NULL)
            {
#ifdef DEBUG_HIGH_LEVEL
              assert(!!needed_fields);
#endif
              break;
            }
          }
        }
        // If it didn't find a valid instance, try to make one
        chosen_inst = node->create_instance(*mit, new_fields, 
                                            blocking_factor,
                                          info.op->get_mappable()->get_depth(),
                                            info.op);
        if (chosen_inst != NULL)
        {
          // We successfully made an instance
          needed_fields = user_mask;
          // Make sure to tell our physical state
          state->record_created_instance(chosen_inst);
          break;
        }
      }
      // Save our chosen instance if it exists in the mapping
      // reference and then return if we have an instance
      if (chosen_inst != NULL)
        result = MappingRef(chosen_inst, needed_fields);
      return (chosen_inst != NULL);
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    bool MappingTraverser<RESTRICTED>::map_reduction_region(RegionNode *node)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(node->context, MAP_REDUCTION_REGION_CALL);
#endif
      std::vector<Memory> &chosen_order = info.req.target_ranking;
      // Filter out any memories that are not visible from 
      // the target processor if there is a processor that 
      // we're targeting (e.g. never do this for premaps)
      if (!chosen_order.empty() && target_proc.exists())
      {
        Machine machine = Machine::get_machine();
        std::set<Memory> visible_memories;
	machine.get_visible_memories(target_proc, visible_memories);
        std::vector<Memory> filtered_memories;
        filtered_memories.reserve(chosen_order.size());
        for (std::vector<Memory>::const_iterator it = chosen_order.begin();
              it != chosen_order.end(); it++)
        {
          if (visible_memories.find(*it) != visible_memories.end())
            filtered_memories.push_back(*it);
          else
          {
            log_region.warning("WARNING: Mapper specified memory " IDFMT
                                     " which is not visible from processor "
                                     IDFMT " when mapping region %d of mappable"
                                     " (ID %lld)!  Removing memory from the "
                                     "chosen ordering!", it->id, 
                                     target_proc.id, index, 
                             info.op->get_mappable()->get_unique_mappable_id());
          }
        }
        chosen_order = filtered_memories;
      }

      std::set<ReductionView*> valid_views;
      PhysicalState *state = node->get_physical_state(info.ctx,
                                                      info.version_info);
      node->find_valid_reduction_views(info.ctx, state, usage.redop, 
                              user_mask, info.version_info, valid_views);

      // Compute the set of valid memories
      std::set<Memory> valid_memories;
      for (std::set<ReductionView*>::const_iterator it = valid_views.begin();
            it != valid_views.end(); it++)
      {
        valid_memories.insert((*it)->get_location());
      }

      ReductionView *chosen_inst = NULL;
      // Go through each of the valid memories and see if we can either find
      // a reduction instance or we can make one
      for (std::vector<Memory>::const_iterator mit = chosen_order.begin();
            mit != chosen_order.end(); mit++)
      {
        if (valid_memories.find(*mit) != valid_memories.end())
        {
          // We've got a valid instance, let's go find it
          for (std::set<ReductionView*>::const_iterator it = 
                valid_views.begin(); it != valid_views.end(); it++)
          {
            if ((*it)->get_location() == *mit)
            {
              chosen_inst = *it;
              break;
            }
          }
#ifdef DEBUG_HIGH_LEVEL
          assert(chosen_inst != NULL);
#endif
          // We've found the instance that we want
          break;
        }
        else
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(info.req.privilege_fields.size() == 1);
#endif
          FieldID fid = *(info.req.privilege_fields.begin());
          // Try making a reduction instance in this memory
          chosen_inst = node->create_reduction(*mit, fid, 
                                               info.req.reduction_list,
                                               info.req.redop,
                                               info.op);
          if (chosen_inst != NULL)
          {
            state->record_created_instance(chosen_inst);
            break;
          }
        }
      }
      if (chosen_inst != NULL)
        result = MappingRef(chosen_inst,FieldMask());
      return (chosen_inst != NULL);
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    bool MappingTraverser<RESTRICTED>::map_restricted_physical(RegionNode *node)
    //--------------------------------------------------------------------------
    {
      // Grab the set of valid instances, we should find exactly one
      // that matches all the fields, if not that is very bad
      LegionMap<LogicalView*,FieldMask>::aligned valid_instances;
      PhysicalState *state = node->get_physical_state(info.ctx,
                                                      info.version_info);
      node->find_valid_instance_views(info.ctx, state, user_mask,
                                      user_mask, info.version_info,
                                      false/*space*/, valid_instances);
      InstanceView *chosen_inst = NULL;
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_instances.begin(); it != valid_instances.end(); it++)
      {
        // Skip any deferred views
        if (it->first->is_deferred_view())
          continue;
#ifdef DEBUG_HIGH_LEVEL
        assert(it->first->is_instance_view());
#endif
        InstanceView *inst_view = it->first->as_instance_view();
        FieldMask uncovered = user_mask - it->second;
        // If all the fields were valid, record it
        if (!uncovered)
        {
          if (chosen_inst != NULL)
          {
            for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it2=
                  valid_instances.begin(); it2 != valid_instances.end(); it2++)
            {
              LogicalView *view = it2->first;
              FieldMask mask = it2->second;
              printf("%p, %p\n", view, &mask);
            }
            log_run.error("Multiple valid instances for restricted cohernece! "
                          "This is almost certainly a runtime bug. Please "
                          "create a minimal test case and report it.");
            assert(false);
          }
          else
            chosen_inst = inst_view;
        }
      }
      if (chosen_inst == NULL)
      {
        log_run.error("No single instance is valid for restricted coherence! "
                      "Need support for multiple instances. This is currently "
                      "a pending feature. Please report your use case.");
        assert(false);
      }
      // We know we don't need any fields to be brought up to date
      result = MappingRef(chosen_inst, FieldMask());
      return (chosen_inst != NULL);
    }

    //--------------------------------------------------------------------------
    template<bool RESTRICTED>
    bool MappingTraverser<RESTRICTED>::map_restricted_reduction(
                                                               RegionNode *node)
    //--------------------------------------------------------------------------
    {
      // TODO: implement this later
      assert(false);
      return false;
    }

    /////////////////////////////////////////////////////////////
    // States 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalState::LogicalState(RegionTreeNode *node, ContextID ctx)
    //--------------------------------------------------------------------------
    {
      // This first time we create the state, we need to pull down
      // any restricted instances from our parent state
      RegionTreeNode *parent = node->get_parent();
      if (parent != NULL)
        parent->set_restricted_fields(ctx, restricted_fields);
      // Everybody starts with version number zero
      field_versions[0] = FieldMask(FIELD_ALL_ONES);
    }

    //--------------------------------------------------------------------------
    LogicalState::LogicalState(const LogicalState &rhs)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalState::~LogicalState(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalState& LogicalState::operator=(const LogicalState &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* LogicalState::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<LogicalState,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void* LogicalState::operator new[](size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<LogicalState,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void LogicalState::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void LogicalState::operator delete[](void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void LogicalState::reset(void)
    //--------------------------------------------------------------------------
    {
      field_states.clear();
      curr_epoch_users.clear();
      prev_epoch_users.clear();
      restricted_fields.clear();
      dirty_below.clear();
      field_versions.clear();
      field_versions[0] = FieldMask(FIELD_ALL_ONES);
      outstanding_reduction_fields.clear();
      outstanding_reductions.clear();
    } 

    //--------------------------------------------------------------------------
    FieldState::FieldState(const GenericUser &user, const FieldMask &m, 
                           const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      redop = 0;
      if (IS_READ_ONLY(user.usage))
        open_state = OPEN_READ_ONLY;
      else if (IS_WRITE(user.usage))
        open_state = OPEN_READ_WRITE;
      else if (IS_REDUCE(user.usage))
      {
        open_state = OPEN_SINGLE_REDUCE;
        redop = user.usage.redop;
      }
      valid_fields = m;
      open_children[c] = m;
      rebuild_timeout = 1;
    }

    //--------------------------------------------------------------------------
    bool FieldState::overlaps(const FieldState &rhs) const
    //--------------------------------------------------------------------------
    {
      if (redop != rhs.redop)
        return false;
      if (redop == 0)
        return (open_state == rhs.open_state);
      else
      {
#ifdef DEBUG_HIGH_LEVEL
        assert((open_state == OPEN_SINGLE_REDUCE) ||
               (open_state == OPEN_MULTI_REDUCE));
        assert((rhs.open_state == OPEN_SINGLE_REDUCE) ||
               (rhs.open_state == OPEN_MULTI_REDUCE));
#endif
        // Only support merging reduction fields with exactly the
        // same mask which should be single fields for reductions
        return (valid_fields == rhs.valid_fields);
      }
    }

    //--------------------------------------------------------------------------
    void FieldState::merge(const FieldState &rhs, RegionTreeNode *node)
    //--------------------------------------------------------------------------
    {
      valid_fields |= rhs.valid_fields;
      for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
            rhs.open_children.begin(); it != rhs.open_children.end(); it++)
      {
        LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
                                      open_children.find(it->first);
        if (finder == open_children.end())
          open_children[it->first] = it->second;
        else
          finder->second |= it->second;
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(redop == rhs.redop);
#endif
      if (redop > 0)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(!open_children.empty());
#endif
        // For the reductions, handle the case where we need to merge
        // reduction modes, if they are all disjoint, we don't need
        // to distinguish between single and multi reduce
        if (node->are_all_children_disjoint())
        {
          open_state = OPEN_READ_WRITE;
          redop = 0;
        }
        else
        {
          if (open_children.size() == 1)
            open_state = OPEN_SINGLE_REDUCE;
          else
            open_state = OPEN_MULTI_REDUCE;
        }
      }
    }

    //--------------------------------------------------------------------------
    void FieldState::print_state(TreeStateLogger *logger,
                                 const FieldMask &capture_mask) const
    //--------------------------------------------------------------------------
    {
      switch (open_state)
      {
        case NOT_OPEN:
          {
            logger->log("Field State: NOT OPEN (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_READ_WRITE:
          {
            logger->log("Field State: OPEN READ WRITE (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_READ_ONLY:
          {
            logger->log("Field State: OPEN READ-ONLY (%ld)", 
                        open_children.size());
            break;
          }
        case OPEN_SINGLE_REDUCE:
          {
            logger->log("Field State: OPEN SINGLE REDUCE Mode %d (%ld)", 
                        redop, open_children.size());
            break;
          }
        case OPEN_MULTI_REDUCE:
          {
            logger->log("Field State: OPEN MULTI REDUCE Mode %d (%ld)", 
                        redop, open_children.size());
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        FieldMask overlap = it->second & capture_mask;
        if (!overlap)
          continue;
        char *mask_buffer = overlap.to_string();
        switch (it->first.get_dim())
        {
          case 0:
            {
              logger->log("Color %d   Mask %s", 
                          it->first.get_index(), mask_buffer);
              break;
            }
          case 1:
            {
              logger->log("Color %d   Mask %s", 
                          it->first[0], mask_buffer);
              break;
            }
          case 2:
            {
              logger->log("Color (%d,%d)   Mask %s", 
                          it->first[0], it->first[1],
                          mask_buffer);
              break;
            }
          case 3:
            {
              logger->log("Color %d   Mask %s", 
                          it->first[0], it->first[1],
                          it->first[2], mask_buffer);
              break;
            }
          default:
            assert(false); // implemenent more dimensions
        }
        free(mask_buffer);
      }
      logger->up();
    }

    /////////////////////////////////////////////////////////////
    // Closers 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CopyTracker::CopyTracker(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    Event CopyTracker::get_termination_event(void) const
    //--------------------------------------------------------------------------
    {
      return Event::merge_events(copy_events);
    }

    //--------------------------------------------------------------------------
    LogicalCloser::LogicalCloser(ContextID c, const LogicalUser &u, 
                                 bool val, bool capture)
      : ctx(c), user(u), validates(val), capture_users(capture)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    LogicalCloser::LogicalCloser(const LogicalCloser &rhs)
      : user(rhs.user), validates(rhs.validates),
        capture_users(rhs.capture_users)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    LogicalCloser::~LogicalCloser(void)
    //--------------------------------------------------------------------------
    {
      // Clear out our version infos
      leave_open_versions.clear();
      force_close_versions.clear();
    }

    //--------------------------------------------------------------------------
    LogicalCloser& LogicalCloser::operator=(const LogicalCloser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::record_closed_child(const ColorPoint &child, 
                                            const FieldMask &mask,
                                            bool leave_open)
    //--------------------------------------------------------------------------
    {
      closed_mask |= mask;
      // IMPORTANT: Always do this even if we don't have any closed users
      // They could have been pruned out because they finished executing, but
      // we still need to do the close operation.
      if (leave_open)
      {
        leave_open_mask |= mask;
        LegionMap<ColorPoint,ClosingInfo>::aligned::iterator finder = 
                                              leave_open_children.find(child);
        if (finder != leave_open_children.end())
        {
          finder->second.child_fields |= mask;
          finder->second.child_users.insert(finder->second.child_users.end(),
                                      closed_users.begin(), closed_users.end());
        }
        else
          leave_open_children[child] = ClosingInfo(mask, closed_users);
      }
      else
      {
        LegionMap<ColorPoint,ClosingInfo>::aligned::iterator finder = 
                                              force_close_children.find(child);
        if (finder != force_close_children.end())
        {
          finder->second.child_fields |= mask;
          finder->second.child_users.insert(finder->second.child_users.end(),
                                      closed_users.begin(), closed_users.end());
        }
        else
          force_close_children[child] = ClosingInfo(mask, closed_users);
      }
      // Always clean out our list of closed users
      closed_users.clear();
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::record_flush_only_fields(const FieldMask &flush_only)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!flush_only_fields);
#endif
      flush_only_fields = flush_only;
      closed_mask |= flush_only;
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::initialize_close_operations(RegionTreeNode *target, 
                                                   Operation *creator,
                                                   const VersionInfo &ver_info,
                                                   const RestrictInfo &res_info,
                                                   const TraceInfo &trace_info)
    //--------------------------------------------------------------------------
    {
      // First sort the close operations into sets of fields which all
      // close the same sets of children
      if (!leave_open_children.empty())
      {
        LegionList<ClosingSet>::aligned leave_open;
        compute_close_sets(leave_open_children, leave_open);
        create_close_operations(target, creator, leave_open_versions, ver_info,
                                res_info, trace_info, true/*leave open*/, 
                                leave_open, leave_open_closes);
      }
      if (!force_close_children.empty())
      {
        LegionList<ClosingSet>::aligned force_close;
        compute_close_sets(force_close_children, force_close);
        create_close_operations(target, creator, force_close_versions, ver_info,
                                res_info, trace_info, false/*leave open*/, 
                                force_close, force_close_closes);
      }
      // Finally if we have any fields which are flush only
      // make a close operation for them and add it to force close
      if (!!flush_only_fields)
      {
        std::set<ColorPoint> empty_children;
        InterCloseOp *flush_op = target->create_close_op(creator,
                                                         flush_only_fields,
                                                         false/*leave open*/,
                                                         empty_children,
                                                         force_close_versions,
                                                         ver_info, res_info,
                                                         trace_info);
        force_close_closes[flush_op] = LogicalUser(flush_op, 0/*idx*/,
                              RegionUsage(flush_op->get_region_requirement()),
                              flush_only_fields);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::add_next_child(const ColorPoint &next_child)
    //--------------------------------------------------------------------------
    {
      // Only need to add children to leave open closes
      for (LegionMap<InterCloseOp*,LogicalUser>::aligned::const_iterator it = 
            leave_open_closes.begin(); it != leave_open_closes.end(); it++)
      {
        it->first->add_next_child(next_child);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void LogicalCloser::compute_close_sets(
                    const LegionMap<ColorPoint,ClosingInfo>::aligned &children,
                    LegionList<ClosingSet>::aligned &close_sets)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<ColorPoint,ClosingInfo>::aligned::const_iterator 
             cit = children.begin(); cit != children.end(); cit++)
      {
        bool inserted = false;
        FieldMask remaining = cit->second.child_fields;
        for (LegionList<ClosingSet>::aligned::iterator it = 
              close_sets.begin(); it != close_sets.end(); it++)
        {
          // Easy case, check for equality
          if (remaining == it->closing_mask)
          {
            it->children.insert(cit->first);
            inserted = true;
            break;
          }
          FieldMask overlap = remaining & it->closing_mask;
          // If they are disjoint keep going
          if (!overlap)
            continue;
          // We are dominated, split into two sets
          // reusing existing set and making a new set
          if (overlap == remaining)
          {
            // Leave the existing set and make it the difference
            it->closing_mask -= overlap;
            close_sets.push_back(ClosingSet(overlap));
            ClosingSet &last = close_sets.back();
            last.children = it->children;
            last.children.insert(cit->first);
            inserted = true;
            break;
          }
          // We dominate the existing set
          if (overlap == it->closing_mask)
          {
            // Add ourselves to the existing set and then
            // keep going for the remaining fields
            it->children.insert(cit->first);
            remaining -= overlap;
            continue;
          }
          // Hard case, neither dominates, compute
          // three distinct sets of fields, keep left
          // one in place and reduce scope, add a new
          // one at the end for overlap, continue
          // iterating for the right one
          it->closing_mask -= overlap;
          const std::set<ColorPoint> &temp_children = it->children;
          it = close_sets.insert(it, ClosingSet(overlap));
          it->children = temp_children;
          it->children.insert(cit->first);
          remaining -= overlap;
          continue;
        }
        // If we didn't add it yet, add it now
        if (!inserted)
        {
          close_sets.push_back(ClosingSet(remaining));
          ClosingSet &last = close_sets.back();
          last.children.insert(cit->first);
        }
      }
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::create_close_operations(RegionTreeNode *target,
                            Operation *creator, const VersionInfo &local_info,
                            const VersionInfo &version_info,
                            const RestrictInfo &restrict_info, 
                            const TraceInfo &trace_info, bool leave_open,
                            const LegionList<ClosingSet>::aligned &close_sets,
                       LegionMap<InterCloseOp*,LogicalUser>::aligned &close_ops)
    //--------------------------------------------------------------------------
    {
      for (LegionList<ClosingSet>::aligned::const_iterator it = 
            close_sets.begin(); it != close_sets.end(); it++)
      {
        InterCloseOp *close_op = target->create_close_op(creator, 
                                                       it->closing_mask,
                                                       leave_open, it->children,
                                                       local_info,
                                                       version_info,
                                                       restrict_info, 
                                                       trace_info);
        if (leave_open)
          leave_open_closes[close_op] = LogicalUser(close_op, 0/*idx*/,
                        RegionUsage(close_op->get_region_requirement()),
                        it->closing_mask);
        else
          force_close_closes[close_op] = LogicalUser(close_op, 0/*idx*/,
                        RegionUsage(close_op->get_region_requirement()),
                        it->closing_mask);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::perform_dependence_analysis(const LogicalUser &current,
                                                    const FieldMask &open_below,
              LegionList<LogicalUser,CURR_LOGICAL_ALLOC>::track_aligned &cusers,
              LegionList<LogicalUser,PREV_LOGICAL_ALLOC>::track_aligned &pusers)
    //--------------------------------------------------------------------------
    {
      // We also need to do dependence analysis against all the other operations
      // that this operation recorded dependences on above in the tree so we
      // don't run too early.
      LegionList<LogicalUser,LOGICAL_REC_ALLOC>::track_aligned &above_users = 
                                              current.op->get_logical_records();
      register_dependences(current, open_below, leave_open_closes, 
                           leave_open_children, above_users, cusers, pusers);
      register_dependences(current, open_below, force_close_closes, 
                           force_close_children, above_users, cusers, pusers);
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::register_dependences(const LogicalUser &current,
                                             const FieldMask &open_below,
           LegionMap<InterCloseOp*,LogicalUser>::aligned &closes,
           LegionMap<ColorPoint,ClosingInfo>::aligned &children,
           LegionList<LogicalUser,LOGICAL_REC_ALLOC >::track_aligned &abv_users,
           LegionList<LogicalUser,CURR_LOGICAL_ALLOC>::track_aligned &cur_users,
           LegionList<LogicalUser,PREV_LOGICAL_ALLOC>::track_aligned &pre_users)
    //--------------------------------------------------------------------------
    {
      // Start dependence analysis for all our closes
      for (LegionMap<InterCloseOp*,LogicalUser>::aligned::iterator op_it = 
            closes.begin(); op_it != closes.end(); op_it++)
      {
        // Mark that we are starting our dependence analysis
        op_it->first->begin_dependence_analysis();
        // First tell the operation to register dependences on any children
        // Register dependences on any interfering children
        // We know that only field non-interference is interesting here
        // because close operations have READ_WRITE EXCLUSIVE
        const FieldMask close_op_mask = op_it->second.field_mask;
        // Get the set of children being closed
        const std::set<ColorPoint> &colors = 
                                        op_it->first->get_target_children();
        for (std::set<ColorPoint>::const_iterator 
              cit = colors.begin(); cit != colors.end(); cit++)
        {
          LegionMap<ColorPoint,ClosingInfo>::aligned::iterator finder = 
                                                        children.find(*cit);
#ifdef DEBUG_HIGH_LEVEL
          assert(finder != children.end());
#endif
          LegionList<LogicalUser,CLOSE_LOGICAL_ALLOC>::track_aligned 
            &child_users = finder->second.child_users;
          // A tricky case here.  We know the current operation is
          // going to register dependences on this close operation,
          // so we can't have the close operation register depencnes
          // on any other users from the same op as the current one
          // we are doing the analysis for (e.g. other region reqs)
          if (!child_users.empty())
          {
            RegionTreeNode::perform_dependence_checks<CLOSE_LOGICAL_ALLOC,
              false/*record*/, true/*has skip*/, false/*track dom*/>(
                                          op_it->second, child_users,
                                          close_op_mask, open_below,
                                          false/*validates*/,
                                          current.op, current.gen);
            // We own mapping references on each one of the users so 
            // we need to remove them once we are done
            for (LegionList<LogicalUser,CLOSE_LOGICAL_ALLOC>::
                  track_aligned::const_iterator it = child_users.begin(); 
                  it != child_users.end(); it++)
            {
              it->op->remove_mapping_reference(it->gen);
            }
            child_users.clear();
          }
        }
        // Next do checks against any operations above in the tree which
        // the operation already recorded dependences. No need for skip
        // here because we know the operation didn't register any 
        // dependences against itself.
        if (!abv_users.empty())
          RegionTreeNode::perform_dependence_checks<LOGICAL_REC_ALLOC,
              false/*record*/, false/*has skip*/, false/*track dom*/>(
                                         op_it->second, abv_users,
                                         close_op_mask, open_below,
                                         false/*validates*/,
                                         current.op, current.gen);
        // Finally register any dependences on users at this level
        // See the note above the for the tricky case
        FieldMask dominator_mask; 
        if (!cur_users.empty())
          dominator_mask = 
            RegionTreeNode::perform_dependence_checks<CURR_LOGICAL_ALLOC,
              false/*record*/, true/*has skip*/, true/*track dom*/>(
                                        op_it->second, cur_users,
                                        close_op_mask, open_below,
                                        false/*validates*/,
                                        current.op, current.gen);
        FieldMask non_dominated_mask = close_op_mask - dominator_mask;
        if (!!non_dominated_mask && !pre_users.empty())
          RegionTreeNode::perform_dependence_checks<PREV_LOGICAL_ALLOC,
            false/*record*/, true/*has skip*/, false/*track dom*/>(
                                 op_it->second, pre_users, 
                                 non_dominated_mask, open_below,
                                 false/*validates*/,
                                 current.op, current.gen);
        // Before we kick off this operation, add a mapping
        // reference to it since we know we are going to put it
        // in the state of the logical region tree
        op_it->first->add_mapping_reference(op_it->first->get_generation());
        // Mark that we are done, this puts the close op in the pipeline!
        // This is why we cache the LogicalUser before kicking off the op
        op_it->first->end_dependence_analysis();
      }
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::register_close_operations(
               LegionList<LogicalUser,CURR_LOGICAL_ALLOC>::track_aligned &users)
    //--------------------------------------------------------------------------
    {
      // Add our close operations onto the list
      // Note we already added our mapping references when we made them
      for (LegionMap<InterCloseOp*,LogicalUser>::aligned::const_iterator it = 
            leave_open_closes.begin(); it != leave_open_closes.end(); it++)
      {
        users.push_back(it->second);
      }
      for (LegionMap<InterCloseOp*,LogicalUser>::aligned::const_iterator it = 
            force_close_closes.begin(); it != force_close_closes.end(); it++)
      {
        users.push_back(it->second);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::record_version_numbers(RegionTreeNode *node,
                                               LogicalState &state,
                                               const FieldMask &local_mask,
                                               bool leave_open)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!!local_mask);
#endif
      // Don't need the previous because writes were already done in the
      // sub-tree we are closing so the version number for the target
      // region has already been advanced.
      if (leave_open)
        node->record_version_numbers(state, local_mask, leave_open_versions, 
                                     false/*previous*/, false/*premap*/, 
                                     false/*path only*/, true/*final*/,
                                     false/*close top*/);
      else
        node->record_version_numbers(state, local_mask, force_close_versions, 
                                     false/*previous*/, false/*premap*/, 
                                     false/*path only*/, true/*final*/,
                                     false/*close top*/);
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::record_top_version_numbers(RegionTreeNode *node,
                                                   LogicalState &state)
    //--------------------------------------------------------------------------
    {
      // If we have any flush only fields, see if we need to bump their
      // version numbers before generating our close operations
      if (!!flush_only_fields)
      {
        FieldMask update_mask = flush_only_fields - state.dirty_below;
        if (!!update_mask)
          node->advance_version_numbers(state, update_mask);
      }
      // We don't need to advance the version numbers because we know
      // that was already done when we 
      if (!!leave_open_mask)
      {
        node->record_version_numbers(state, leave_open_mask,leave_open_versions,
                                     true/*previous*/, false/*premap*/, 
                                     false/*path only*/, true/*final*/,
                                     true/*close top*/);
        FieldMask force_close_mask = closed_mask - leave_open_mask;
        if (!!force_close_mask)
          node->record_version_numbers(state, force_close_mask,
                                       force_close_versions, true/*previous*/,
                                       false/*premap*/, false/*path only*/,
                                       true/*final*/, true/*close top*/);
      }
      else
      {
        // Normal case is simple
        node->record_version_numbers(state, closed_mask, force_close_versions, 
                                     true/*previous*/, false/*premap*/, 
                                     false/*path only*/, true/*final*/,
                                     true/*close top*/);
      }
    }

    //--------------------------------------------------------------------------
    void LogicalCloser::merge_version_info(VersionInfo &target,
                                           const FieldMask &merge_mask)
    //--------------------------------------------------------------------------
    {
      target.merge(leave_open_versions, merge_mask);
      target.merge(force_close_versions, merge_mask);
    }

    //--------------------------------------------------------------------------
    PhysicalCloser::PhysicalCloser(const MappableInfo &in, 
                                   bool open, LogicalRegion h)
      : info(in), handle(h), permit_leave_open(open), targets_selected(false)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalCloser::PhysicalCloser(const PhysicalCloser &rhs)
      : info(rhs.info), handle(rhs.handle), 
        permit_leave_open(rhs.permit_leave_open),
        upper_targets(rhs.get_lower_targets())
    //--------------------------------------------------------------------------
    {
      targets_selected = !upper_targets.empty(); 
    }

    //--------------------------------------------------------------------------
    PhysicalCloser::~PhysicalCloser(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PhysicalCloser& PhysicalCloser::operator=(const PhysicalCloser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    bool PhysicalCloser::needs_targets(void) const
    //--------------------------------------------------------------------------
    {
      return !targets_selected;
    }

    //--------------------------------------------------------------------------
    void PhysicalCloser::add_target(MaterializedView *target)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(target != NULL);
#endif
      upper_targets.push_back(target);
      targets_selected = true;
    }

    //--------------------------------------------------------------------------
    void PhysicalCloser::close_tree_node(RegionTreeNode *node,
                                         const FieldMask &closing_mask)
    //--------------------------------------------------------------------------
    {
      // Lower the targets
      lower_targets.resize(upper_targets.size());
      for (unsigned idx = 0; idx < upper_targets.size(); idx++)
        lower_targets[idx] = 
          upper_targets[idx]->get_materialized_subview(node->get_color());

      // Close the node
      node->close_physical_node(*this, closing_mask);

      // Clear out the lowered targets
      lower_targets.clear();
    }

    //--------------------------------------------------------------------------
    const std::vector<MaterializedView*>& PhysicalCloser::
                                                  get_upper_targets(void) const
    //--------------------------------------------------------------------------
    {
      return upper_targets;
    }

    //--------------------------------------------------------------------------
    const std::vector<MaterializedView*>& PhysicalCloser::
                                                  get_lower_targets(void) const
    //--------------------------------------------------------------------------
    {
      return lower_targets;
    }

    //--------------------------------------------------------------------------
    void PhysicalCloser::update_dirty_mask(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      dirty_mask |= mask;
    }

    //--------------------------------------------------------------------------
    const FieldMask& PhysicalCloser::get_dirty_mask(void) const
    //--------------------------------------------------------------------------
    {
      return dirty_mask;
    }

    //--------------------------------------------------------------------------
    void PhysicalCloser::update_node_views(RegionTreeNode *node,
                                           PhysicalState *state)
    //--------------------------------------------------------------------------
    {
      // Note that permit leave open means that we don't update
      // the dirty bits when we update the state
      node->update_valid_views(state, info.traversal_mask,
                               dirty_mask, upper_targets);
    } 

    //--------------------------------------------------------------------------
    CompositeCloser::CompositeCloser(ContextID c, VersionInfo &info, bool open)
      : ctx(c), permit_leave_open(open), version_info(info)
    //--------------------------------------------------------------------------
    {
      composite_version_info = new CompositeVersionInfo();
    }

    //--------------------------------------------------------------------------
    CompositeCloser::CompositeCloser(const CompositeCloser &rhs)
      : ctx(0), permit_leave_open(false), version_info(rhs.version_info)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    CompositeCloser::~CompositeCloser(void)
    //--------------------------------------------------------------------------
    {
      // Only delete the version info if there are no constructed nodes
      if (constructed_nodes.empty())
        delete composite_version_info;
    }

    //--------------------------------------------------------------------------
    CompositeCloser& CompositeCloser::operator=(const CompositeCloser &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    CompositeNode* CompositeCloser::get_composite_node(RegionTreeNode *node,
                                                       CompositeNode *parent)
    //--------------------------------------------------------------------------
    {
      std::map<RegionTreeNode*,CompositeNode*>::const_iterator finder = 
        constructed_nodes.find(node);
      if (finder != constructed_nodes.end())
        return finder->second;
      CompositeNode *result = legion_new<CompositeNode>(node, parent,
                                                        composite_version_info);
      constructed_nodes[node] = result;
      return result;
    }

    //--------------------------------------------------------------------------
    void CompositeCloser::create_valid_view(PhysicalState *state,
                                            CompositeNode *root,
                                            const FieldMask &closed_mask)
    //--------------------------------------------------------------------------
    {
      RegionTreeNode *node = root->logical_node;
      DistributedID did = 
                    node->context->runtime->get_available_distributed_id(false);
      CompositeView *composite_view = legion_new<CompositeView>(node->context, 
                                   did, node->context->runtime->address_space,
                                   node, node->context->runtime->address_space, 
                                   closed_mask, true/*register now*/);
      // Set the root value
      composite_view->add_root(root, closed_mask, true/*top*/);
      // Fill in the version infos
      VersionInfo &target_version_info = 
                                composite_version_info->get_version_info();
      target_version_info.clone_from(version_info);
      if (!reduction_views.empty())
      {
        for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it =
              reduction_views.begin(); it != reduction_views.end(); it++)
        {
          composite_view->update_reduction_views(it->first, it->second);
        }
      }
      // Now update the state of the node
      // Note that if we are permitted to leave the subregions
      // open then we don't make the view dirty
      node->update_valid_views(state, closed_mask,
                               true/*dirty*/, composite_view);
    }

    //--------------------------------------------------------------------------
    void CompositeCloser::capture_physical_state(CompositeNode *target,
                                                 RegionTreeNode *node,
                                                 PhysicalState *state,
                                                 const FieldMask &capture_mask,
                                                 FieldMask &dirty_mask)
    //--------------------------------------------------------------------------
    {
      // Do the capture and then update capture mask
      target->capture_physical_state(node, state, capture_mask, *this,
                                     dirty_mask, state->dirty_mask,
                                     state->valid_views);
      // Capture any reduction views
      FieldMask reduction_capture = capture_mask & state->reduction_mask;
      if (!!reduction_capture)
      {
        dirty_mask |= reduction_capture;
        for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it = 
              state->reduction_views.begin(); it != 
              state->reduction_views.end(); it++)
        {
          FieldMask overlap = capture_mask & it->second;
          if (!overlap)
            continue;
          LegionMap<ReductionView*,FieldMask>::aligned::iterator finder = 
            reduction_views.find(it->first);
          if (finder == reduction_views.end())
            reduction_views[it->first] = overlap;
          else
            finder->second |= overlap;
        }
      }
      // Record that we've captured the fields for this node
      update_capture_mask(node, capture_mask);
    }

    //--------------------------------------------------------------------------
    void CompositeCloser::update_capture_mask(RegionTreeNode *node,
                                              const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      LegionMap<RegionTreeNode*,FieldMask>::aligned::iterator finder = 
                                                  capture_fields.find(node);
      if (finder == capture_fields.end())
        capture_fields[node] = capture_mask;
      else
        finder->second |= capture_mask;
    }

    //--------------------------------------------------------------------------
    bool CompositeCloser::filter_capture_mask(RegionTreeNode *node,
                                              FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      LegionMap<RegionTreeNode*,FieldMask>::aligned::const_iterator finder = 
        capture_fields.find(node);
      if (finder != capture_fields.end() && !(capture_mask * finder->second))
      {
        capture_mask -= finder->second;
        return true;
      }
      return false;
    }

    /////////////////////////////////////////////////////////////
    // Physical State 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PhysicalState::PhysicalState(VersionManager *m)
      : manager(m)
#ifdef DEBUG_HIGH_LEVEL
        , node(NULL)
#endif
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(false); // shouldn't be calling this constructor in debug mode
#endif
    }

#ifdef DEBUG_HIGH_LEVEL
    //--------------------------------------------------------------------------
    PhysicalState::PhysicalState(VersionManager *m, RegionTreeNode *n)
      : manager(m), node(n)
    //--------------------------------------------------------------------------
    {
    }
#endif

    //--------------------------------------------------------------------------
    PhysicalState::PhysicalState(const PhysicalState &rhs)
      : manager(NULL)
#ifdef DEBUG_HIGH_LEVEL
        , node(NULL)
#endif
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PhysicalState::~PhysicalState(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(created_instances.empty());
#endif
      // Remove references to our version states and delete them if necessary
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator vit =
            version_states.begin(); vit != version_states.end(); vit++)
      {
        const VersionStateInfo &info = vit->second;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          if (it->first->remove_base_gc_ref(PHYSICAL_STATE_REF)) 
            legion_delete(it->first);
        }
      }
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator vit =
            advance_states.begin(); vit != advance_states.end(); vit++)
      {
        const VersionStateInfo &info = vit->second;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          if (it->first->remove_base_gc_ref(PHYSICAL_STATE_REF))
            legion_delete(it->first);
        }
      }
      version_states.clear();
      advance_states.clear();
    }

    //--------------------------------------------------------------------------
    PhysicalState& PhysicalState::operator=(const PhysicalState &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }
    
    //--------------------------------------------------------------------------
    void* PhysicalState::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<PhysicalState,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void* PhysicalState::operator new[](size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<PhysicalState,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void PhysicalState::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void PhysicalState::operator delete[](void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void PhysicalState::add_version_state(VersionState *state, 
                                          const FieldMask &state_mask)
    //--------------------------------------------------------------------------
    {
      VersionStateInfo &info = version_states[state->version_number];
      LegionMap<VersionState*,FieldMask>::aligned::iterator finder = 
        info.states.find(state);
      if (finder == info.states.end())
      {
        state->add_base_gc_ref(PHYSICAL_STATE_REF);
        info.states[state] = state_mask;
      }
      else
        finder->second |= state_mask;
      info.valid_fields |= state_mask;
    }

    //--------------------------------------------------------------------------
    void PhysicalState::add_advance_state(VersionState *state, 
                                          const FieldMask &state_mask)
    //--------------------------------------------------------------------------
    {
      VersionStateInfo &info = advance_states[state->version_number];
      LegionMap<VersionState*,FieldMask>::aligned::iterator finder = 
        info.states.find(state);
      if (finder == info.states.end())
      {
        state->add_base_gc_ref(PHYSICAL_STATE_REF);
        info.states[state] = state_mask;
      }
      else
        finder->second |= state_mask;
      info.valid_fields |= state_mask;
    }

    //--------------------------------------------------------------------------
    void PhysicalState::capture_state(bool path_only, bool close_top,
                  const LegionMap<VersionID,FieldMask>::aligned &field_versions)
    //--------------------------------------------------------------------------
    {
      if (close_top)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(!path_only);
#endif
        // Capture everything but the open children below from the
        // normal version states, but get the open children from the
        // advance states since that's where the sub-operations have
        // been writing to.
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = version_states.begin(); vit != version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it =
                info.states.begin(); it != info.states.end(); it++)
          {
            it->first->update_close_top_state(this, it->second);
          }
        }
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator
              vit = advance_states.begin(); vit != advance_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it =
                info.states.begin(); it != info.states.end(); it++)
          {
            it->first->update_open_children_state(this, it->second);
          }
        }
      }
      else if (path_only)
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = version_states.begin(); vit != version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it =
                info.states.begin(); it != info.states.end(); it++)
          {
            it->first->update_path_only_state(this, it->second);
          }
        }
      }
      else
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = version_states.begin(); vit != version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it =
                info.states.begin(); it != info.states.end(); it++)
          {
            it->first->update_physical_state(this, it->second);
          }
        }
      }
      // Check to see if we have any persistent views to include
      if (manager->has_persistent_views())
      {
        // Build this up from the set of version numbers since
        // the version states might not exist if we've advanced
        // more than one version number ahead.
        FieldMask space_mask;
        for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
              field_versions.begin(); it != field_versions.end(); it++)
        {
          space_mask |= it->second;
        }
        manager->capture_persistent_views(this, space_mask);
      }
    }

    //--------------------------------------------------------------------------
    void PhysicalState::apply_premapping_state(bool advance) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(created_instances.empty());
#endif
      if (advance && !advance_states.empty())
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = advance_states.begin(); vit != 
              advance_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->merge_premap_state(this, it->second);
          }
        }
      }
      else
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = version_states.begin(); vit != 
              version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->merge_premap_state(this, it->second); 
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void PhysicalState::apply_state(bool advance)
    //--------------------------------------------------------------------------
    {
      if (advance && !advance_states.empty())
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = advance_states.begin(); vit != 
              advance_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->merge_physical_state(this, it->second);
          }
        }
      }
      else
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = version_states.begin(); vit != 
              version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->merge_physical_state(this, it->second); 
          }
        }
      }
      // If we have any created instances, we can now remove our
      // valid references on them because we've applied all our updates
      if (!created_instances.empty())
      {
        for (std::deque<InstanceView*>::const_iterator it = 
              created_instances.begin(); it != created_instances.end(); it++)
        {
          if ((*it)->remove_base_valid_ref(INITIAL_CREATION_REF))
            LogicalView::delete_logical_view(*it);
        }
        created_instances.clear();
      }
    }

    //--------------------------------------------------------------------------
    void PhysicalState::filter_and_apply(bool top, bool filter_children)
    //--------------------------------------------------------------------------
    {
      if (top)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(!advance_states.empty());
#endif
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = advance_states.begin(); vit != 
              advance_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->filter_and_merge_physical_state(this, it->second, 
                                                       true, filter_children);
          }
        }
      }
      else
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(advance_states.empty());
#endif
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
              vit = version_states.begin(); vit != 
              version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->filter_and_merge_physical_state(this, it->second, 
                                                       false, filter_children);
          }
        }
      }
      // If we have any created instances, we can now remove our
      // valid references on them because we've applied all our updates
      if (!created_instances.empty())
      {
        for (std::deque<InstanceView*>::const_iterator it = 
              created_instances.begin(); it != created_instances.end(); it++)
        {
          if ((*it)->remove_base_valid_ref(INITIAL_CREATION_REF))
            LogicalView::delete_logical_view(*it);
        }
        created_instances.clear();
      }
    }

    //--------------------------------------------------------------------------
    void PhysicalState::reset(void)
    //--------------------------------------------------------------------------
    {
      dirty_mask.clear();
      reduction_mask.clear();
      children.valid_fields.clear();
      children.open_children.clear();
      valid_views.clear();
      reduction_views.clear();
      // Don't clear version states or advance states, we need those
    }

    //--------------------------------------------------------------------------
    void PhysicalState::record_created_instance(InstanceView *view)
    //--------------------------------------------------------------------------
    {
      view->add_base_valid_ref(INITIAL_CREATION_REF); 
      created_instances.push_back(view);
    }

    //--------------------------------------------------------------------------
    PhysicalState* PhysicalState::clone(bool capture_state) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      PhysicalState *result = legion_new<PhysicalState>(manager, node);
#else
      PhysicalState *result = legion_new<PhysicalState>(manager);
#endif
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator it1 =
            version_states.begin(); it1 != version_states.end(); it1++)
      {
        const VersionStateInfo &info = it1->second;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          result->add_version_state(it->first, it->second);
        }
      }
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator it1 =
            advance_states.begin(); it1 != advance_states.end(); it1++)
      {
        const VersionStateInfo &info = it1->second;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          result->add_advance_state(it->first, it->second);
        }
      }
      if (capture_state)
      {
        // No need to copy over the close mask
        result->dirty_mask = dirty_mask;
        result->reduction_mask = reduction_mask;
        result->children = children;
        result->valid_views = valid_views;
        result->reduction_views = reduction_views;
      }
      return result;
    }

    //--------------------------------------------------------------------------
    PhysicalState* PhysicalState::clone(const FieldMask &clone_mask, 
                                        bool capture_state) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      PhysicalState *result = legion_new<PhysicalState>(manager, node);
#else
      PhysicalState *result = legion_new<PhysicalState>(manager);
#endif
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator it1 =
            version_states.begin(); it1 != version_states.end(); it1++)
      {
        const VersionStateInfo &info = it1->second;
        if (it1->second.valid_fields * clone_mask)
          continue;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          FieldMask overlap = it->second & clone_mask;
          if (!overlap)
            continue;
          result->add_version_state(it->first, it->second);
        }
      }
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator it1 =
            advance_states.begin(); it1 != advance_states.end(); it1++)
      {
        const VersionStateInfo &info = it1->second;
        if (it1->second.valid_fields * clone_mask)
          continue;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          FieldMask overlap = it->second & clone_mask;
          if (!overlap)
            continue;
          result->add_advance_state(it->first, it->second);
        }
      }
      if (capture_state)
      {
        // No need to copy over the close mask
        result->dirty_mask = dirty_mask & clone_mask;
        result->reduction_mask = reduction_mask & clone_mask;
        for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
              valid_views.begin(); it != valid_views.end(); it++)
        {
          FieldMask overlap = it->second & clone_mask;
          if (!overlap)
            continue;
          result->valid_views[it->first] = overlap;
        }
        if (!!result->reduction_mask)
        {
          for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it =
                reduction_views.begin(); it != reduction_views.end(); it++)
          {
            FieldMask overlap = it->second & clone_mask;
            if (!overlap)
              continue;
            result->reduction_views[it->first] = overlap;
          }
        }
        if (!(children.valid_fields * clone_mask))
        {
          for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
                children.open_children.begin(); it !=
                children.open_children.end(); it++)
          {
            FieldMask overlap = it->second & clone_mask;
            if (!overlap)
              continue;
            result->children.open_children[it->first] = overlap;
            result->children.valid_fields |= overlap;
          }
        }
      }
      return result;
    }

    //--------------------------------------------------------------------------
    void PhysicalState::make_local(std::set<Event> &preconditions, 
                                   bool needs_final)
    //--------------------------------------------------------------------------
    {
      if (needs_final)
      {
        // If we are either advancing or closing, then we need the final
        // version states for all the field versions
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
             vit = version_states.begin(); vit != version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->request_final_version_state(it->second, preconditions);
          }
        }
      }
      else
      {
        // Otherwise, we just need one instance of the initial version state
        for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator 
             vit = version_states.begin(); vit != version_states.end(); vit++)
        {
          const VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                it = info.states.begin(); it != info.states.end(); it++)
          {
            it->first->request_initial_version_state(it->second, preconditions);
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void PhysicalState::print_physical_state(const FieldMask &capture_mask,
                          LegionMap<ColorPoint,FieldMask>::aligned &to_traverse,
                                             TreeStateLogger *logger)
    //--------------------------------------------------------------------------
    {
      // Dirty Mask
      {
        FieldMask overlap = dirty_mask & capture_mask;
        char *dirty_buffer = overlap.to_string();
        logger->log("Dirty Mask: %s",dirty_buffer);
        free(dirty_buffer);
      }
      // Valid Views
      {
        unsigned num_valid = 0;
        for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
              valid_views.begin(); it != valid_views.end(); it++)
        {
          if (it->second * capture_mask)
            continue;
          num_valid++;
        }
        logger->log("Valid Instances (%d)", num_valid);
        logger->down();
        for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
              valid_views.begin(); it != valid_views.end(); it++)
        {
          FieldMask overlap = it->second & capture_mask;
          if (!overlap)
            continue;
          if (it->first->is_deferred_view())
            continue;
#ifdef DEBUG_HIGH_LEVEL
          assert(it->first->as_instance_view()->is_materialized_view());
#endif
          MaterializedView *current = 
            it->first->as_instance_view()->as_materialized_view();
          char *valid_mask = overlap.to_string();
          logger->log("Instance " IDFMT "   Memory " IDFMT "   Mask %s",
                      current->manager->get_instance().id, 
                      current->manager->memory.id, valid_mask);
          free(valid_mask);
        }
        logger->up();
      }
      // Valid Reduction Views
      {
        unsigned num_valid = 0;
        for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it =
              reduction_views.begin(); it != 
              reduction_views.end(); it++)
        {
          if (it->second * capture_mask)
            continue;
          num_valid++;
        }
        logger->log("Valid Reduction Instances (%d)", num_valid);
        logger->down();
        for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it = 
              reduction_views.begin(); it != 
              reduction_views.end(); it++)
        {
          FieldMask overlap = it->second & capture_mask;
          if (!overlap)
            continue;
          char *valid_mask = overlap.to_string();
          logger->log("Reduction Instance " IDFMT "   Memory " IDFMT 
                      "  Mask %s",
                      it->first->manager->get_instance().id, 
                      it->first->manager->memory.id, valid_mask);
          free(valid_mask);
        }
        logger->up();
      }
      // Open Children
      {
        logger->log("Open Children (%ld)", 
            children.open_children.size());
        logger->down();
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator 
              it = children.open_children.begin(); it !=
              children.open_children.end(); it++)
        {
          FieldMask overlap = it->second & capture_mask;
          if (!overlap)
            continue;
          char *mask_buffer = overlap.to_string();
          switch (it->first.get_dim())
          {
            case 0:
              {
                logger->log("Color %d   Mask %s", 
                            it->first.get_index(), mask_buffer);
                break;
              }
            case 1:
              {

                logger->log("Color %d   Mask %s", 
                            it->first[0], mask_buffer);
                break;
              }
            case 2:
              {
                logger->log("Color (%d,%d)   Mask %s", 
                            it->first[0],
                            it->first[1], mask_buffer);
                break;
              }
            case 3:
              {
                logger->log("Color (%d,%d,%d)   Mask %s", 
                            it->first[0], it->first[1],
                            it->first[2], mask_buffer);
                break;
              }
            default:
              assert(false);
          }
          free(mask_buffer);
          // Mark that we should traverse this child
          to_traverse[it->first] = overlap;
        }
        logger->up();
      }
    }

    /////////////////////////////////////////////////////////////
    // Version State 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    VersionState::VersionState(VersionID vid, Runtime *rt, DistributedID id,
                               AddressSpaceID own_sp, AddressSpaceID local_sp, 
                               VersionManager *man, const FieldMask &mask,
                               bool initialize)
      : DistributedCollectable(rt, id, own_sp, local_sp), version_number(vid), 
        manager(man), state_lock(Reservation::create_reservation())
#ifdef DEBUG_HIGH_LEVEL
        , currently_active(true), currently_valid(true)
#endif
    //--------------------------------------------------------------------------
    {
      // Initialize any fields that we now have
      if (initialize)
        initial_fields |= mask;
      // If we are not the owner, add a valid and resource reference
      if (!is_owner())
      {
        add_base_valid_ref(REMOTE_DID_REF);
        add_base_resource_ref(REMOTE_DID_REF);
        // If we are remote and we are now initialized send our notification
        if (initialize)
        {
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(mask);
          }
          runtime->send_version_state_initialization(owner_space, rez);
        }
      }
      // If we're the owner and we are in eventual state, add ourselves
      else if (initialize)
        initial_nodes[local_space] = mask;
    }

    //--------------------------------------------------------------------------
    VersionState::VersionState(const VersionState &rhs)
      : DistributedCollectable(rhs), version_number(rhs.version_number),
        manager(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    VersionState::~VersionState(void)
    //--------------------------------------------------------------------------
    {
      state_lock.destroy_reservation();
      state_lock = Reservation::NO_RESERVATION;
#ifdef DEBUG_HIGH_LEVEL
      assert(!currently_valid);
#endif 
      // If we are the owner, then remote resource 
      // references on our remote instances 
      if (is_owner())
      {
        // If we're the owner, remove our valid references on remote nodes
        UpdateReferenceFunctor<RESOURCE_REF_KIND,false/*add*/> functor(this); 
        map_over_remote_instances(functor);
      }
    }

    //--------------------------------------------------------------------------
    VersionState& VersionState::operator=(const VersionState &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* VersionState::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<VersionState,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void* VersionState::operator new[](size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<VersionState,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void VersionState::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void VersionState::operator delete[](void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void VersionState::initialize(LogicalView *new_view, Event term_event,
                                  const RegionUsage &usage,
                                  const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
      assert(new_view->is_instance_view());
#endif
      new_view->add_nested_gc_ref(did);
      new_view->add_nested_valid_ref(did);
      InstanceView *inst_view = new_view->as_instance_view();
      if (inst_view->is_reduction_view())
      {
        ReductionView *view = inst_view->as_reduction_view();
        LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
          track_aligned::iterator finder = reduction_views.find(view); 
        if (finder == reduction_views.end())
          reduction_views[view] = user_mask;
        else
          finder->second |= user_mask;
        reduction_mask |= user_mask;
        inst_view->add_initial_user(term_event, usage, user_mask);
      }
      else
      {
        LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
          track_aligned::iterator finder = valid_views.find(new_view);
        if (finder == valid_views.end())
          valid_views[new_view] = user_mask;
        else
          finder->second |= user_mask;
        if (HAS_WRITE(usage))
          dirty_mask |= user_mask;
        inst_view->add_initial_user(term_event, usage, user_mask);
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::update_close_top_state(PhysicalState *state,
                                             const FieldMask &update_mask) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
#endif
      // We're reading so we only the need the lock in read-only mode
      AutoLock s_lock(state_lock,1,false/*exclusive*/);
      if (!!dirty_mask)
        state->dirty_mask |= (dirty_mask & update_mask);
      FieldMask reduction_update = reduction_mask & update_mask;
      if (!!reduction_update)
        state->reduction_mask |= reduction_update;
      for (LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
            track_aligned::const_iterator it = valid_views.begin();
            it != valid_views.end(); it++)
      {
        FieldMask overlap = it->second & update_mask;
        if (!overlap && !it->first->has_space(update_mask))
          continue;
        LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::track_aligned::
          iterator finder = state->valid_views.find(it->first);
        if (finder == state->valid_views.end())
          state->valid_views[it->first] = overlap;
        else
          finder->second |= overlap;
      }
      if (!!reduction_update)
      {
        for (LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
              track_aligned::const_iterator it = reduction_views.begin();
              it != reduction_views.end(); it++)
        {
          FieldMask overlap = it->second & update_mask; 
          if (!overlap)
            continue;
          LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
            track_aligned::iterator finder = 
              state->reduction_views.find(it->first);
          if (finder == state->reduction_views.end())
            state->reduction_views[it->first] = overlap;
          else
            finder->second |= overlap;
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::update_open_children_state(PhysicalState *state,
                                             const FieldMask &update_mask) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
#endif
      // We're reading so we only the need the lock in read-only mode
      AutoLock s_lock(state_lock,1,false/*exclusive*/);
      if (!(update_mask * children.valid_fields))
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
              children.open_children.begin(); it != 
              children.open_children.end(); it++)
        {
          FieldMask overlap = update_mask & it->second;
          if (!overlap)
            continue;
          LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
            state->children.open_children.find(it->first);
          if (finder == state->children.open_children.end())
            state->children.open_children[it->first] = overlap;
          else
            finder->second |= overlap;
          state->children.valid_fields |= overlap;
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::update_path_only_state(PhysicalState *state,
                                             const FieldMask &update_mask) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
#endif
      // We're reading so we only the need the lock in read-only mode
      AutoLock s_lock(state_lock,1,false/*exclusive*/);
      // If we are premapping, we only need to update the dirty bits
      // and the valid instance views 
      if (!!dirty_mask)
        state->dirty_mask |= (dirty_mask & update_mask);
      for (LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
            track_aligned::const_iterator it = valid_views.begin();
            it != valid_views.end(); it++)
      {
        FieldMask overlap = it->second & update_mask;
        if (!overlap && !it->first->has_space(update_mask))
          continue;
        LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::track_aligned::
          iterator finder = state->valid_views.find(it->first);
        if (finder == state->valid_views.end())
          state->valid_views[it->first] = overlap;
        else
          finder->second |= overlap;
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::update_physical_state(PhysicalState *state,
                                             const FieldMask &update_mask) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
#endif
      // We're reading so we only the need the lock in read-only mode
      AutoLock s_lock(state_lock,1,false/*exclusive*/);
      if (!!dirty_mask)
        state->dirty_mask |= (dirty_mask & update_mask);
      FieldMask reduction_update = reduction_mask & update_mask;
      if (!!reduction_update)
        state->reduction_mask |= reduction_update;
      FieldMask child_overlap = children.valid_fields & update_mask;
      if (!!child_overlap)
      {
        state->children.valid_fields |= child_overlap;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              children.open_children.begin(); it != 
              children.open_children.end(); it++)
        {
          FieldMask overlap = it->second & update_mask;
          if (!overlap)
            continue;
          LegionMap<ColorPoint,FieldMask>::aligned::iterator finder =
            state->children.open_children.find(it->first);
          if (finder == state->children.open_children.end())
            state->children.open_children[it->first] = overlap;
          else
            finder->second |= overlap;
        }
      }
      for (LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
            track_aligned::const_iterator it = valid_views.begin();
            it != valid_views.end(); it++)
      {
        FieldMask overlap = it->second & update_mask;
        if (!overlap && !it->first->has_space(update_mask))
          continue;
        LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::track_aligned::
          iterator finder = state->valid_views.find(it->first);
        if (finder == state->valid_views.end())
          state->valid_views[it->first] = overlap;
        else
          finder->second |= overlap;
      }
      if (!!reduction_update)
      {
        for (LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
              track_aligned::const_iterator it = reduction_views.begin();
              it != reduction_views.end(); it++)
        {
          FieldMask overlap = it->second & update_mask; 
          if (!overlap)
            continue;
          LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
            track_aligned::iterator finder = 
              state->reduction_views.find(it->first);
          if (finder == state->reduction_views.end())
            state->reduction_views[it->first] = overlap;
          else
            finder->second |= overlap;
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::merge_premap_state(const PhysicalState *state,
                                          const FieldMask &merge_mask)
    //--------------------------------------------------------------------------
    {
      // We're writing so we need the lock in exclusive mode
      AutoLock s_lock(state_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
#endif
      // For premapping, all we need to merge is the open children
      FieldMask child_overlap = state->children.valid_fields & merge_mask;
      if (!!child_overlap)
      {
        children.valid_fields |= child_overlap;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
              state->children.open_children.begin(); it !=
              state->children.open_children.end(); it++)
        {
          FieldMask overlap = it->second & merge_mask;
          if (!overlap)
            continue;
          LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
            children.open_children.find(it->first);
          if (finder == children.open_children.end())
            children.open_children[it->first] = overlap;
          else
            finder->second |= overlap;
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::merge_physical_state(const PhysicalState *state,
                                            const FieldMask &merge_mask,
                                            bool need_lock /* = true*/)
    //--------------------------------------------------------------------------
    {
      if (need_lock)
      {
        // We're writing so we need the lock in exclusive mode
        Event acquire_event = state_lock.acquire();
        acquire_event.wait();
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
#endif
      if (!!state->dirty_mask)
        dirty_mask |= (state->dirty_mask & merge_mask);
      FieldMask reduction_merge = state->reduction_mask & merge_mask;
      if (!!reduction_merge)
        reduction_mask |= reduction_merge;
      FieldMask child_overlap = state->children.valid_fields & merge_mask;
      if (!!child_overlap)
      {
        children.valid_fields |= child_overlap;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
              state->children.open_children.begin(); it !=
              state->children.open_children.end(); it++)
        {
          FieldMask overlap = it->second & merge_mask;
          if (!overlap)
            continue;
          LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
            children.open_children.find(it->first);
          if (finder == children.open_children.end())
            children.open_children[it->first] = overlap;
          else
            finder->second |= overlap;
        }
      }
      for (LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
            track_aligned::const_iterator it = state->valid_views.begin();
            it != state->valid_views.end(); it++)
      {
        FieldMask overlap = it->second & merge_mask;
        if (!overlap)
          continue;
        LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
          track_aligned::iterator finder = valid_views.find(it->first);
        if (finder == valid_views.end())
        {
          it->first->add_nested_gc_ref(did);
          it->first->add_nested_valid_ref(did);
          valid_views[it->first] = overlap;
        }
        else
          finder->second |= overlap;
      }
      if (!!reduction_merge)
      {
        for (LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
              track_aligned::const_iterator it = state->reduction_views.begin();
              it != state->reduction_views.end(); it++)
        {
          FieldMask overlap = it->second & merge_mask;
          if (!overlap)
            continue;
          LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
            track_aligned::iterator finder = reduction_views.find(it->first);
          if (finder == reduction_views.end())
          {
            it->first->add_nested_gc_ref(did);
            it->first->add_nested_valid_ref(did);
            reduction_views[it->first] = overlap;
          }
          else
            finder->second |= overlap;
        }
      }
      // Finally update our state if we are the owner
      if (is_owner())
      {
        initial_fields |= merge_mask;
        initial_nodes[local_space] |= merge_mask;
      }
      if (need_lock)
        state_lock.release();
    }

    //--------------------------------------------------------------------------
    void VersionState::filter_and_merge_physical_state(
                        const PhysicalState *state, const FieldMask &merge_mask,
                        bool top, bool filter_children)
    //--------------------------------------------------------------------------
    {
      // We're writing so we need the lock in exclusive mode
      AutoLock s_lock(state_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
#endif
      // Do the filtering first
      if (!top)
      {
        dirty_mask -= merge_mask;
        reduction_mask -= merge_mask;
        // When filtering these views make sure that they aren't also
        // still in the state before removing references which is necessary
        // for the correctness of the garbage collection scheme
        if (!valid_views.empty())
        {
          std::vector<LogicalView*> to_delete;
          for (LegionMap<LogicalView*,FieldMask>::aligned::iterator it =
                valid_views.begin(); it != valid_views.end(); it++)
          {
            it->second -= merge_mask;
            if (!it->second && (state->valid_views.find(it->first) == 
                                state->valid_views.end()))
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<LogicalView*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              valid_views.erase(*it);
              (*it)->remove_nested_valid_ref(did);
              if ((*it)->remove_nested_gc_ref(did))
                legion_delete(*it);
            }
          }
        }
        if (!reduction_views.empty())
        {
          std::vector<ReductionView*> to_delete;
          for (LegionMap<ReductionView*,FieldMask>::aligned::iterator it = 
                reduction_views.begin(); it != reduction_views.end(); it++)
          {
            it->second -= merge_mask;
            if (!it->second && (state->reduction_views.find(it->first) ==
                                state->reduction_views.end()))
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<ReductionView*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              reduction_views.erase(*it);
              (*it)->remove_nested_valid_ref(did);
              if ((*it)->remove_nested_gc_ref(did))
                legion_delete(*it);
            }
          }
        }
      }
      if (filter_children)
      {
        children.valid_fields -= merge_mask;  
        if (!children.valid_fields)
          children.open_children.clear();
        else
        {
          std::vector<ColorPoint> to_delete;
          for (LegionMap<ColorPoint,FieldMask>::aligned::iterator it = 
                children.open_children.begin(); it != 
                children.open_children.end(); it++)
          {
            it->second -= merge_mask;
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<ColorPoint>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              children.open_children.erase(*it);
            }
          }
        }
      }
      // Now we can do the merge
      merge_physical_state(state, merge_mask, false/*need lock*/);
    }

    //--------------------------------------------------------------------------
    void VersionState::notify_active(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_active); // should be monotonic
#endif
    }

    //--------------------------------------------------------------------------
    void VersionState::notify_inactive(void)
    //--------------------------------------------------------------------------
    {
      // Do nothing we only care about valid references
      AutoLock s_lock(state_lock,1,false/*exclusive*/);
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_active);
      currently_active = false;
#endif
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.begin(); it != valid_views.end(); it++)
      {
        it->first->remove_nested_gc_ref(did);
      }
      for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it = 
            reduction_views.begin(); it != reduction_views.end(); it++)
      {
        it->first->remove_nested_gc_ref(did);
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::notify_valid(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid); // should be monotonic
#endif
    }

    //--------------------------------------------------------------------------
    void VersionState::notify_invalid(void)
    //--------------------------------------------------------------------------
    {
      AutoLock s_lock(state_lock,1,false/*exclusive*/);
#ifdef DEBUG_HIGH_LEVEL
      assert(currently_valid);
      currently_valid = false;
#endif
      // When we are no longer valid, remove all references to instance views
      // No need to hold the lock since no one else should be accessing us
      if (is_owner())
      {
        // If we're the owner, remove our valid references on remote nodes
        UpdateReferenceFunctor<VALID_REF_KIND,false/*add*/> functor(this); 
        map_over_remote_instances(functor);
      }
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.begin(); it != valid_views.end(); it++)
      {
        it->first->remove_nested_valid_ref(did);
      }
      for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it = 
            reduction_views.begin(); it != reduction_views.end(); it++)
      {
        it->first->remove_nested_valid_ref(did);
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::record_initial_fields(const FieldMask &initial_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!is_owner());
#endif
      {
        AutoLock s_lock(state_lock);
        initial_fields |= initial_mask;
      }
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(initial_mask);
      }
      runtime->send_version_state_initialization(owner_space, rez);
    }

    //--------------------------------------------------------------------------
    void VersionState::request_initial_version_state(
                  const FieldMask &request_mask, std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      UserEvent ready_event = UserEvent::NO_USER_EVENT;
      FieldMask remaining_mask = request_mask;
      LegionDeque<RequestInfo>::aligned targets;
      {
        AutoLock s_lock(state_lock);
        // Check to see which fields we already have initial events for
        if (!initial_events.empty() && !(request_mask * initial_fields))
        {
          for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                initial_events.begin(); it != initial_events.end(); it++)
          {
            if (remaining_mask * it->second)
              continue;
            preconditions.insert(it->first);
          }
          remaining_mask -= initial_fields;
          if (!remaining_mask)
            return;
        }
        // Also check to see which fields we already have final events for
        if (!final_events.empty() && !(request_mask * final_fields))
        {
          for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                final_events.begin(); it != final_events.end(); it++)
          {
            if (remaining_mask * it->second)
              continue;
            preconditions.insert(it->first);
          }
          remaining_mask -= final_fields;
          if (!remaining_mask)
            return;
        }
        // If we are the owner and there are no existing versions, we can
        // immediately make ourselves a local_version
        if (is_owner())
        {
          if (initial_nodes.empty())
          {
            initial_fields |= remaining_mask;
            initial_nodes[local_space] = remaining_mask;
          }
          else
          {
            select_initial_targets(local_space, remaining_mask, 
                                   targets, preconditions);
            // At this point we can now upgrade our initial fields
            // and our node information
            initial_fields |= request_mask;
            initial_nodes[local_space] |= request_mask;
          }
        }
        else
        {
          // Make a user event and record it as a precondition
          ready_event = UserEvent::create_user_event();
          initial_events[ready_event] = remaining_mask;
          initial_fields |= remaining_mask;
          preconditions.insert(ready_event);
        }
      }
      if (!is_owner())
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(!!remaining_mask);
#endif
        // If we make it here, then send a request to the owner
        // for the remaining fields
        send_version_state_request(owner_space, local_space, ready_event, 
                                   remaining_mask, false/*final*/);
      }
      else if (!targets.empty())
      {
        // otherwise we're the owner, send out requests to all 
        for (LegionDeque<RequestInfo>::aligned::const_iterator 
              it = targets.begin(); it != targets.end(); it++)
        {
          send_version_state_request(it->target, local_space, it->to_trigger,
                                     it->request_mask, it->request_final);
                                
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::request_final_version_state(const FieldMask &req_mask,
                                                 std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      UserEvent ready_event;
      FieldMask remaining_mask = req_mask;
      LegionDeque<RequestInfo>::aligned targets;
      {
        AutoLock s_lock(state_lock);
        if (!final_events.empty() && !(req_mask * final_fields))
        {
          for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                final_events.begin(); it != final_events.end(); it++)
          {
            if (remaining_mask * it->second)
              continue;
            preconditions.insert(it->first);
          }
          remaining_mask -= final_fields;
          if (!remaining_mask)
            return;
        }
        if (is_owner())
        {
          select_final_targets(local_space, remaining_mask,
                               targets, preconditions);
          // We can now update our final fields and local information
          final_fields |= remaining_mask;
          final_nodes[local_space] |= remaining_mask;
        }
        else
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(!!remaining_mask);
#endif
          // Make a user event and record it as a precondition   
          ready_event = UserEvent::create_user_event();
          final_fields |= remaining_mask;
          final_events[ready_event] = remaining_mask;
          preconditions.insert(ready_event);
        }
      }
      if (!is_owner())
      {
        send_version_state_request(owner_space, local_space, ready_event,
                                   remaining_mask, true/*final*/); 
      }
      else if (!targets.empty())
      {
        for (LegionDeque<RequestInfo>::aligned::const_iterator 
              it = targets.begin(); it != targets.end(); it++)
        {
          send_version_state_request(it->target, local_space, it->to_trigger,
                                     it->request_mask, it->request_final);
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::select_initial_targets(AddressSpaceID request_space,
                                              FieldMask &needed_mask,
                                     LegionDeque<RequestInfo>::aligned &targets,
                                              std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      // Better be called while holding the lock
#ifdef DEBUG_HIGH_LEVEL
      assert(is_owner()); // should only be called on the owner node
#endif
      // Iterate over the initial nodes and issue requests to the first
      // node we happen upon who has the valid data
      for (LegionMap<AddressSpaceID,FieldMask>::aligned::const_iterator it = 
            initial_nodes.begin(); it != initial_nodes.end(); it++)
      {
        // Skip the requesting space
        if (request_space == it->first)
          continue;
        FieldMask overlap = needed_mask & it->second;
        if (!overlap)
          continue;
        targets.push_back(RequestInfo());
        RequestInfo &info = targets.back();
        info.target = it->first;
        info.to_trigger = UserEvent::create_user_event();
        info.request_mask = overlap;
        info.request_final = false;
        // Add the event to the set of preconditions
        preconditions.insert(info.to_trigger);
        // If we are the requester, then update our initial events
        if (request_space == local_space)
          initial_events[info.to_trigger] = overlap;
        needed_mask -= overlap;
        if (!needed_mask)
          break;
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::select_final_targets(AddressSpaceID request_space,
                                            FieldMask &needed_mask,
                                    LegionDeque<RequestInfo>::aligned &targets,
                                            std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      // Better be called while holding the lock
#ifdef DEBUG_HIGH_LEVEL
      assert(is_owner()); // should only be called on the owner node
#endif
      // First check to see if there are any final versions we can copy from
      for (LegionMap<AddressSpaceID,FieldMask>::aligned::const_iterator it = 
            final_nodes.begin(); it != final_nodes.end(); it++)
      {
        // Skip the requesting state
        if (request_space == it->first)
          continue;
        FieldMask overlap = needed_mask & it->second;
        if (!overlap)
          continue;
        targets.push_back(RequestInfo());
        RequestInfo &info = targets.back();
        info.target = it->first;
        info.to_trigger = UserEvent::create_user_event();
        info.request_mask = overlap;
        info.request_final = true;
        // Add the event to the set of preconditions
        preconditions.insert(info.to_trigger);
        // If we are the requester, then update our final events
        if (request_space == local_space)
          final_events[info.to_trigger] = overlap;
        needed_mask -= overlap;
        if (!needed_mask)
          return;
      }
      // Now if we still have needed fields, we need to create a final
      // version from all of the earlier versions across all the nodes
      FieldMask requested_mask;
      std::set<Event> merge_preconditions;
      for (LegionMap<AddressSpaceID,FieldMask>::aligned::const_iterator it = 
            initial_nodes.begin(); it != initial_nodes.end(); it++)
      {
        if (request_space == it->first)
          continue;
        FieldMask overlap = needed_mask & it->second;
        if (!overlap)
          continue;
        RequestInfo &info = targets.back();
        info.target = it->first;
        info.to_trigger = UserEvent::create_user_event();
        info.request_mask = overlap;
        info.request_final = false;
        merge_preconditions.insert(info.to_trigger); 
        requested_mask |= overlap;
      }
      if (!!requested_mask)
      {
        Event precondition = Event::merge_events(merge_preconditions);
        if (precondition.exists())
        {
          preconditions.insert(precondition);
          if (request_space == local_space)
            final_events[precondition] = requested_mask;
        }
        needed_mask -= requested_mask;
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::send_version_state(AddressSpaceID target,
                                          const FieldMask &request_mask,
                                          UserEvent to_trigger)
    //--------------------------------------------------------------------------
    {
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(to_trigger);
        // Hold the lock in read-only mode while iterating these structures
        AutoLock s_lock(state_lock,1,false/*exclusive*/);
        // See if we should send all the fields or just do a partial send
        if (!((initial_fields | final_fields) - request_mask))
        {
          // Send everything
          rez.serialize(dirty_mask);
          rez.serialize(reduction_mask);
          rez.serialize<size_t>(children.open_children.size()); 
          for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
                children.open_children.begin(); it != 
                children.open_children.end(); it++)
          {
            rez.serialize(it->first);
            rez.serialize(it->second);
          }
          rez.serialize<size_t>(valid_views.size());
          for (LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
                track_aligned::const_iterator it = valid_views.begin(); it !=
                valid_views.end(); it++)
          {
            DistributedID view_did = it->first->send_view(target, it->second);
            rez.serialize(view_did);
            rez.serialize(it->second);
          }
          rez.serialize<size_t>(reduction_views.size());
          for (LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
                track_aligned::const_iterator it = reduction_views.begin(); 
                it != reduction_views.end(); it++)
          {
            DistributedID reduc_did = it->first->send_view(target, it->second);
            rez.serialize(reduc_did);
            rez.serialize(it->second);
          }
        }
        else
        {
          // Partial send
          rez.serialize(dirty_mask & request_mask);
          rez.serialize(reduction_mask & request_mask);
          if (!children.open_children.empty())
          {
            Serializer child_rez;
            size_t count = 0;
            for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
                  children.open_children.begin(); it !=
                  children.open_children.end(); it++)
            {
              FieldMask overlap = it->second & request_mask;
              if (!overlap)
                continue;
              child_rez.serialize(it->first);
              child_rez.serialize(overlap);
              count++;
            }
            rez.serialize(count);
            rez.serialize(child_rez.get_buffer(), child_rez.get_used_bytes());
          }
          else
            rez.serialize<size_t>(0);
          if (!valid_views.empty())
          {
            Serializer valid_rez;
            size_t count = 0;
            for (LegionMap<LogicalView*,FieldMask,VALID_VIEW_ALLOC>::
                  track_aligned::const_iterator it = valid_views.begin(); it !=
                  valid_views.end(); it++)
            {
              FieldMask overlap = it->second & request_mask;
              if (!overlap)
                continue;
              DistributedID view_did = it->first->send_view(target, overlap);
              valid_rez.serialize(view_did);
              valid_rez.serialize(overlap);
              count++;
            }
            rez.serialize(count);
            rez.serialize(valid_rez.get_buffer(), valid_rez.get_used_bytes());
          }
          else
            rez.serialize<size_t>(0);
          if (!reduction_views.empty())
          {
            Serializer reduc_rez;
            size_t count = 0;
            for (LegionMap<ReductionView*,FieldMask,VALID_REDUCTION_ALLOC>::
                  track_aligned::const_iterator it = reduction_views.begin();
                  it != reduction_views.end(); it++)
            {
              FieldMask overlap = it->second & request_mask;
              if (!overlap)
                continue;
              DistributedID reduc_did = it->first->send_view(target, overlap);
              reduc_rez.serialize(reduc_did);
              reduc_rez.serialize(overlap);
              count++;
            }
            rez.serialize(count);
            rez.serialize(reduc_rez.get_buffer(), reduc_rez.get_used_bytes());
          }
          else
            rez.serialize<size_t>(0);
        }
      }
      runtime->send_version_state_response(target, rez);
      // The owner will get updated automatically so only update remotely
      if (!is_owner())
        update_remote_instances(target);
    }

    //--------------------------------------------------------------------------
    void VersionState::send_version_state_request(AddressSpaceID target,
                                    AddressSpaceID source, UserEvent to_trigger, 
                                    const FieldMask &request_mask, 
                                    bool request_final)
    //--------------------------------------------------------------------------
    {
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(source);
        rez.serialize(to_trigger);
        rez.serialize(request_final);
        rez.serialize(request_mask);
      }
      runtime->send_version_state_request(target, rez);
    }

    //--------------------------------------------------------------------------
    void VersionState::launch_send_version_state(AddressSpaceID target,
        UserEvent to_trigger, const FieldMask &request_mask, Event precondition)
    //--------------------------------------------------------------------------
    {
      SendVersionStateArgs args;
      args.hlr_id = HLR_SEND_VERSION_STATE_TASK_ID;
      args.proxy_this = this;
      args.target = target;
      args.request_mask = new FieldMask(request_mask);
      args.to_trigger = to_trigger;
      runtime->issue_runtime_meta_task(&args, sizeof(args),
                                       HLR_SEND_VERSION_STATE_TASK_ID, 
                                       NULL/*op*/, precondition);
    }

    //--------------------------------------------------------------------------
    void VersionState::handle_version_state_initialization(
                                 AddressSpaceID source, FieldMask &initial_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(is_owner());
#endif
      // First transform the initial mask
      manager->owner->column_source->transform_field_mask(initial_mask, source);
      AutoLock s_lock(state_lock);
      initial_nodes[source] |= initial_mask;
    }

    //--------------------------------------------------------------------------
    void VersionState::handle_version_state_request(AddressSpaceID source,
              UserEvent to_trigger, bool request_final, FieldMask &request_mask)
    //--------------------------------------------------------------------------
    {
      if (!is_owner())
      {
        // First things first, transform the field mask
        manager->owner->column_source->transform_field_mask(request_mask, 
                                                            owner_space);
        // If we are not the owner, we should definitely be able to handle this 
        std::set<Event> launch_preconditions;
        FieldMask remaining_mask = request_mask;
        if (request_final)
        {
          AutoLock s_lock(state_lock,1,false/*exclusive*/);
          for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                final_events.begin(); it != final_events.end(); it++)
          {
            FieldMask overlap = it->second & request_mask;
            if (!overlap)
              continue;
            launch_preconditions.insert(it->first);
            remaining_mask -= overlap;
            if (!remaining_mask)
              break;
          }
        }
        else
        {
          AutoLock s_lock(state_lock,1,false/*exclusive*/);
          for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                initial_events.begin(); it != initial_events.end(); it++)
          {
            FieldMask overlap = it->second & request_mask;
            if (!overlap)
              continue;
            launch_preconditions.insert(it->first);
            remaining_mask -= overlap;
            if (!remaining_mask)
              break;
          }
        }
#ifdef DEBUG_HIGH_LEVEL
        assert(!remaining_mask); // request mask should now be empty
#endif
        if (!launch_preconditions.empty())
        {
          Event pre = Event::merge_events(launch_preconditions);
          launch_send_version_state(source, to_trigger, request_mask, pre);
        }
        else
          launch_send_version_state(source, to_trigger, request_mask);
      }
      else
      {
        // First things first, transform the field mask
        manager->owner->column_source->transform_field_mask(request_mask, 
                                                            source);
        // We're the owner, figure out what to do
        FieldMask remaining_fields = request_mask;
        FieldMask local_fields;
        int initial_local_index = -1;
        std::set<Event> local_preconditions, done_conditions;
        LegionDeque<RequestInfo>::aligned targets;
        if (request_final)
        {
          AutoLock s_lock(state_lock);
          // See if we can handle any of the fields locally
          local_fields = remaining_fields & final_fields;
          if (!!local_fields)
          {
            for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                  final_events.begin(); it != final_events.end(); it++)
            {
              if (it->second * local_fields)
                continue;
              local_preconditions.insert(it->first);
            }
            remaining_fields -= local_fields;
          }
          // See if there are any remote nodes that can handle it
          if (!!remaining_fields && !final_nodes.empty())
          {
            select_final_targets(source, remaining_fields,
                                 targets, done_conditions);
          }
          // Once we are here, we can update our remote state information
          final_nodes[source] |= request_mask;
        }
        else
        {
          AutoLock s_lock(state_lock);
          // See if we can handle any of the fields locally
          local_fields = remaining_fields & initial_fields;
          if (!!local_fields)
          {
            for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                  initial_events.begin(); it != initial_events.end(); it++)
            {
              if (it->second * local_fields)
                continue;
              local_preconditions.insert(it->first);
            }
            remaining_fields -= local_fields;
          }
          // Now see if there are any remote nodes that can handle it
          if (!!remaining_fields && !initial_nodes.empty())
          {
            select_initial_targets(source, remaining_fields,
                                   targets, done_conditions);
          }
          // Once we are here we can update our remote state information
          initial_nodes[source] |= request_mask;
        }
        // First, issue all our remote requests, pull out any that
        // were actually intended for us
        if (!targets.empty())
        {
          int idx = 0;
          for (LegionDeque<RequestInfo>::aligned::const_iterator it = 
                targets.begin(); it != targets.end(); it++, idx++)
          {
            if (it->target == local_space)
            {
              // Special case if we were supposed to send it
#ifdef DEBUG_HIGH_LEVEL
              assert(request_final && !it->request_final);
#endif
              initial_local_index = idx;
            }
            else
              send_version_state_request(it->target, source, it->to_trigger,
                                         it->request_mask, it->request_final);
          }
        }
        // Now see if we have any local fields to send
        if (!!local_fields)
        {
          UserEvent local_trigger = UserEvent::create_user_event();
          if (!local_preconditions.empty())
          {
            Event pre = Event::merge_events(local_preconditions);
            launch_send_version_state(source, to_trigger, local_fields, pre); 
          }
          else
            launch_send_version_state(source, to_trigger, local_fields); 
          done_conditions.insert(local_trigger);
        }
        // We might also have some additional local fields to send
        if (initial_local_index >= 0)
        {
          RequestInfo &info = targets[initial_local_index];
          local_preconditions.clear();
          // Retake the lock to read the local data structure
          {
            AutoLock s_lock(state_lock,1,false/*exclusive*/);
            for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                  initial_events.begin(); it != initial_events.end(); it++)
            {
              if (it->second * info.request_mask)
                continue;
              local_preconditions.insert(it->first);
            }
          }
          if (!local_preconditions.empty())
          {
            Event pre = Event::merge_events(local_preconditions);
            launch_send_version_state(source, info.to_trigger, 
                                      info.request_mask, pre);
          }
          else
            launch_send_version_state(source, info.to_trigger, 
                                      info.request_mask);
          done_conditions.insert(info.to_trigger);
        }
        // Now if we have any done conditions we trigger the proper 
        // precondition event, otherwise we can do it immediately
        if (!done_conditions.empty())
          to_trigger.trigger(Event::merge_events(done_conditions));
        else
          to_trigger.trigger();
      }
    }

    //--------------------------------------------------------------------------
    void VersionState::handle_version_state_response(AddressSpaceID source,
                                      UserEvent to_trigger, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      // Keep track of any composite veiws we need to check 
      // for having recursive version states at here
      std::vector<CompositeView*> composite_views;
      {
        RegionTreeNode *owner_node = manager->owner;
        FieldSpaceNode *field_node = owner_node->column_source;
        // Hold the lock when touching the data structures because we might
        // be getting multiple updates from different locations
        AutoLock s_lock(state_lock);
        // Check to see what state we are generating, if it is the initial
        // one then we can just do our unpack in-place, otherwise we have
        // to unpack separately and then do a merge.
        if (!initial_fields && !final_fields)
        {
          derez.deserialize(dirty_mask);
          field_node->transform_field_mask(dirty_mask, source); 
          derez.deserialize(reduction_mask);
          field_node->transform_field_mask(reduction_mask, source);
          size_t num_children;
          derez.deserialize(num_children);
          for (unsigned idx = 0; idx < num_children; idx++)
          {
            ColorPoint child;
            derez.deserialize(child);
            FieldMask &mask = children.open_children[child];
            derez.deserialize(mask);
            field_node->transform_field_mask(mask, source);
            children.valid_fields |= mask;
          }
          size_t num_valid_views;
          derez.deserialize(num_valid_views);
          for (unsigned idx = 0; idx < num_valid_views; idx++)
          {
            DistributedID did;
            derez.deserialize(did);
            LogicalView *view = owner_node->find_view(did);
            // Check for composite view
            if (view->is_deferred_view())
            {
              DeferredView *def_view = view->as_deferred_view();
              if (def_view->is_composite_view())
                composite_views.push_back(def_view->as_composite_view());
            }
            FieldMask &mask = valid_views[view];
            derez.deserialize(mask);
            field_node->transform_field_mask(mask, source);
            view->add_nested_gc_ref(did);
            view->add_nested_valid_ref(did);
          }
          size_t num_reduction_views;
          derez.deserialize(num_reduction_views);
          for (unsigned idx = 0; idx < num_reduction_views; idx++)
          {
            DistributedID did;
            derez.deserialize(did);
            LogicalView *view = owner_node->find_view(did);
#ifdef DEBUG_HIGH_LEVEL
            assert(view->is_instance_view());
            assert(view->as_instance_view()->is_reduction_view());
#endif
            ReductionView *red_view = 
              view->as_instance_view()->as_reduction_view();
            FieldMask &mask = reduction_views[red_view];
            derez.deserialize(mask);
            field_node->transform_field_mask(mask, source);
            view->add_nested_gc_ref(did);
            view->add_nested_valid_ref(did);
          }
        }
        else
        {
          {
            FieldMask dirty_update;
            derez.deserialize(dirty_update);
            field_node->transform_field_mask(dirty_update, source);
            dirty_mask |= dirty_update;
          }
          {
            FieldMask reduction_update;
            derez.deserialize(reduction_update);
            field_node->transform_field_mask(reduction_update, source);
            reduction_mask |= reduction_update;
          }
          size_t num_children;
          derez.deserialize(num_children);
          for (unsigned idx = 0; idx < num_children; idx++)
          {
            ColorPoint child;
            derez.deserialize(child);
            LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
              children.open_children.find(child);
            if (finder != children.open_children.end())
            {
              FieldMask child_update;
              derez.deserialize(child_update);
              field_node->transform_field_mask(child_update, source);
              finder->second |= child_update;
              children.valid_fields |= child_update;
            }
            else
            {
              FieldMask &mask = children.open_children[child];
              derez.deserialize(mask);
              field_node->transform_field_mask(mask, source);
              children.valid_fields |= mask;
            }
          }
          size_t num_valid_views;
          derez.deserialize(num_valid_views);
          for (unsigned idx = 0; idx < num_valid_views; idx++)
          {
            DistributedID did;
            derez.deserialize(did);
            LogicalView *view = owner_node->find_view(did);
            // Check for composite view
            if (view->is_deferred_view())
            {
              DeferredView *def_view = view->as_deferred_view();
              if (def_view->is_composite_view())
                composite_views.push_back(def_view->as_composite_view());
            }
            LegionMap<LogicalView*,FieldMask>::aligned::iterator finder = 
              valid_views.find(view);
            if (finder != valid_views.end())
            {
              FieldMask update_mask;
              derez.deserialize(update_mask);
              field_node->transform_field_mask(update_mask, source);
              finder->second |= update_mask;
            }
            else
            {
              FieldMask &mask = valid_views[view];
              derez.deserialize(mask);
              field_node->transform_field_mask(mask, source);
              view->add_nested_gc_ref(did);
              view->add_nested_valid_ref(did);
            }
          }
          size_t num_reduction_views;
          derez.deserialize(num_reduction_views);
          for (unsigned idx = 0; idx < num_reduction_views; idx++)
          {
            DistributedID did;
            derez.deserialize(did);
            LogicalView *view = owner_node->find_view(did);
#ifdef DEBUG_HIGH_LEVEL
            assert(view->is_instance_view());
            assert(view->as_instance_view()->is_reduction_view());
#endif
            ReductionView *red_view = 
              view->as_instance_view()->as_reduction_view();
            LegionMap<ReductionView*,FieldMask>::aligned::iterator finder = 
              reduction_views.find(red_view);
            if (finder != reduction_views.end())
            {
              FieldMask update_mask;
              derez.deserialize(update_mask);
              field_node->transform_field_mask(update_mask, source);
              finder->second |= update_mask;
            }
            else
            {
              FieldMask &mask = reduction_views[red_view];
              derez.deserialize(mask);
              field_node->transform_field_mask(mask, source);
              view->add_nested_gc_ref(did);
              view->add_nested_valid_ref(did);
            }
          }
        }
      }
      // If we have composite views, then we need to make sure
      // that their version states are local as well
      if (!composite_views.empty())
      {
        std::set<Event> preconditions;
        for (std::vector<CompositeView*>::const_iterator it = 
              composite_views.begin(); it != composite_views.end(); it++)
        {
          (*it)->make_local(preconditions); 
        }
        if (!preconditions.empty())
          to_trigger.trigger(Event::merge_events(preconditions));
        else
          to_trigger.trigger();
      }
      else
      {
        // Finally trigger the event saying we have the data
        to_trigger.trigger();
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void VersionState::process_version_state_initialization(
                        Runtime *rt, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      FieldMask initial_mask;
      derez.deserialize(initial_mask);
      DistributedCollectable *target = rt->find_distributed_collectable(did);
#ifdef DEBUG_HIGH_LEVEL
      VersionState *vs = dynamic_cast<VersionState*>(target);
      assert(vs != NULL);
#else
      VersionState *vs = static_cast<VersionState*>(target);
#endif
      vs->handle_version_state_initialization(source, initial_mask);
    }

    //--------------------------------------------------------------------------
    /*static*/ void VersionState::process_version_state_request(Runtime *rt,
                                                            Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      AddressSpaceID source;
      derez.deserialize(source);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      bool request_final;
      derez.deserialize(request_final);
      FieldMask request_mask;
      derez.deserialize(request_mask);
      DistributedCollectable *target = rt->find_distributed_collectable(did);
#ifdef DEBUG_HIGH_LEVEL
      VersionState *vs = dynamic_cast<VersionState*>(target);
      assert(vs != NULL);
#else
      VersionState *vs = static_cast<VersionState*>(target);
#endif
      vs->handle_version_state_request(source, to_trigger, 
                                       request_final, request_mask);
    }

    //--------------------------------------------------------------------------
    /*static*/ void VersionState::process_version_state_response(Runtime *rt,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);
      DistributedCollectable *target = rt->find_distributed_collectable(did);
#ifdef DEBUG_HIGH_LEVEL
      VersionState *vs = dynamic_cast<VersionState*>(target);
      assert(vs != NULL);
#else
      VersionState *vs = static_cast<VersionState*>(target);
#endif
      vs->handle_version_state_response(source, to_trigger, derez);
    }

    /////////////////////////////////////////////////////////////
    // Version Manager 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    VersionManager::VersionManager(RegionTreeNode *own)
      : owner(own), version_lock(Reservation::create_reservation()),
        has_persistent(false), persistent_lock(Reservation::NO_RESERVATION)
    //--------------------------------------------------------------------------
    {
    } 

    //--------------------------------------------------------------------------
    VersionManager::VersionManager(const VersionManager &rhs)
      : owner(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    VersionManager::~VersionManager(void)
    //--------------------------------------------------------------------------
    {
      version_lock.destroy_reservation();
      version_lock = Reservation::NO_RESERVATION;
      if (persistent_lock.exists())
      {
        persistent_lock.destroy_reservation();
        persistent_lock = Reservation::NO_RESERVATION;
      }
    }

    //--------------------------------------------------------------------------
    VersionManager& VersionManager::operator=(const VersionManager &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }
    
    //--------------------------------------------------------------------------
    PhysicalState* VersionManager::construct_state(RegionTreeNode *node,
                        const LegionMap<VersionID,FieldMask>::aligned &versions,
                        bool path_only, bool close_top, bool advance, 
                        bool initialize, bool capture)
    //--------------------------------------------------------------------------
    {
      // Create the result
#ifdef DEBUG_HIGH_LEVEL
      PhysicalState *state = legion_new<PhysicalState>(this, node);
#else
      PhysicalState *state = legion_new<PhysicalState>(this);
#endif 
      // There is an important tradeoff that we're making here: we should
      // be able to predict from our versioning number scheme in the logical
      // analysis exactly which version IDs need to be filtered.  However,
      // this can be tricky, and on remote nodes, the version states do
      // not get updated eagerly. Therefore, we instead filter on fields with
      // an understanding that in the worst case we may need to iterate over
      // O(F) fields where F is the number of fields in a field mask. If
      // this proves to be a problem we can revisit it.
      {
        // Take the lock in exclusive mode since we are potentially
        // filtering old version states and/or making new version states
        AutoLock v_lock(version_lock);
#ifdef DEBUG_HIGH_LEVEL
        sanity_check();
#endif
        if (advance)
        {
          // If we're advancing we need to filter everything less than
          // VID from previous states and VID+1 from current states
          for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
                versions.begin(); it != versions.end(); it++)
          {
            if (it->first > 0)
              filter_previous_states(it->first-1, it->second);
            filter_current_states(it->first, it->second);
            capture_previous_states(it->first, it->second, state);
            FieldMask to_create = it->second;
            capture_current_states(it->first+1, it->second, state, 
                                    to_create, true/*advance*/);
            if (!!to_create)
            {
#ifdef DEBUG_HIGH_LEVEL
              FieldMask &observed_mask = observed[it->first+1];
              assert(observed_mask * to_create);
              observed_mask |= to_create;
#endif
              VersionState *new_state = 
                create_new_version_state(it->first+1, to_create, initialize);
              new_state->add_base_valid_ref(VERSION_MANAGER_REF);
              VersionStateInfo &info = current_version_infos[it->first+1];
              info.states[new_state] = to_create;
              info.valid_fields |= to_create;
              state->add_advance_state(new_state, to_create);
            }
          }
        }
        else
        {
          // If we're not advancing then we need to filter everything
          // less than VID-1 from both previous and current states
          for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
                versions.begin(); it != versions.end(); it++)
          {
            if (it->first > 1)
              filter_previous_states(it->first-2, it->second);
            if (it->first > 0)
              filter_current_states(it->first-1, it->second);
            FieldMask to_create = it->second;
            capture_previous_states(it->first, it->second, state, to_create);
            if (!!to_create)
              capture_current_states(it->first, it->second, state, 
                                     to_create, false/*advance*/);
            if (!!to_create)
            {
#ifdef DEBUG_HIGH_LEVEL
              FieldMask &observed_mask = observed[it->first];
              assert(observed_mask * to_create);
              observed_mask |= to_create;
#endif
              VersionState *new_state = 
                create_new_version_state(it->first, to_create, initialize);
              new_state->add_base_valid_ref(VERSION_MANAGER_REF);
              VersionStateInfo &info = current_version_infos[it->first];
              info.states[new_state] = to_create;
              info.valid_fields |= to_create;
              state->add_version_state(new_state, to_create);
            }
          }
        }
#ifdef DEBUG_HIGH_LEVEL
        sanity_check();
#endif
      }
      // Now that we've got the set of states, tell the physical state
      // to capture from them
      if (capture)
        state->capture_state(path_only, close_top, versions);
      return state;
    } 

    //--------------------------------------------------------------------------
    void VersionManager::filter_previous_states(VersionID vid,
                                                const FieldMask &filter_mask)
    //--------------------------------------------------------------------------
    {
      std::vector<VersionID> empty_versions;
      for (LegionMap<VersionID,VersionStateInfo>::aligned::iterator vit = 
            previous_version_infos.begin(); vit != 
            previous_version_infos.end(); vit++)
      {
        // If we've exceeded the target version we are done
        if (vit->first > vid)
          break;
        VersionStateInfo &info = vit->second;
        FieldMask overlap = info.valid_fields & filter_mask;
        // No overlapping fields means there is nothing to do
        if (!overlap)
          continue;
        info.valid_fields -= overlap;
        if (!info.valid_fields)
        {
          empty_versions.push_back(vit->first);
          // Only need to remove references
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it =
                info.states.begin(); it != info.states.end(); it++)
          {
            if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
              legion_delete(it->first);
          }
        }
        else
        {
          std::vector<VersionState*> to_delete;
          for (LegionMap<VersionState*,FieldMask>::aligned::iterator it = 
                info.states.begin(); it != info.states.end(); it++)
          {
            it->second -= overlap;
            if (!it->second)
            {
              to_delete.push_back(it->first);
              if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
                legion_delete(it->first);
            }
          }
          if (!to_delete.empty())
          {
            for (std::vector<VersionState*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              info.states.erase(*it);
            }
          }
        }
      }
      if (!empty_versions.empty())
      {
        for (std::vector<VersionID>::const_iterator it = 
              empty_versions.begin(); it != empty_versions.end(); it++)
        {
          previous_version_infos.erase(*it);
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::filter_current_states(VersionID vid,
                                               const FieldMask &filter_mask)
    //--------------------------------------------------------------------------
    {
      std::vector<VersionID> empty_versions;
      for (LegionMap<VersionID,VersionStateInfo>::aligned::iterator vit = 
            current_version_infos.begin(); vit != 
            current_version_infos.end(); vit++)
      {
        // If we've exceeded the target version we are done
        if (vit->first > vid)
          break;
        VersionStateInfo &info = vit->second;
        FieldMask overlap = info.valid_fields & filter_mask;
        // No overlapping fields means there is nothing to do
        if (!overlap)
          continue;
        // Special case if the version ID equals the filter vid
        // In this case move everything back to the previous versions
        if (vit->first == vid)
        {
          VersionStateInfo &target = previous_version_infos[vid];
          target.valid_fields |= overlap;
          info.valid_fields -= overlap;
          if (!info.valid_fields)
          {
            // Easy case, just merge the version states into the previous
            empty_versions.push_back(vit->first);
            if (!target.states.empty())
            {
              for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator 
                    it = info.states.begin(); it != info.states.end(); it++)
              {
                LegionMap<VersionState*,FieldMask>::aligned::iterator finder =
                  target.states.find(it->first);
                if (finder != target.states.end())
                {
                  finder->second |= it->second;
                  // Need to remove the old reference, technically we don't
                  // need the check but we'll keep it for completeness
                  if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
                    legion_delete(it->first);
                }
                else
                  target.states[it->first] = it->second; // reference flows back
              }
            }
            else
              target.states = info.states; // references flow back
          }
          else
          {
            std::vector<VersionState*> to_delete;
            // Figure out which ones we need to move back
            if (!target.states.empty())
            {
              for (LegionMap<VersionState*,FieldMask>::aligned::iterator it = 
                    info.states.begin(); it != info.states.end(); it++)
              {
                FieldMask local_overlap = it->second & overlap;
                if (!local_overlap)
                  continue;
                it->second -= local_overlap;
                LegionMap<VersionState*,FieldMask>::aligned::iterator finder = 
                  target.states.find(it->first);
                if (finder == target.states.end())
                {
                  target.states[it->first] = local_overlap;
                  // If we duplicated then we need to add a reference
                  if (!it->second)
                    to_delete.push_back(it->first);
                  else
                    it->first->add_base_valid_ref(VERSION_MANAGER_REF);
                }
                else
                {
                  finder->second |= local_overlap;
                  if (!it->second)
                  {
                    to_delete.push_back(it->first);
                    // Didn't flow back so we need to remove the reference
                    // Technically we don't need to check but do it anyway
                    // for completeness
                    if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
                      legion_delete(it->first);
                  }
                }
              }
            }
            else
            {
              // Target is empty
              for (LegionMap<VersionState*,FieldMask>::aligned::iterator it = 
                    info.states.begin(); it != info.states.end(); it++)
              {
                FieldMask local_overlap = it->second & overlap;
                if (!local_overlap)
                  continue;
                target.states[it->first] = local_overlap;
                // If we duplicated we need to add a reference otherwise
                // the reference flows back
                it->second -= local_overlap; 
                if (!it->second)
                  to_delete.push_back(it->first);
                else
                  it->first->add_base_valid_ref(VERSION_MANAGER_REF);
              }
            }
            if (!to_delete.empty())
            {
              for (std::vector<VersionState*>::const_iterator it = 
                    to_delete.begin(); it != to_delete.end(); it++)
              {
                info.states.erase(*it);
              }
            }
          }
          continue;
        }
        info.valid_fields -= overlap;
        if (!info.valid_fields)
        {
          empty_versions.push_back(vit->first);
          // Only need to remove references
          for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it =
                info.states.begin(); it != info.states.end(); it++)
          {
            if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
              legion_delete(it->first);
          }
        }
        else
        {
          std::vector<VersionState*> to_delete;
          for (LegionMap<VersionState*,FieldMask>::aligned::iterator it = 
                info.states.begin(); it != info.states.end(); it++)
          {
            it->second -= overlap;
            if (!it->second)
            {
              to_delete.push_back(it->first);
              if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
                legion_delete(it->first);
            }
          }
          if (!to_delete.empty())
          {
            for (std::vector<VersionState*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              info.states.erase(*it);
            }
          }
        }
      }
      if (!empty_versions.empty())
      {
        for (std::vector<VersionID>::const_iterator it = 
              empty_versions.begin(); it != empty_versions.end(); it++)
        {
          current_version_infos.erase(*it);
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::capture_previous_states(VersionID vid,
                                                 const FieldMask &capture_mask,
                                                 PhysicalState *state)
    //--------------------------------------------------------------------------
    {
      LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator finder = 
        previous_version_infos.find(vid);
      if (finder == previous_version_infos.end())
        return;
      if (finder->second.valid_fields * capture_mask)
        return;
      const VersionStateInfo &info = finder->second;
      for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
            info.states.begin(); it != info.states.end(); it++)
      {
        FieldMask overlap = it->second & capture_mask;
        if (!overlap)
          continue;
        state->add_version_state(it->first, overlap);
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::capture_previous_states(VersionID vid,
                                                 const FieldMask &capture_mask,
                                                 PhysicalState *state,
                                                 FieldMask &to_create)
    //--------------------------------------------------------------------------
    {
      LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator finder = 
        previous_version_infos.find(vid);
      if (finder == previous_version_infos.end())
        return;
      if (finder->second.valid_fields * capture_mask)
        return;
      const VersionStateInfo &info = finder->second;
      for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
            info.states.begin(); it != info.states.end(); it++)
      {
        FieldMask overlap = it->second & capture_mask;
        if (!overlap)
          continue;
        state->add_version_state(it->first, overlap);
        to_create -= overlap;
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::capture_current_states(VersionID vid,
                                                const FieldMask &capture_mask,
                                                PhysicalState *state,
                                                FieldMask &to_create, 
                                                bool advance)
    //--------------------------------------------------------------------------
    {
      LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator finder = 
        current_version_infos.find(vid);
      if (finder == current_version_infos.end())
        return;
      if (finder->second.valid_fields * capture_mask)
        return;
      const VersionStateInfo &info = finder->second;
      for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
            info.states.begin(); it != info.states.end(); it++)
      {
        FieldMask overlap = it->second & capture_mask;
        if (!overlap)
          continue;
        if (advance)
          state->add_advance_state(it->first, overlap);
        else
          state->add_version_state(it->first, overlap);
        to_create -= overlap;
      }
    }
        
    //--------------------------------------------------------------------------
    void VersionManager::initialize_state(LogicalView *view, Event term_event,
                                          const RegionUsage &usage,
                                          const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(current_version_infos.empty() || 
              (current_version_infos.size() == 1));
      assert(previous_version_infos.empty());
#endif
      // No need to hold the lock when initializing
      if (current_version_infos.empty())
      {
        VersionState *init_state = 
          create_new_version_state(0, user_mask, true/*initialize*/);
        init_state->add_base_valid_ref(VERSION_MANAGER_REF);
        init_state->initialize(view, term_event, usage, user_mask);
        current_version_infos[0].valid_fields = user_mask;
        current_version_infos[0].states[init_state] = user_mask;
      }
      else
      {
        LegionMap<VersionID,VersionStateInfo>::aligned::iterator finder = 
          current_version_infos.find(0);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != current_version_infos.end());
#endif
        finder->second.valid_fields |= user_mask;
#ifdef DEBUG_HIGH_LEVEL
        assert(finder->second.states.size() == 1);
#endif
        LegionMap<VersionState*,FieldMask>::aligned::iterator it = 
          finder->second.states.begin();
        it->first->initialize(view, term_event, usage, user_mask);
        it->second |= user_mask;
      }
#ifdef DEBUG_HIGH_LEVEL
      sanity_check();
#endif
    }
     
    //--------------------------------------------------------------------------
    void VersionManager::check_init(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(current_version_infos.empty());
      assert(previous_version_infos.empty());
      assert(!has_persistent);
      assert(!persistent_lock.exists());
      assert(persistent_views.empty());
#endif
    }

    //--------------------------------------------------------------------------
    void VersionManager::clear(void)
    //--------------------------------------------------------------------------
    {
      // Iterate over all the version infos and remove our references
      // No need to hold the lock because this happens in serial with everything
      if (!current_version_infos.empty())
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::iterator vit = 
              current_version_infos.begin(); vit != 
              current_version_infos.end(); vit++)
        {
          VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::iterator it = 
                info.states.begin(); it != info.states.end(); it++)
          {
            if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
              legion_delete(it->first);
          }
        }
        current_version_infos.clear();
      }
      if (!previous_version_infos.empty())
      {
        for (LegionMap<VersionID,VersionStateInfo>::aligned::iterator vit = 
              previous_version_infos.begin(); vit != 
              previous_version_infos.end(); vit++)
        {
          VersionStateInfo &info = vit->second;
          for (LegionMap<VersionState*,FieldMask>::aligned::iterator it = 
                info.states.begin(); it != info.states.end(); it++)
          {
            if (it->first->remove_base_valid_ref(VERSION_MANAGER_REF))
              legion_delete(it->first);
          }
        }
        previous_version_infos.clear();
      }
#ifdef DEBUG_HIGH_LEVEL
      observed.clear();
#endif
      if (!persistent_views.empty())
      {
        for (std::set<MaterializedView*>::const_iterator it =
              persistent_views.begin(); it != persistent_views.end(); it++)
        {
          if ((*it)->remove_base_valid_ref(PERSISTENCE_REF))
            legion_delete(*it);
        }
        persistent_views.clear();
        persistent_lock.destroy_reservation();
        persistent_lock = Reservation::NO_RESERVATION;
        has_persistent = false;
      }
#ifdef DEBUG_HIGH_LEVEL
      else
      {
        assert(!has_persistent);
        assert(!persistent_lock.exists());
      }
#endif
    }

    //--------------------------------------------------------------------------
    void VersionManager::sanity_check(void)
    //--------------------------------------------------------------------------
    {
      // This code is a sanity check that each field appears for at most
      // one version number
      FieldMask current_version_fields;
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator vit =
            current_version_infos.begin(); vit != 
            current_version_infos.end(); vit++)
      {
        const VersionStateInfo &info = vit->second;
        assert(!!info.valid_fields);
        // Make sure each field appears once in each version state info
        FieldMask local_version_fields;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          assert(!!it->second); // better not be empty
          assert(local_version_fields * it->second); // better not overlap
          local_version_fields |= it->second;
        }
        assert(info.valid_fields == local_version_fields); // beter match
        // Should not overlap with other fields in the current version
        assert(current_version_fields * info.valid_fields);
        current_version_fields |= info.valid_fields;
      }
      FieldMask previous_version_fields;
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator vit =
            previous_version_infos.begin(); vit != 
            previous_version_infos.end(); vit++)
      {
        const VersionStateInfo &info = vit->second;
        assert(!!info.valid_fields);
        // Make sure each field appears once in each version state info
        FieldMask local_version_fields;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              info.states.begin(); it != info.states.end(); it++)
        {
          assert(!!it->second); // better not be empty
          assert(local_version_fields * it->second); // better not overlap
          local_version_fields |= it->second;
        }
        assert(info.valid_fields == local_version_fields); // beter match
        // Should not overlap with other fields in the current version
        assert(previous_version_fields * info.valid_fields);
        previous_version_fields |= info.valid_fields;
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::add_persistent_view(MaterializedView *view)
    //--------------------------------------------------------------------------
    {
      view->add_base_valid_ref(PERSISTENCE_REF);
      // First see if we need to make the lock
      if (!persistent_lock.exists())
      {
        // Take the version lock just to be sure
        AutoLock v_lock(version_lock);
        if (!persistent_lock.exists())
          persistent_lock = Reservation::create_reservation();
      }
      bool remove_extra = false;
      {
        AutoLock p_lock(persistent_lock);
        if (persistent_views.find(view) == persistent_views.end())
          persistent_views.insert(view);
        else
          remove_extra = true;
        has_persistent = true;
      }
      if (remove_extra)
        view->remove_base_valid_ref(PERSISTENCE_REF);
    }

    //--------------------------------------------------------------------------
    void VersionManager::capture_persistent_views(PhysicalState *target,
                                                  const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      FieldMask empty_mask;
      // If we are here then we know the lock exists
      AutoLock p_lock(persistent_lock,1,false/*exclusive*/);
      for (std::set<MaterializedView*>::const_iterator it = 
            persistent_views.begin(); it != persistent_views.end(); it++)
      {
        if ((*it)->has_space(capture_mask))
        {
          LegionMap<LogicalView*,FieldMask>::aligned::const_iterator finder = 
            target->valid_views.find(*it);
          // Only need to add it if it is not already there
          if (finder == target->valid_views.end())
            target->valid_views[*it] = empty_mask;
        }
      }
    }

    //--------------------------------------------------------------------------
    void VersionManager::detach_instance(const FieldMask &mask, 
                                         PhysicalManager *target)
    //--------------------------------------------------------------------------
    {
#ifdef UNIMPLEMENTED_VERSIONING

#else
      assert(false);
#endif
    }

    //--------------------------------------------------------------------------
    VersionState* VersionManager::create_new_version_state(VersionID vid,
                                         const FieldMask &mask, bool initialize) 
    //--------------------------------------------------------------------------
    {
      DistributedID new_did = 
        owner->context->runtime->get_available_distributed_id(false);
      AddressSpace local_space = owner->context->runtime->address_space;
      return legion_new<VersionState>(vid, owner->context->runtime, 
                    new_did, local_space, local_space, this, mask, initialize);
    }

    //--------------------------------------------------------------------------
    VersionState* VersionManager::create_remote_version_state(VersionID vid,
                                  DistributedID did, AddressSpaceID owner_space,
                                  const FieldMask &mask, bool initialize)
    //--------------------------------------------------------------------------
    {
      AddressSpace local_space = owner->context->runtime->address_space;
#ifdef DEBUG_HIGH_LEVEL
      assert(owner_space != local_space);
#endif
      return legion_new<VersionState>(vid, owner->context->runtime, 
                        did, owner_space, local_space, this, mask, initialize);
    }

    //--------------------------------------------------------------------------
    VersionState* VersionManager::find_remote_version_state(VersionID vid,
                                       DistributedID did, AddressSpaceID source,
                                       const FieldMask &mask, bool initialize)
    //--------------------------------------------------------------------------
    {
      // Use the lock on the version manager to ensure that we don't
      // replicated version states on a node
      VersionState *result = NULL;
      Runtime *runtime = owner->context->runtime;
      {
        AutoLock v_lock(version_lock);
        if (runtime->has_distributed_collectable(did))
        {
          DistributedCollectable *dc = 
            runtime->find_distributed_collectable(did);
#ifdef DEBUG_HIGH_LEVEL
          result = dynamic_cast<VersionState*>(dc);
          assert(result != NULL);
#else
          result = static_cast<VersionState*>(dc);
#endif
          if (initialize)
            result->record_initial_fields(mask);
        }
        else // Otherwise make it
          result = create_remote_version_state(vid, did, source, 
                                               mask, initialize);
      }
      return result;
    }

    //--------------------------------------------------------------------------
    void VersionManager::print_physical_state(RegionTreeNode *node,
                                              const FieldMask &capture_mask,
                          LegionMap<ColorPoint,FieldMask>::aligned &to_traverse,
                                              TreeStateLogger *logger)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      PhysicalState temp_state(this, node);
#else
      PhysicalState temp_state(this);
#endif
      logger->log("Versions:");
      logger->down();
      for (LegionMap<VersionID,VersionStateInfo>::aligned::const_iterator vit = 
            current_version_infos.begin(); vit != 
            current_version_infos.end(); vit++)
      {
        if (capture_mask * vit->second.valid_fields)
          continue;
        FieldMask version_fields;
        for (LegionMap<VersionState*,FieldMask>::aligned::const_iterator it = 
              vit->second.states.begin(); it != vit->second.states.end(); it++)
        {
          FieldMask overlap = capture_mask & it->second;
          if (!overlap)
            continue;
          version_fields |= overlap;
          it->first->update_physical_state(&temp_state, overlap);
        }
        assert(!!version_fields);
        char *version_buffer = version_fields.to_string();
        logger->log("%lld: %s", vit->first, version_buffer);
        free(version_buffer);
      }
      logger->up();
      temp_state.print_physical_state(capture_mask, to_traverse, logger);
    }

    /////////////////////////////////////////////////////////////
    // Region Tree Node 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RegionTreeNode::RegionTreeNode(RegionTreeForest *ctx, 
                                   FieldSpaceNode *column_src)
      : context(ctx), column_source(column_src)
    //--------------------------------------------------------------------------
    {
      this->node_lock = Reservation::create_reservation(); 
    }

    //--------------------------------------------------------------------------
    RegionTreeNode::~RegionTreeNode(void)
    //--------------------------------------------------------------------------
    {
      node_lock.destroy_reservation();
      node_lock = Reservation::NO_RESERVATION;
      for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
            semantic_info.begin(); it != semantic_info.end(); it++)
      {
        legion_free(SEMANTIC_INFO_ALLOC, it->second.buffer, it->second.size);
      }
    }

    //--------------------------------------------------------------------------
    LogicalState& RegionTreeNode::get_logical_state(ContextID ctx)
    //--------------------------------------------------------------------------
    {
      // We pass in the necessary information for initializing restricted
      // fields in case the logical state is going to be create
      return *(logical_states.lookup_entry(ctx, this, ctx));
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::set_restricted_fields(ContextID ctx,
                                               FieldMask &child_restricted)
    //--------------------------------------------------------------------------
    {
      LogicalState &state = get_logical_state(ctx);
      if (!!state.restricted_fields)
        child_restricted = state.restricted_fields;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::attach_semantic_information(SemanticTag tag,
                                                     const NodeSet &mask,
                                                     const void *buffer,
                                                     size_t size)
    //--------------------------------------------------------------------------
    {
      // Make a copy
      void *local = legion_malloc(SEMANTIC_INFO_ALLOC, size);
      memcpy(local, buffer, size);
      NodeSet diff, current;
      {
        AutoLock n_lock(node_lock); 
        // See if it already exists
        LegionMap<SemanticTag,SemanticInfo>::aligned::iterator finder = 
          semantic_info.find(tag);
        if (finder != semantic_info.end())
        {
          // Check to make sure that the bits are the same
          if (size != finder->second.size)
          {
            log_run.error("ERROR: Inconsistent Semantic Tag value "
                                "for tag %ld with different sizes of %ld"
                                " and %ld for region tree node", 
                                tag, size, finder->second.size);
#ifdef DEBUG_HIGH_LEVEL
            assert(false);
#endif
            exit(ERROR_INCONSISTENT_SEMANTIC_TAG);       
          }
          // Otherwise do a bitwise comparison
          {
            const char *orig = (const char*)finder->second.buffer;
            const char *next = (const char*)buffer;
            for (unsigned idx = 0; idx < size; idx++)
            {
              char diff = orig[idx] ^ next[idx];
              if (diff)
              {
                log_run.error("ERROR: Inconsistent Semantic Tag value "
                                    "for tag %ld with different values at"
                                    "byte %d for region tree node, %x != %x", 
                                    tag, idx, orig[idx], next[idx]);
#ifdef DEBUG_HIGH_LEVEL
                assert(false);
#endif
                exit(ERROR_INCONSISTENT_SEMANTIC_TAG);
              }
            }
          }
          finder->second.node_mask |= mask;
          diff = creation_set - finder->second.node_mask;
          current = finder->second.node_mask;
        }
        else
        {
          semantic_info[tag] = SemanticInfo(local, size, mask);
          diff = creation_set - mask;
          current = mask;
        }
      }
      if (!!diff)
        send_semantic_info(diff, tag, local, size, current);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::retrieve_semantic_information(SemanticTag tag,
                                                       const void *&result,
                                                       size_t &size)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      LegionMap<SemanticTag,SemanticInfo>::aligned::const_iterator finder = 
        semantic_info.find(tag);
      if (finder == semantic_info.end())
      {
        log_run.error("ERROR: invalid semantic tag %ld for "
                            "index tree node", tag);   
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_SEMANTIC_TAG);
      }
      result = finder->second.buffer;
      size = finder->second.size;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::register_logical_node(ContextID ctx, 
                                               const LogicalUser &user,
                                               RegionTreePath &path,
                                               VersionInfo &version_info,
                                               RestrictInfo &restrict_info,
                                               const TraceInfo &trace_info,
                                               const bool projecting)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REGISTER_LOGICAL_NODE_CALL);
#endif
      LogicalState &state = get_logical_state(ctx);
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
      const unsigned depth = get_depth();
      const bool arrived = !path.has_child(depth);
      FieldMask open_below;
      ColorPoint next_child;
      if (!arrived)
        next_child = path.get_child(depth);
      // If we've arrived and we're doing analysis for a projection 
      // requirement then we skip the closes for now as we will end up
      // doing them later. We can also skip the close operations for 
      // any operations which have arrived and have read-write privileges
      // because they will be doing their own close operations.
      if (!arrived || !(projecting || IS_WRITE(user.usage)))
      {
        // Now check to see if we need to do any close operations
        // Close up any children which we may have dependences on below
        LogicalCloser closer(ctx, user, arrived/*validates*/, true/*captures*/);
        siphon_logical_children(closer, state, user.field_mask,
                  !arrived || IS_READ_ONLY(user.usage) || IS_REDUCE(user.usage),
                  next_child, open_below);
        // We always need to create and register close operations
        // regardless of whether we are tracing or not
        // If we're not replaying a trace we need to do work here
        // See if we need to register a close operation
        if (closer.has_closed_fields())
        {
          // Generate the close operations         
          const FieldMask &closed_mask = closer.get_closed_mask();
          // We need to record the version numbers for this node as well
          closer.record_top_version_numbers(this, state);
          closer.initialize_close_operations(this, user.op, version_info, 
                                             restrict_info, trace_info);
          if (!arrived)
            closer.add_next_child(next_child);
          // Perform dependence analysis for all the close operations
          closer.perform_dependence_analysis(user, open_below,
                                             state.curr_epoch_users,
                                             state.prev_epoch_users);
          // Now we can flush out all the users dominated by closes
          filter_prev_epoch_users(state, closed_mask);
          filter_curr_epoch_users(state, closed_mask);
          // Note we don't need to update the version numbers because
          // that happened when we recorded dirty fields below. 
          // However, we do need to mark that there is no longer any
          // dirty data below this node for all the closed fields
          state.dirty_below -= closed_mask;
          // Now we can add the close operations to the current epoch
          closer.register_close_operations(state.curr_epoch_users);
        }
      }
      FieldMask dominator_mask;
      if (!arrived || !projecting)
      {
        // We also always do our dependence analysis even if we have
        // already traced because we need to pick up dependences on 
        // any dynamic close operations that we need to do
        // Now that we registered any close operation, do our analysis
        dominator_mask = 
               perform_dependence_checks<CURR_LOGICAL_ALLOC,
                         true/*record*/,false/*has skip*/,true/*track dom*/>(
                            user, state.curr_epoch_users, user.field_mask, 
                            open_below, arrived/*validates*/ && !projecting);
        FieldMask non_dominated_mask = user.field_mask - dominator_mask;
        // For the fields that weren't dominated, we have to check
        // those fields against the previous epoch's users
        if (!!non_dominated_mask)
        {
          perform_dependence_checks<PREV_LOGICAL_ALLOC,
                          true/*record*/, false/*has skip*/, false/*track dom*/>(
                              user, state.prev_epoch_users, non_dominated_mask, 
                              open_below, arrived/*validates*/ && !projecting);
        }
      }
      const bool is_write = IS_WRITE(user.usage); // only writes
      if (arrived)
      { 
        // If we dominated and this is our final destination then we 
        // can filter the operations since we actually do dominate them
        if (!!dominator_mask)
        {
          // Dominator mask is not empty
          // Mask off all the dominated fields from the previous set
          // of epoch users and remove any previous epoch users
          // that were totally dominated
          filter_prev_epoch_users(state, dominator_mask); 
          // Mask off all dominated fields from current epoch users and move
          // them to prev epoch users.  If all fields masked off, then remove
          // them from the list of current epoch users.
          filter_curr_epoch_users(state, dominator_mask);
          // We only advance version numbers for fields which are being
          // written and dominated the previous epoch because multiple 
          // writes for atomic and simultaneous go in the same generation.
          if (is_write)
            advance_version_numbers(state, dominator_mask);
        }
        // Now that we've arrived, check to see if we are a projection 
        // region requirement or a normal region requirement. If we are normal
        // then we can do the regular analysis, otherwise, we have to traverse
        // the paths for all the projected regions
        if (projecting)
        {
          // Compute the fat tree path for this operation and then do the
          // traversal over the entire sub-tree
          FatTreePath *fat_path = user.op->compute_fat_path(user.idx); 
          register_logical_fat_path(ctx, user, fat_path, 
                                    version_info, restrict_info, trace_info);
          delete fat_path;
        }
        else
        {
          // Easy case, we are there 
          // If we have arrived and we are doing read-write access, then we
          // need to capture any versions in sub-trees for which we will 
          // be issuing a close when we actually map.
          if (is_write)
          {
            LogicalCloser closer(ctx,user,true/*validates*/,false/*captures*/);
            // There's no point in siphoning, we know we need to close
            // everything up that interferes with this task
            close_logical_subtree(closer, user.field_mask);
            // We've registered dependences on any users in the sub-tree
            // and we definitely interfered with them all so all we need
            // to do now is capture the version information.
            closer.merge_version_info(version_info, user.field_mask);
          }
          // We also need to record the needed version numbers for this node
          // Note that we do this after the version numbers have been updated
          // so that we get the version numbers that we are contributing to
          // as part of the execution for this operation. If we are writing
          // in any way, then record the previous version number
          // If this is a projection requirement, we also need to record any
          // version numbers from farther down in the tree as well. 
          // Do this before the version numbers can be updated.
          record_version_numbers(state, user.field_mask, version_info, 
                                 is_write, false/*premap*/, false/*path only*/,
                                 is_write, false/*close top*/);
          
          // If this is a reduction, record that we have an outstanding 
          // reduction at this node in the region tree
          if (user.usage.redop > 0)
            record_logical_reduction(state, user.usage.redop, user.field_mask);
          // Record any restrictions we have on mappings if necessary
          if (restrict_info.needs_check())
          {
            FieldMask restricted = user.field_mask & state.restricted_fields;
            if (!!restricted)
            {
              RegionNode *local_this = as_region_node();
              restrict_info.add_restriction(local_this->handle, restricted);
            }
          }
        }
        // Here is the only difference with tracing.  If we already
        // traced then we don't need to register ourselves as a user
        if (!trace_info.already_traced)
        {
          // Register ourself with as a current user of this region
          // Record a mapping reference on this operation
          user.op->add_mapping_reference(user.gen);
          // Add ourselves to the current epoch
          state.curr_epoch_users.push_back(user);
        }
      }
      else // We're still not there, so keep going
      {
        const bool has_write = HAS_WRITE(user.usage); // write or reduce
        // If we are writing in any way (including reduces), check to
        // see if we have already marked those fields dirty. If not,
        // advance the version numbers for those fields.
        if (has_write)
        {
          FieldMask new_dirty_fields = user.field_mask - state.dirty_below;
          if (!!new_dirty_fields)
          {
            advance_version_numbers(state, new_dirty_fields);
            state.dirty_below |= new_dirty_fields;
          }
        }
        // Record version numbers of the fields which we are contributing
        // to from this node.
        record_version_numbers(state, user.field_mask, version_info, 
                               is_write, true/*premap*/, true/*path only*/,
                               false/*final*/, false/*close top*/);
        RegionTreeNode *child = get_tree_child(next_child);
        if (!open_below)
          child->open_logical_node(ctx, user, path, version_info,
                        restrict_info, trace_info.already_traced, projecting);
        else
          child->register_logical_node(ctx, user, path, version_info, 
                                       restrict_info, trace_info, projecting);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::open_logical_node(ContextID ctx,
                                             const LogicalUser &user,
                                             RegionTreePath &path,
                                             VersionInfo &version_info,
                                             RestrictInfo &restrict_info,
                                             const bool already_traced,
                                             const bool projecting)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, OPEN_LOGICAL_NODE_CALL);
#endif
      LogicalState &state = get_logical_state(ctx);
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
      const unsigned depth = get_depth(); 
      const bool is_write = IS_WRITE(user.usage);
      if (!path.has_child(depth))
      {
        // If this is a write, then update our version numbers
        if (is_write)
          advance_version_numbers(state, user.field_mask);
        // If this is a projection then we do need to capture any 
        // child version information because it might change
        if (projecting)
        {
          // Compute the fat tree path for this operation and then do the
          // traversal over the entire sub-tree
          FatTreePath *fat_path = user.op->compute_fat_path(user.idx); 
          open_logical_fat_path(ctx, user, fat_path, 
                                version_info, restrict_info);
          delete fat_path;
        }
        else
        {
          // First record any version information that we need
          record_version_numbers(state, user.field_mask, version_info, 
                                 is_write, false/*premap*/, false/*path only*/, 
                                 is_write, false/*close top*/);
          // If this is a reduction, record that we have an outstanding 
          // reduction at this node in the region tree
          if (user.usage.redop > 0)
            record_logical_reduction(state, user.usage.redop, user.field_mask);
          // Record any restrictions we have on mappings if necessary
          if (restrict_info.needs_check())
          {
            FieldMask restricted = user.field_mask & state.restricted_fields;
            if (!!restricted)
            {
              RegionNode *local_this = as_region_node();
              restrict_info.add_restriction(local_this->handle, restricted);
            }
          }
        }
        if (!already_traced)
        {
          // We've arrived where we're going,
          // add ourselves as a user
          // Record a mapping reference on this operation
          user.op->add_mapping_reference(user.gen);
          state.curr_epoch_users.push_back(user);
        }
      }
      else
      {
        const bool has_write = HAS_WRITE(user.usage);
        // If we are writing in any way (including reduces), check to
        // see if we have already marked those fields dirty. If not,
        // advance the version numbers for those fields.
        if (has_write)
        {
          FieldMask new_dirty_fields = user.field_mask - state.dirty_below;
          if (!!new_dirty_fields)
          {
            advance_version_numbers(state, new_dirty_fields);
            state.dirty_below |= new_dirty_fields;
          }
        }
        // Record version numbers of the fields which we are contributing
        // to from this node.
        record_version_numbers(state, user.field_mask, version_info, 
                               is_write, true/*premap*/, true/*path only*/, 
                               false/*final*/, false/*close top*/);
        const ColorPoint &next_child = path.get_child(depth);
        // Update our field states
        merge_new_field_state(state, 
                              FieldState(user, user.field_mask, next_child));
#ifdef DEBUG_HIGH_LEVEL
        sanity_check_logical_state(state);
#endif
        // Then continue the traversal
        RegionTreeNode *child_node = get_tree_child(next_child);
        child_node->open_logical_node(ctx, user, path, version_info, 
                                     restrict_info, already_traced, projecting);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::register_logical_fat_path(ContextID ctx,
                                                   const LogicalUser &user,
                                                   FatTreePath *fat_path,
                                                   VersionInfo &version_info,
                                                   RestrictInfo &restrict_info,
                                                   const TraceInfo &trace_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REGISTER_LOGICAL_NODE_CALL);
#endif
      LogicalState &state = get_logical_state(ctx);
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
      const std::map<ColorPoint,FatTreePath*> &children = 
                                                fat_path->get_children();
      const bool arrived = children.empty();
      std::map<ColorPoint,bool> open_only;
      FieldMask any_open_below;
      // Perform the close operations for all the children
      {
        LogicalCloser closer(ctx, user, arrived/*validates*/, true/*captures*/);
        if (!arrived)
        {
          // Close up any interfering children, we know our children
          // are disjoint so don't worry about them interferring with
          // each other. This was checked when the fat path was created
          for (std::map<ColorPoint,FatTreePath*>::const_iterator it = 
                children.begin(); it != children.end(); it++)
          {
            FieldMask open_below;
            siphon_logical_children(closer, state, user.field_mask,
                        true/*not arrived so close*/, it->first, open_below);
            open_only[it->first] = !open_below;
            any_open_below |= open_below;
          }
        }
        else if (!IS_WRITE(user.usage))
        {
          // Anything other than a write will do the normal close
          // Writes get their own special close routine
          // Otherwise just do the normal single close operation
          siphon_logical_children(closer, state, user.field_mask,
                          IS_READ_ONLY(user.usage) || IS_REDUCE(user.usage),
                          ColorPoint(), any_open_below);
        }
        if (closer.has_closed_fields())
        {
          // Generate the close operations         
          const FieldMask &closed_mask = closer.get_closed_mask();
          // We need to record the version numbers for this node as well
          closer.record_top_version_numbers(this, state);
          closer.initialize_close_operations(this, user.op, version_info, 
                                             restrict_info, trace_info);
          if (!arrived)
          {
            for (std::map<ColorPoint,FatTreePath*>::const_iterator it = 
                  children.begin(); it != children.end(); it++)
            {
              closer.add_next_child(it->first);
            }
          }
          // Perform dependence analysis for all the close operations
          closer.perform_dependence_analysis(user, any_open_below,
                                             state.curr_epoch_users,
                                             state.prev_epoch_users);
          // Now we can flush out all the users dominated by closes
          filter_prev_epoch_users(state, closed_mask);
          filter_curr_epoch_users(state, closed_mask);
          // Note we don't need to update the version numbers because
          // that happened when we recorded dirty fields below. 
          // However, we do need to mark that there is no longer any
          // dirty data below this node for all the closed fields
          state.dirty_below -= closed_mask;
          // Now we can add the close operations to the current epoch
          closer.register_close_operations(state.curr_epoch_users);
        }
      }
      // We also always do our dependence analysis even if we have
      // already traced because we need to pick up dependences on 
      // any dynamic close operations that we need to do
      // Now that we registered any close operation, do our analysis
      FieldMask dominator_mask = 
             perform_dependence_checks<CURR_LOGICAL_ALLOC,
                       true/*record*/,false/*has skip*/,true/*track dom*/>(
                          user, state.curr_epoch_users, user.field_mask, 
                          any_open_below, arrived/*validates*/);
      FieldMask non_dominated_mask = user.field_mask - dominator_mask;
      // For the fields that weren't dominated, we have to check
      // those fields against the previous epoch's users
      if (!!non_dominated_mask)
      {
        perform_dependence_checks<PREV_LOGICAL_ALLOC,
                        true/*record*/, false/*has skip*/, false/*track dom*/>(
                            user, state.prev_epoch_users, non_dominated_mask, 
                            any_open_below, arrived/*validates*/);
      }
      // Unlike for a normal traversal, the user we are registering is 
      // actually the upper bound which was registered at this level of
      // the tree or above, so we can always consider ourselves a dominator
      if (!!dominator_mask)
      {
        // Dominator mask is not empty
        // Mask off all the dominated fields from the previous set
        // of epoch users and remove any previous epoch users
        // that were totally dominated
        filter_prev_epoch_users(state, dominator_mask); 
        // Mask off all dominated fields from current epoch users and move
        // them to prev epoch users.  If all fields masked off, then remove
        // them from the list of current epoch users.
        filter_curr_epoch_users(state, dominator_mask);
      }
      const bool is_write = IS_WRITE(user.usage);
      if (arrived)
      {
        // If we dominated and this is our final destination then we 
        // can filter the operations since we actually do dominate them
        // We only advance version numbers for fields which are being
        // written and dominated the previous epoch because multiple 
        // writes for atomic and simultaneous go in the same generation.
        if (!!dominator_mask && is_write)
            advance_version_numbers(state, dominator_mask);
        // If we have arrived and we are doing read-write access, then we
        // need to capture any versions in sub-trees for which we will 
        // be issuing a close when we actually map.
        if (is_write)
        {
          LogicalCloser closer(ctx, user, true/*validates*/, false/*captures*/);
          // There's no point in siphoning, we know we need to close
          // everything up that interferes with this task
          close_logical_subtree(closer, user.field_mask);
          // We've registered dependences on any users in the sub-tree
          // and we definitely interfered with them all so all we need
          // to do now is capture the version information.
          closer.merge_version_info(version_info, user.field_mask);
        }
        // No need to register ourselves as a user 
        record_version_numbers(state, user.field_mask, version_info, 
                               is_write, false/*premap*/, false/*path only*/, 
                               is_write, false/*close top*/);
        // If this is a reduction, record that we have an outstanding 
        // reduction at this node in the region tree
        if (user.usage.redop > 0)
          record_logical_reduction(state, user.usage.redop, user.field_mask);
        // Record any restrictions we have on mappings if necessary
        if (restrict_info.needs_check())
        {
          FieldMask restricted = user.field_mask & state.restricted_fields;
          if (!!restricted)
          {
            RegionNode *local_this = as_region_node();
            restrict_info.add_restriction(local_this->handle, restricted);
          }
        }
      }
      else
      {
        const bool has_write = HAS_WRITE(user.usage);
        // If we are writing in any way (including reduces), check to
        // see if we have already marked those fields dirty. If not,
        // advance the version numbers for those fields.
        if (has_write)
        {
          FieldMask new_dirty_fields = user.field_mask - state.dirty_below;
          if (!!new_dirty_fields)
          {
            advance_version_numbers(state, new_dirty_fields);
            state.dirty_below |= new_dirty_fields;
          }
        }
        // Record version numbers of the fields which we are contributing
        // to from this node.
        record_version_numbers(state, user.field_mask, version_info, 
                               is_write, false/*premap*/, true/*path only*/, 
                               false/*final*/, false/*close top*/);
        for (std::map<ColorPoint,FatTreePath*>::const_iterator it = 
              children.begin(); it != children.end(); it++)
        {
          RegionTreeNode *child = get_tree_child(it->first);
          if (open_only[it->first])
            child->open_logical_fat_path(ctx, user, it->second, 
                                         version_info, restrict_info);
          else
            child->register_logical_fat_path(ctx, user, it->second,
                                       version_info, restrict_info, trace_info);
        }
      }
    }
    
    //--------------------------------------------------------------------------
    void RegionTreeNode::open_logical_fat_path(ContextID ctx,
                                               const LogicalUser &user,
                                               FatTreePath *fat_path,
                                               VersionInfo &version_info,
                                               RestrictInfo &restrict_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, OPEN_LOGICAL_NODE_CALL);
#endif
      LogicalState &state = get_logical_state(ctx); 
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
      const std::map<ColorPoint,FatTreePath*> &children = 
                                                fat_path->get_children();
      const bool arrived = children.empty();
      const bool is_write = IS_WRITE(user.usage);
      if (arrived)
      {
        if (is_write)
          advance_version_numbers(state, user.field_mask);
        // First record any version information that we need
        record_version_numbers(state, user.field_mask, version_info, 
                               is_write, false/*premap*/, false/*path only*/, 
                               is_write, false/*close top*/);
        // If this is a reduction, record that we have an outstanding 
        // reduction at this node in the region tree
        if (user.usage.redop > 0)
          record_logical_reduction(state, user.usage.redop, user.field_mask);
        // Record any restrictions we have on mappings if necessary
        if (restrict_info.needs_check())
        {
          FieldMask restricted = user.field_mask & state.restricted_fields;
          if (!!restricted)
          {
            RegionNode *local_this = as_region_node();
            restrict_info.add_restriction(local_this->handle, restricted);
          }
        }
      }
      else
      {
        const bool has_write = HAS_WRITE(user.usage);
        // If we are writing in any way (including reduces), check to
        // see if we have already marked those fields dirty. If not,
        // advance the version numbers for those fields.
        if (has_write)
        {
          FieldMask new_dirty_fields = user.field_mask - state.dirty_below;
          if (!!new_dirty_fields)
          {
            advance_version_numbers(state, new_dirty_fields);
            state.dirty_below |= new_dirty_fields;
          }
        }
        // Record version numbers of the fields which we are contributing
        // to from this node.
        record_version_numbers(state, user.field_mask, version_info, 
                               is_write, false/*premap*/, true/*path only*/, 
                               false/*final*/, false/*close top*/);
        for (std::map<ColorPoint,FatTreePath*>::const_iterator it = 
              children.begin(); it != children.end(); it++)
        {
          // Update our field states
          merge_new_field_state(state, 
                                FieldState(user, user.field_mask, it->first));
#ifdef DEBUG_HIGH_LEVEL
          sanity_check_logical_state(state);
#endif

          RegionTreeNode *child = get_tree_child(it->first);
          child->open_logical_fat_path(ctx, user, it->second, 
                                       version_info, restrict_info);
        }
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::close_reduction_analysis(ContextID ctx, 
                                                  const LogicalUser &user,
                                                  VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
      LogicalState &state = get_logical_state(ctx);
      LogicalCloser closer(ctx, user, false/*validates*/, true/*captures*/);
      ColorPoint dummy_next_child;
      FieldMask dummy_open_below;
      siphon_logical_children(closer, state, user.field_mask, false/*record*/,
                              dummy_next_child, dummy_open_below);
      // At this point we have closed up any children and captured dependences
      // Get the version info
      closer.merge_version_info(version_info, user.field_mask);
      // Capture dependences on any users at this level
      perform_closing_checks<CURR_LOGICAL_ALLOC>(closer, 
                                     state.curr_epoch_users, user.field_mask);
      perform_closing_checks<PREV_LOGICAL_ALLOC>(closer, 
                                     state.prev_epoch_users, user.field_mask);
      record_version_numbers(state, user.field_mask, version_info,
                             false/*advance*/, false/*premap only*/,
                             false/*path only*/, true/*final*/, 
                             false/*close top*/);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::close_logical_subtree(LogicalCloser &closer,
                                               const FieldMask &closing_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, CLOSE_LOGICAL_NODE_CALL);
#endif
      LogicalState &state = get_logical_state(closer.ctx);

      // No need to perform any local checks since they have all been done
      // Recursively traverse any open children and close them
      LegionDeque<FieldState>::aligned dummy_states;
      ColorPoint next_child; // invalid next point
      for (std::list<FieldState>::iterator it = state.field_states.begin();
            it != state.field_states.end(); /*nothing*/)
      {
        FieldMask overlap = it->valid_fields & closing_mask;
        if (!overlap)
        {
          it++;
          continue;
        }
        FieldMask already_open;
        perform_close_operations(closer, overlap, *it,
                                 next_child, false/*allow next*/,
                                 false/*upgrade*/, false/*leave open*/,
                                 false/*record close operations*/,
                                 dummy_states, already_open);
        // Remove the state if it is now empty
        if (!it->valid_fields)
          it = state.field_states.erase(it);
        else
          it++;
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(dummy_states.empty());
#endif
      // No need to record or advance version numbers since that will 
      // be done by the caller
      // We can mark that there is no longer any dirty data below
      state.dirty_below -= closing_mask;
      // We can also clear any outstanding reduction fields
      if (!(state.outstanding_reduction_fields * closing_mask))
        clear_logical_reduction_fields(state, closing_mask);
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::close_logical_node(LogicalCloser &closer,
                                            const FieldMask &closing_mask,
                                            bool permit_leave_open)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, CLOSE_LOGICAL_NODE_CALL);
#endif
      LogicalState &state = get_logical_state(closer.ctx);

      // Perform closing checks on both the current epoch users
      // as well as the previous epoch users
      perform_closing_checks<CURR_LOGICAL_ALLOC>(closer, 
                                     state.curr_epoch_users, closing_mask);
      perform_closing_checks<PREV_LOGICAL_ALLOC>(closer, 
                                     state.prev_epoch_users, closing_mask);
      
      // Recursively traverse any open children and close them as well
      LegionDeque<FieldState>::aligned new_states;
      for (std::list<FieldState>::iterator it = state.field_states.begin();
            it != state.field_states.end(); /*nothing*/)
      {
        FieldMask overlap = it->valid_fields & closing_mask;
        if (!overlap)
        {
          it++;
          continue;
        }
        // Recursively perform any close operations
        FieldMask already_open;
        perform_close_operations(closer, overlap, *it, 
                                 ColorPoint()/*next child*/,
                                 false/*allow next*/, false/*upgrade*/,
                                 permit_leave_open,
                                 false/*record close operations*/,
                                 new_states, already_open);
        // Remove the state if it is now empty
        if (!it->valid_fields)
          it = state.field_states.erase(it);
        else
          it++;
      }
      // Merge any new field states
      merge_new_field_states(state, new_states);
      // Record the version numbers that we need
      closer.record_version_numbers(this, state, 
                                    closing_mask, permit_leave_open);
      // If we're doing a close operation, that means someone is
      // going to be writing to a region that aliases with this one
      // so we need to advance the field version. However, if we're
      // staying open then we don't need to be advanced since we
      // will still be valid
      if (!permit_leave_open)
        advance_version_numbers(state, closing_mask);
      // We can also mark that there is no longer any dirty data below
      state.dirty_below -= closing_mask;
      // We can also clear any outstanding reduction fields
      if (!(state.outstanding_reduction_fields * closing_mask))
        clear_logical_reduction_fields(state, closing_mask);
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::siphon_logical_children(LogicalCloser &closer,
                                                 LogicalState &state,
                                                 const FieldMask &current_mask,
                                                 bool record_close_operations,
                                                 const ColorPoint &next_child,
                                                 FieldMask &open_below)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, SIPHON_LOGICAL_CHILDREN_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
      LegionDeque<FieldState>::aligned new_states;
      // Before looking at any child states, first check to see if we need
      // to do any closes to flush open reductions. This should be a pretty
      // rare operation since we often won't have lots of reductions going
      // on at different levels of the region tree.
      if (!!state.outstanding_reduction_fields)
      {
        FieldMask reduction_flush_fields = 
          current_mask & state.outstanding_reduction_fields;
        if (!!reduction_flush_fields)
        {
          // If we are doing a reduction too, check to see if they are 
          // the same in which case we can skip these fields
          if (closer.user.usage.redop > 0)
          {
            LegionMap<ReductionOpID,FieldMask>::aligned::const_iterator finder =
              state.outstanding_reductions.find(closer.user.usage.redop);
            // Don't need to flush fields we are reducing to with the
            // same operation
            if (finder != state.outstanding_reductions.end())
              reduction_flush_fields -= finder->second;
          }
          // See if we still have fields to close
          if (!!reduction_flush_fields)
          {
            FieldMask flushed_fields;
            // We need to flush these fields so issue close operations
            for (std::list<FieldState>::iterator it = 
                  state.field_states.begin(); it != 
                  state.field_states.end(); /*nothing*/)
            {
              FieldMask overlap = it->valid_fields & reduction_flush_fields;
              if (!overlap)
              {
                it++;
                continue;
              }
              flushed_fields |= overlap;
              perform_close_operations(closer, overlap, *it,
                                       next_child, false/*allow_next*/,
                                       false/*needs upgrade*/,
                                       false/*permit leave open*/,
                                       record_close_operations,
                                       new_states, open_below);
              if (!it->valid_fields)
                it = state.field_states.erase(it);
              else
                it++;
            }
            // Check to see if we have any unflushed fields
            // These are fields which still need a close operation
            // to be performed but only to flush the reductions
            FieldMask unflushed = reduction_flush_fields - flushed_fields;
            if (!!unflushed)
              closer.record_flush_only_fields(unflushed);
            // Then we can mark that these fields no longer have 
            // unflushed reductions
            clear_logical_reduction_fields(state, reduction_flush_fields);
          }
        }
      }

      // Now we can look at all the children
      for (std::list<FieldState>::iterator it = state.field_states.begin();
            it != state.field_states.end(); /*nothing*/)
      {
        // Quick check for disjointness, in which case we can continue
        if (it->valid_fields * current_mask)
        {
          it++;
          continue;
        }
        // Now check the current state
        switch (it->open_state)
        {
          case OPEN_READ_ONLY:
            {
              if (IS_READ_ONLY(closer.user.usage))
              {
                // Everything is read-only
                // See if the child that we want is already open
                if (next_child.is_valid())
                {
                  LegionMap<ColorPoint,FieldMask>::aligned::const_iterator 
                    finder = it->open_children.find(next_child);
                  if (finder != it->open_children.end())
                  {
                    // Remove the child's open fields from the
                    // list of fields we need to open
                    open_below |= finder->second;
                  }
                }
                it++;
              }
              else
              {
                // Not read-only
                // Close up all the open partitions except the one
                // we want to go down, make a new state to be added
                // containing the fields that are still open and mark
                // that we need an upgrade from read-only to some
                // kind of write operation. Note that we don't need to
                // actually perform close operations here because closing
                // read-only children requires no work.
                const bool needs_upgrade = HAS_WRITE(closer.user.usage);
                FieldMask already_open;
                perform_close_operations(closer, current_mask, *it, next_child,
                                         true/*allow next*/,
                                         needs_upgrade,
                                         false/*permit leave open*/,
                                         false/*record_close_operations*/,
                                         new_states, already_open);
                open_below |= already_open;
                if (needs_upgrade && !!already_open)
                  new_states.push_back(
                      FieldState(closer.user, already_open, next_child));
                // See if there are still any valid open fields
                if (!it->valid_fields)
                  it = state.field_states.erase(it);
                else
                  it++;
              }
              break;
            }
          case OPEN_READ_WRITE:
            {
              // Close up any open partitions that conflict with ours
              perform_close_operations(closer, current_mask, *it, next_child,
                                       true/*allow next*/,
                                       false/*needs upgrade*/,
                                       IS_READ_ONLY(closer.user.usage),
                                       record_close_operations,
                                       new_states, open_below);
              if (!it->valid_fields)
                it = state.field_states.erase(it);
              else
                it++;
              break;
            }
          case OPEN_SINGLE_REDUCE:
            {
              // Check to see if we have a child we want to go down
              if (next_child.is_valid())
              {
                // There are four cases here:
                //   1. Same reduction, same child -> everything stays the same
                //   2. Same reduction, different child -> go to MULTI_REDUCE
                //   3. Diff operation, same child -> go to READ_WRITE
                //   4. Diff operation, diff child -> close everything up
                if (IS_REDUCE(closer.user.usage) && 
                    (it->redop == closer.user.usage.redop))
                {
                  // Cases 1 and 2
                  bool needs_recompute = false;
                  std::vector<ColorPoint> to_delete;
                  // Go through all the children and see if there is any overlap
                  for (LegionMap<ColorPoint,FieldMask>::aligned::iterator 
                        cit = it->open_children.begin(); cit !=
                        it->open_children.end(); cit++)
                  {
                    FieldMask already_open = cit->second & current_mask;
                    // If disjoint children, nothing to do
                    if (!already_open || 
                        are_children_disjoint(cit->first, next_child))
                      continue;
                    // Add the already open fields to this open_below mask
                    // since either they are already open for the right child
                    // or we're going to mark them open in a new FieldState
                    open_below |= already_open;
                    // Case 2
                    if (cit->first != (next_child))
                    {
                      // Different child so we need to create a new
                      // FieldState in MULTI_REDUCE mode with two
                      // children open
                      FieldState new_state(closer.user,already_open,cit->first);
                      // Add the next child as well
                      new_state.open_children[next_child] = 
                        already_open;
                      new_state.open_state = OPEN_MULTI_REDUCE;
#ifdef DEBUG_HIGH_LEVEL
                      assert(!!new_state.valid_fields);
#endif
                      new_states.push_back(new_state);
                      // Update the current child, mark that we need to
                      // recompute the valid fields for the state
                      cit->second -= already_open;
                      if (!cit->second)
                        to_delete.push_back(cit->first);
                      needs_recompute = true;
                    }
                    // Otherwise same child so case 1 and everything just
                    // stays in SINGLE_REDUCE_MODE
                  }
                  // See if we need to recompute any properties
                  // of the current state to see if they are still valid
                  if (needs_recompute)
                  {
                    // Remove all the empty children
                    for (std::vector<ColorPoint>::const_iterator cit = 
                          to_delete.begin(); cit != to_delete.end(); cit++)
                    {
                      LegionMap<ColorPoint,FieldMask>::aligned::iterator 
                        finder = it->open_children.find(*cit);
#ifdef DEBUG_HIGH_LEVEL
                      assert(finder != it->open_children.end());
                      assert(!finder->second);
#endif
                      it->open_children.erase(finder);
                    }
                    // Then recompute the valid mask for the current state
                    FieldMask new_valid_mask;
                    for (LegionMap<ColorPoint,FieldMask>::aligned::
                          const_iterator cit = it->open_children.begin(); 
                          cit != it->open_children.end(); cit++)
                    {
#ifdef DEBUG_HIGH_LEVEL
                      assert(!!cit->second);
#endif
                      new_valid_mask |= cit->second;
                    }
                    // Update the valid mask on the field state, we'll
                    // check to see fi we need to delete it at the end
                    it->valid_fields = new_valid_mask;
                  }
                }
                else
                {
                  // Cases 3 and 4
                  FieldMask already_open;
                  perform_close_operations(closer, current_mask, *it, 
                                           next_child, 
                                           true/*allow next*/,
                                           true/*needs upgrade*/,
                                           false/*permit leave open*/,
                                           record_close_operations,
                                           new_states, already_open);
                  open_below |= already_open;
                  if (!!already_open)
                  {
                    // Create a new FieldState open in whatever mode is
                    // appropriate based on the usage
                    FieldState new_state(closer.user, already_open, next_child);
                    // Note if it is another reduction in the same child
                    if (IS_REDUCE(closer.user.usage))
                      new_state.open_state = OPEN_READ_WRITE;
                    new_states.push_back(new_state);
                  }
                }
              }
              else
              {
                // Closing everything up, so just do it
                FieldMask already_open;
                perform_close_operations(closer, current_mask, *it, next_child,
                                         false/*allow next*/,
                                         false/*needs upgrade*/,
                                         false/*permit leave open*/,
                                         record_close_operations,
                                         new_states, already_open);
                open_below |= already_open;
              }
              // Now see if the current field state is still valid
              if (!it->valid_fields)
                it = state.field_states.erase(it);
              else
                it++;
              break;
            }
          case OPEN_MULTI_REDUCE:
            {
              // See if this reduction is a reduction of the same kind
              if (IS_REDUCE(closer.user.usage) &&
                  (closer.user.usage.redop == it->redop))
              {
                if (next_child.is_valid())
                {
                  LegionMap<ColorPoint,FieldMask>::aligned::const_iterator
                    finder = it->open_children.find(next_child);
                  if (finder != it->open_children.end())
                  {
                    // Already open, so add the open fields
                    open_below |= (finder->second & current_mask);
                  }
                }
                it++;
              }
              else
              {
                // Need to close up the open field since we're going
                // to have to do it anyway
                FieldMask already_open;
                perform_close_operations(closer, current_mask, *it, next_child,
                                         false/*allow next child*/,
                                         false/*needs upgrade*/,
                                         false/*permit leave open*/,
                                         record_close_operations,
                                         new_states, already_open);
#ifdef DEBUG_HIGH_LEVEL
                assert(!already_open); // should all be closed now
#endif
                if (!it->valid_fields)
                  it = state.field_states.erase(it);
                else
                  it++;
              }
              break;
            }
          default:
            assert(false); // should never get here
        }
      }
      // If we had any fields that still need to be opened, create
      // a new field state and add it into the set of new states
      FieldMask open_mask = current_mask - open_below;
      if (next_child.is_valid() && !!open_mask)
        new_states.push_back(FieldState(closer.user, open_mask, next_child));
      merge_new_field_states(state, new_states);
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif 
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::perform_close_operations(LogicalCloser &closer,
                                            const FieldMask &closing_mask,
                                            FieldState &state,
                                            const ColorPoint &next_child, 
                                            bool allow_next_child,
                                            bool upgrade_next_child,
                                            bool permit_leave_open,
                                            bool record_close_operations,
                                   LegionDeque<FieldState>::aligned &new_states,
                                            FieldMask &already_open)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, PERFORM_LOGICAL_CLOSE_CALL);
#endif
      // First, if we have a next child and we know all pairs of children
      // are disjoint, then we can skip a lot of this
      bool removed_fields = false;
      if (next_child.is_valid() && are_all_children_disjoint())
      {
        // Check to see if we have anything to close
        LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
                              state.open_children.find(next_child);
        if (finder != state.open_children.end())
        {
          FieldMask close_mask = finder->second & closing_mask;
          if (!!close_mask)
          {
            if (allow_next_child)
            {
              already_open |= close_mask;
              if (upgrade_next_child)
              {
                finder->second -= close_mask;
                removed_fields = true;
                if (!finder->second)
                  state.open_children.erase(finder);
              }
            }
            else
            {
              // Otherwise we actually need to do the close
              RegionTreeNode *child_node = get_tree_child(finder->first);
              child_node->close_logical_node(closer, close_mask, 
                                             permit_leave_open);
              if (record_close_operations)
                closer.record_closed_child(finder->first, 
                                           close_mask, permit_leave_open);
              // Remove the closed fields
              finder->second -= close_mask;
              removed_fields = true;
              if (permit_leave_open)
              {
                new_states.push_back(FieldState(closer.user,
                                  close_mask, finder->first));
              }
              if (!finder->second)
                state.open_children.erase(finder);
            }
          }
          // Otherwise disjoint fields, nothing to do
        }
        // Otherwise it's closed so it doesn't matter
      }
      else
      {
        std::vector<ColorPoint> to_delete;
        // Go through and close all the children which we overlap with
        // and aren't the next child that we're going to use
        for (LegionMap<ColorPoint,FieldMask>::aligned::iterator it = 
              state.open_children.begin(); it != 
              state.open_children.end(); it++)
        {
          FieldMask close_mask = it->second & closing_mask;
          // check for field disjointness
          if (!close_mask)
            continue;
          // Check for same child, only allow upgrades in some cases
          // such as read-only -> exclusive.  This is calling context
          // sensitive hence the parameter.
          if (allow_next_child && next_child.is_valid() && 
              ((next_child) == it->first))
          {
            FieldMask open_fields = close_mask;
            already_open |= open_fields;
            if (upgrade_next_child)
            {
              it->second -= open_fields;
              removed_fields = true;
              if (!it->second)
                to_delete.push_back(it->first);
              // The upgraded field state gets added by the caller
            }
            continue;
          }
          // Check for child disjointness
          if (next_child.is_valid() && 
              are_children_disjoint(it->first, next_child))
            continue;
          // Perform the close operation
          RegionTreeNode *child_node = get_tree_child(it->first);
          child_node->close_logical_node(closer, close_mask, permit_leave_open);
          if (record_close_operations)
            closer.record_closed_child(it->first, 
                                       close_mask, permit_leave_open);
          // Remove the close fields
          it->second -= close_mask;
          removed_fields = true;
          if (!it->second)
            to_delete.push_back(it->first);
          // If we're allowed to leave this open, add a new
          // state for the current user
          if (permit_leave_open)
          {
            new_states.push_back(FieldState(closer.user,close_mask,it->first));
          }
        }
        // Remove the children that can be deleted
        for (std::vector<ColorPoint>::const_iterator it = to_delete.begin();
              it != to_delete.end(); it++)
        {
          state.open_children.erase(*it);
        }
      }
      // See if it is time to rebuild the valid mask 
      if (removed_fields)
      {
        if (state.rebuild_timeout == 0)
        {
          // Rebuild the valid fields mask
          FieldMask new_valid_mask;
          for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
                state.open_children.begin(); it != 
                state.open_children.end(); it++)
          {
            new_valid_mask |= it->second;
          }
          state.valid_fields = new_valid_mask;    
          // Reset the timeout to the order of the number of open children
          state.rebuild_timeout = state.open_children.size();
        }
        else
          state.rebuild_timeout--;
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::merge_new_field_state(LogicalState &state,
                                               const FieldState &new_state)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!!new_state.valid_fields);
#endif
      for (std::list<FieldState>::iterator it = state.field_states.begin();
            it != state.field_states.end(); it++)
      {
        if (it->overlaps(new_state))
        {
          it->merge(new_state, this);
          return;
        }
      }
      // Otherwise just push it on the back
      state.field_states.push_back(new_state);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::merge_new_field_states(LogicalState &state,
                             const LegionDeque<FieldState>::aligned &new_states)
    //--------------------------------------------------------------------------
    {
      for (unsigned idx = 0; idx < new_states.size(); idx++)
      {
        const FieldState &next = new_states[idx];
        merge_new_field_state(state, next);
      }
#ifdef DEBUG_HIGH_LEVEL
      sanity_check_logical_state(state);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::filter_prev_epoch_users(LogicalState &state,
                                                 const FieldMask &field_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FILTER_PREV_EPOCH_CALL);
#endif
      for (LegionList<LogicalUser,PREV_LOGICAL_ALLOC>::track_aligned::iterator 
            it = state.prev_epoch_users.begin(); it != 
            state.prev_epoch_users.end(); /*nothing*/)
      {
        it->field_mask -= field_mask;
        if (!it->field_mask)
        {
          // Remove the mapping reference
          it->op->remove_mapping_reference(it->gen);
          it = state.prev_epoch_users.erase(it); // empty so erase it
        }
        else
          it++; // still has non-dominated fields
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::filter_curr_epoch_users(LogicalState &state,
                                                 const FieldMask &field_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FILTER_CURR_EPOCH_CALL);
#endif
      for (LegionList<LogicalUser,CURR_LOGICAL_ALLOC>::track_aligned::iterator 
            it = state.curr_epoch_users.begin(); it !=
            state.curr_epoch_users.end(); /*nothing*/)
      {
        FieldMask local_dom = it->field_mask & field_mask;
        if (!!local_dom)
        {
          // Move a copy over to the previous epoch users for
          // the fields that were dominated
          state.prev_epoch_users.push_back(*it);
          state.prev_epoch_users.back().field_mask = local_dom;
          // Add a mapping reference
          it->op->add_mapping_reference(it->gen);
        }
        else
        {
          it++;
          continue;
        }
        // Update the field mask with the non-dominated fields
        it->field_mask -= field_mask;
        if (!it->field_mask)
        {
          // Remove the mapping reference
          it->op->remove_mapping_reference(it->gen);
          it = state.curr_epoch_users.erase(it); // empty so erase it
        }
        else
          it++; // not empty so keep going
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::record_version_numbers(LogicalState &state,
                                                const FieldMask &mask,
                                                VersionInfo &version_info,
                                                bool capture_previous,
                                                bool premap_only,
                                                bool path_only,
                                                bool needs_final,
                                                bool close_top)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      version_info.sanity_check(this);
#endif
      // Capture the version information for this logical region  
      VersionInfo::NodeInfo &node_info = version_info.find_tree_node_info(this);
      if (premap_only)
        node_info.set_premap_only();
      if (path_only)
        node_info.set_path_only();
      if (needs_final)
        node_info.set_needs_final();
      if (capture_previous)
        node_info.set_advance();
      if (close_top)
        node_info.set_close_top();
#ifdef DEBUG_HIGH_LEVEL
      FieldMask unversioned = mask;
#endif
      LegionMap<VersionID,FieldMask,
               VERSION_ID_ALLOC>::track_aligned &current = state.field_versions;
      if (capture_previous)
      {
        for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
              current.begin(); it != current.end(); it++)
        {
          FieldMask overlap = it->second & mask; 
          if (!overlap)
            continue;
#ifdef DEBUG_HIGH_LEVEL
          unversioned -= overlap;
          assert(it->first > 0);
#endif
          if (node_info.field_versions == NULL)
          {
            node_info.field_versions = new FieldVersions();
            node_info.field_versions->add_reference();
          }
          node_info.field_versions->add_field_version(it->first-1, overlap);
        }
      }
      else
      {
        for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
              current.begin(); it != current.end(); it++)
        {
          FieldMask overlap = it->second & mask; 
          if (!overlap)
            continue;
          if (node_info.field_versions == NULL)
          {
            node_info.field_versions = new FieldVersions();
            node_info.field_versions->add_reference();
          }
          node_info.field_versions->add_field_version(it->first, overlap);
#ifdef DEBUG_HIGH_LEVEL
          unversioned -= overlap;
#endif
        }
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(!unversioned); // all the fields should have a version number
      version_info.sanity_check(this);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::advance_version_numbers(LogicalState &state,
                                                 const FieldMask &mask)
    //--------------------------------------------------------------------------
    {  
      std::set<VersionID> to_delete;
      LegionMap<VersionID,FieldMask>::aligned to_add; 
      LegionMap<VersionID,FieldMask,
               VERSION_ID_ALLOC>::track_aligned &current = state.field_versions;
      // Do this in reverse order so we can forward in place if possible
      for (LegionMap<VersionID,FieldMask>::aligned::reverse_iterator it = 
            current.rbegin(); it != current.rend(); it++)
      {
        FieldMask overlap = it->second & mask;
        if (!overlap)
          continue;
        // We can't add or delete anything, but we can propagate updates 
        // Find the next version number
        VersionID next_version = it->first + 1;
        LegionMap<VersionID,FieldMask>::aligned::iterator finder = 
          current.find(next_version);
        if (finder != current.end())
        {
          finder->second |= overlap;
          // If we propagated remove that entry from the delete list
          to_delete.erase(next_version);
        }
        else
          to_add[next_version] = overlap;
        // Remove the overlap fields
        it->second -= overlap;
        if (!it->second)
          to_delete.insert(it->first);
      }
      // Remove any deleted version numbers
      for (std::set<VersionID>::const_iterator it = to_delete.begin();
            it != to_delete.end(); it++)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(!current[*it]); // Should be empty
#endif
        current.erase(*it);
      }
      for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
            to_add.begin(); it != to_add.end(); it++)
      {
#ifdef DEBUG_HIGH_LEVEL
        // The entry should not already exist
        assert(current.find(it->first) == current.end());
#endif
        current.insert(*it);
      }
    } 

    //--------------------------------------------------------------------------
    void RegionTreeNode::record_logical_reduction(LogicalState &state,
                                                  ReductionOpID redop,
                                                  const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      state.outstanding_reduction_fields |= user_mask;
      LegionMap<ReductionOpID,FieldMask>::aligned::iterator finder = 
        state.outstanding_reductions.find(redop);
      if (finder == state.outstanding_reductions.end())
        state.outstanding_reductions[redop] = user_mask;
      else
        finder->second |= user_mask;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::clear_logical_reduction_fields(LogicalState &state,
                                                  const FieldMask &cleared_mask)
    //--------------------------------------------------------------------------
    {
      state.outstanding_reduction_fields -= cleared_mask; 
      std::vector<ReductionOpID> to_delete;
      for (LegionMap<ReductionOpID,FieldMask>::aligned::iterator it = 
            state.outstanding_reductions.begin(); it !=
            state.outstanding_reductions.end(); it++)
      {
        it->second -= cleared_mask; 
        if (!it->second)
          to_delete.push_back(it->first);
      }
      for (std::vector<ReductionOpID>::const_iterator it = 
            to_delete.begin(); it != to_delete.end(); it++)
      {
        state.outstanding_reductions.erase(*it);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::sanity_check_logical_state(LogicalState &state)
    //--------------------------------------------------------------------------
    {
      // For every child and every field, it should only be open in one mode
      LegionMap<ColorPoint,FieldMask>::aligned previous_children;
      for (std::list<FieldState>::const_iterator fit = 
            state.field_states.begin(); fit != 
            state.field_states.end(); fit++)
      {
        FieldMask actually_valid;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              fit->open_children.begin(); it != 
              fit->open_children.end(); it++)
        {
          actually_valid |= it->second;
          if (previous_children.find(it->first) == previous_children.end())
          {
            previous_children[it->first] = it->second;
          }
          else
          {
            FieldMask &previous = previous_children[it->first];
            assert(!(previous & it->second));
            previous |= it->second;
          }
        }
        // Actually valid should be greater than or equal
        assert(!(actually_valid - fit->valid_fields));
      }
      // Also check that for each field it is either only open in one mode
      // or two children in different modes are disjoint
      for (std::list<FieldState>::const_iterator it1 = 
            state.field_states.begin(); it1 != 
            state.field_states.end(); it1++)
      {
        for (std::list<FieldState>::const_iterator it2 = 
              state.field_states.begin(); it2 != 
              state.field_states.end(); it2++)
        {
          // No need to do comparisons if they are the same field state
          if (it1 == it2) 
            continue;
          const FieldState &f1 = *it1;
          const FieldState &f2 = *it2;
          for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator 
                cit1 = f1.open_children.begin(); cit1 != 
                f1.open_children.end(); cit1++)
          {
            for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator 
                  cit2 = f2.open_children.begin(); 
                  cit2 != f2.open_children.end(); cit2++)
            {
              
              // Disjointness check on fields
              if (cit1->second * cit2->second)
                continue;
#ifndef NDEBUG
              ColorPoint c1 = cit1->first;
              ColorPoint c2 = cit2->first;
#endif
              // Some aliasing in the fields, so do the check 
              // for child disjointness
              assert(c1 != c2);
              assert(are_children_disjoint(c1, c2));
            }
          }
        }
      }
      // Make sure each field appears in exactly one version number
      FieldMask version_numbers;
      for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
            state.field_versions.begin(); it != 
            state.field_versions.end(); it++)
      {
        assert(version_numbers * it->second);
        version_numbers |= it->second;
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::initialize_logical_state(ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, INITIALIZE_LOGICAL_CALL);
#endif
      if (logical_states.has_entry(ctx))
      {
        LogicalState &state = get_logical_state(ctx);
#ifdef DEBUG_HIGH_LEVEL
        // Technically these should already be empty
        assert(state.field_states.empty());
        assert(state.curr_epoch_users.empty());
        assert(state.prev_epoch_users.empty());
#endif
        state.reset();
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::invalidate_logical_state(ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, INVALIDATE_LOGICAL_CALL);
#endif
      if (logical_states.has_entry(ctx))
      {
        LogicalState &state = get_logical_state(ctx);     
        for (LegionList<LogicalUser,CURR_LOGICAL_ALLOC>::track_aligned::
              const_iterator it = state.curr_epoch_users.begin(); it != 
              state.curr_epoch_users.end(); it++)
        {
          it->op->remove_mapping_reference(it->gen); 
        }
        for (LegionList<LogicalUser,PREV_LOGICAL_ALLOC>::track_aligned::
              const_iterator it = state.prev_epoch_users.begin(); it != 
              state.prev_epoch_users.end(); it++)
        {
          it->op->remove_mapping_reference(it->gen); 
        }
        state.reset();
      }
    }

    //--------------------------------------------------------------------------
    template<bool DOMINATE>
    void RegionTreeNode::register_logical_dependences(ContextID ctx, 
                                    Operation *op, const FieldMask &field_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REGISTER_LOGICAL_DEPS_CALL);
#endif
      LogicalState &state = get_logical_state(ctx);
      for (LegionList<LogicalUser,CURR_LOGICAL_ALLOC>::track_aligned::iterator 
            it = state.curr_epoch_users.begin(); it != 
            state.curr_epoch_users.end(); /*nothing*/)
      {
        if (!(it->field_mask * field_mask))
        {
#ifdef LEGION_LOGGING
          LegionLogging::log_mapping_dependence(
              Processor::get_executing_processor(),
              op->get_parent()->get_unique_task_id(),
              it->uid, it->idx, op->get_unique_op_id(),
              0/*idx*/, TRUE_DEPENDENCE);
#endif
#ifdef LEGION_SPY
          LegionSpy::log_mapping_dependence(
              op->get_parent()->get_unique_task_id(),
              it->uid, it->idx, op->get_unique_op_id(),
              0/*idx*/, TRUE_DEPENDENCE);
#endif
          // Do this after the logging since we 
          // are going to update the iterator
          if (op->register_dependence(it->op, it->gen))
          {
            // Prune it from the list
            it = state.curr_epoch_users.erase(it);
          }
          else if (DOMINATE)
            it = state.curr_epoch_users.erase(it);
          else
            it++;
        }
        else if (DOMINATE)
          it = state.curr_epoch_users.erase(it);
        else
          it++;
      }
      for (LegionList<LogicalUser,PREV_LOGICAL_ALLOC>::track_aligned::iterator 
            it = state.prev_epoch_users.begin(); it != 
            state.prev_epoch_users.end(); /*nothing*/)
      {
        if (!(it->field_mask * field_mask))
        {
#ifdef LEGION_LOGGING
          LegionLogging::log_mapping_dependence(
              Processor::get_executing_processor(),
              op->get_parent()->get_unique_task_id(),
              it->uid, it->idx, op->get_unique_op_id(),
              0/*idx*/, TRUE_DEPENDENCE);
#endif
#ifdef LEGION_SPY
          LegionSpy::log_mapping_dependence(
              op->get_parent()->get_unique_task_id(),
              it->uid, it->idx, op->get_unique_op_id(), 0/*idx*/, 
              TRUE_DEPENDENCE);
#endif
          // Do this after the logging since we are going
          // to update the iterator
          if (op->register_dependence(it->op, it->gen))
          {
            // Prune it from the list
            it = state.prev_epoch_users.erase(it);
          }
          else if (DOMINATE)
            it = state.prev_epoch_users.erase(it);
          else
            it++;
        }
        else if (DOMINATE)
          it = state.prev_epoch_users.erase(it);
        else
          it++;
      }
    } 

    //--------------------------------------------------------------------------
    void RegionTreeNode::add_restriction(ContextID ctx, 
                                         const FieldMask &restricted_mask)
    //--------------------------------------------------------------------------
    {
      LogicalState &state = get_logical_state(ctx);
      state.restricted_fields |= restricted_mask;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::release_restriction(ContextID ctx,
                                             const FieldMask &restricted_mask)
    //--------------------------------------------------------------------------
    {
      LogicalState &state = get_logical_state(ctx);
      state.restricted_fields -= restricted_mask;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::record_logical_restrictions(ContextID ctx,
                                                    RestrictInfo &restrict_info,
                                                    const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(is_region());
#endif
      LogicalState &state = get_logical_state(ctx);
      FieldMask restricted = mask & state.restricted_fields;
      if (!!restricted)
      {
        RegionNode *local_this = as_region_node();
        restrict_info.add_restriction(local_this->handle, restricted);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::close_physical_node(PhysicalCloser &closer,
                                             const FieldMask &closing_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, CLOSE_PHYSICAL_NODE_CALL);
#endif
      // Acquire the physical state of the node to close
      ContextID ctx = closer.info.ctx;
      // Figure out if we have dirty data.  If we do, issue copies back to
      // each of the target instances specified by the closer.  Note we
      // don't need to issue copies if the target view is already in
      // the list of currently valid views.  Then
      // perform the close operation on each of our open partitions that
      // interfere with the closing mask.
      // If there are any dirty fields we have to copy them back

      // Not we only need to do this operation for nodes which are
      // actually regions since they are the only ones that store
      // actual instances.
      
      LegionMap<LogicalView*,FieldMask>::aligned valid_instances;
      LegionMap<ReductionView*,FieldMask>::aligned valid_reductions;
      PhysicalState *state = 
        get_physical_state(ctx, closer.info.version_info);
      FieldMask dirty_fields = state->dirty_mask & closing_mask;
      FieldMask reduc_fields = state->reduction_mask & closing_mask;
      if (is_region())
      {
        if (!!dirty_fields)
        {
          // Pull down instance views so we don't issue unnecessary copies
          pull_valid_instance_views(ctx, state, closing_mask, 
                                false/*need space*/, closer.info.version_info);
#ifdef DEBUG_HIGH_LEVEL
          assert(!state->valid_views.empty());
#endif
          find_valid_instance_views(ctx, state, closing_mask, closing_mask, 
              closer.info.version_info, false/*needs space*/, valid_instances);
        }
        if (!!reduc_fields)
        {
          for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator
                it = state->reduction_views.begin(); it != 
                state->reduction_views.end(); it++)
          {
            FieldMask overlap = it->second & closing_mask;
            if (!!overlap)
              valid_reductions[it->first] = overlap;
          }
        }
      }
      if (is_region() && !!dirty_fields)
      {
        const std::vector<MaterializedView*> &targets = 
          closer.get_lower_targets();
        for (std::vector<MaterializedView*>::const_iterator it = 
              targets.begin(); it != targets.end(); it++)
        {
          LegionMap<LogicalView*,FieldMask>::aligned::const_iterator 
            finder = valid_instances.find(*it);
          // Check to see if it is already a valid instance for some fields
          if (finder == valid_instances.end())
          {
            issue_update_copies(closer.info, *it, 
                                dirty_fields, valid_instances, &closer);
          }
          else
          {
            // Only need to issue update copies for dirty fields for which
            // we are not currently valid
            FieldMask diff_fields = dirty_fields - finder->second;
            if (!!diff_fields)
              issue_update_copies(closer.info, *it, 
                                  diff_fields, valid_instances, &closer);
          }
        }
      }
      // Now we need to issue close operations for all our children
#ifdef DEBUG_HIGH_LEVEL
      assert(!closer.needs_targets());
#endif
      PhysicalCloser next_closer(closer);
      bool create_composite = false;
      std::set<ColorPoint> empty_next_children;
#ifdef DEBUG_HIGH_LEVEL
      bool result = 
#endif
      siphon_physical_children(next_closer, state, closing_mask,
                               empty_next_children, create_composite);
#ifdef DEBUG_HIGH_LEVEL
      assert(result); // should always succeed since targets already exist
      assert(!create_composite);
#endif
      // Update the closer's dirty mask
      const FieldMask &dirty_below = next_closer.get_dirty_mask();
      closer.update_dirty_mask(dirty_fields | reduc_fields | dirty_below);
      // Apply any reductions that we might have for the closing
      // fields back to the target instances
      // Again this only needs to be done for region nodes but
      // we should always invalidate reduction views
      if (is_region() && !!reduc_fields)
      {
        const std::vector<MaterializedView*> &targets = 
          closer.get_lower_targets();
        for (std::vector<MaterializedView*>::const_iterator it = 
              targets.begin(); it != targets.end(); it++)
        {
          issue_update_reductions(*it, reduc_fields, closer.info.version_info,
                                  closer.info.local_proc, 
                                  valid_reductions, closer.info.op, &closer);
        }
      }
      // If we are leaving this state open, we have to do some clean-up
      // so that it can remain valid, otherwise, if we're not leaving it
      // open then it doesn't matter anyway.
      if (closer.permit_leave_open)
      {
        if (!!dirty_below)
          invalidate_instance_views(state, dirty_below);
        state->dirty_mask -= closing_mask;
        if (!!reduc_fields)
          invalidate_reduction_views(state, reduc_fields); 
      }
    } 

    //--------------------------------------------------------------------------
    bool RegionTreeNode::select_close_targets(PhysicalCloser &closer,
                                              PhysicalState *state,
                                              const FieldMask &closing_mask,
                  const LegionMap<LogicalView*,FieldMask>::aligned &valid_views,
                  LegionMap<MaterializedView*,FieldMask>::aligned &update_views,
                                              bool &create_composite)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, SELECT_CLOSE_TARGETS_CALL);
#endif
      // First get the list of valid instances
      // Get the set of memories for which we have valid instances
      std::set<Memory> valid_memories;
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.begin(); it != valid_views.end(); it++)
      {
        if (it->first->is_deferred_view())
          continue;
#ifdef DEBUG_HIGH_LEVEL
        assert(it->first->as_instance_view()->is_materialized_view());
#endif
        MaterializedView *view = 
          it->first->as_instance_view()->as_materialized_view();
        valid_memories.insert(view->get_location());
      }
      // Now ask the mapper what it wants to do
      bool create_one;
      std::set<Memory> to_reuse;
      std::vector<Memory> to_create;
      size_t blocking_factor = 1;
      size_t max_blocking_factor = 
        context->get_domain_volume(closer.handle.index_space); 
      create_composite = context->runtime->invoke_mapper_rank_copy_targets(
                                               closer.info.local_proc, 
                                               closer.info.op->get_mappable(),
                                               closer.handle,
                                               valid_memories,
                                               true/*complete*/,
                                               max_blocking_factor,
                                               to_reuse,
                                               to_create,
                                               create_one,
                                               blocking_factor);
      // Filter out any re-use memories which are not in the list of
      // valid memories
      if (!to_reuse.empty())
      {
        std::vector<Memory> to_delete;
        for (std::set<Memory>::const_iterator it = to_reuse.begin();
              it != to_reuse.end(); it++)
        {
          if (valid_memories.find(*it) == valid_memories.end())
          {
            log_region.warning("WARNING: memory " IDFMT " was specified "
                                     "to be reused in rank_copy_targets "
                                     "when closing mappable operation ID %lld, "
                                     "but no instance exists in that memory."
                                     "Memory " IDFMT " is being added to the "
                                     "set of create memories.", it->id,
                       closer.info.op->get_mappable()->get_unique_mappable_id(),
                                     it->id);
            // Add it to the list of memories to try creating
            to_create.push_back(*it);
            to_delete.push_back(*it);
          }
        }
        if (!to_delete.empty())
        {
          for (std::vector<Memory>::const_iterator it = to_delete.begin();
                it != to_delete.end(); it++)
          {
            to_reuse.erase(*it);
          }
        }
      }
      // See if the mapper gave us reasonable output
      if (!create_composite && to_reuse.empty() && to_create.empty())
      {
        log_region.error("Invalid mapper output for rank_copy_targets "
                               "when closing mappable operation ID %lld. "
                               "Must specify at least one target memory in "
                               "'to_reuse' or 'to_create'.",
                     closer.info.op->get_mappable()->get_unique_mappable_id());
#ifdef DEBUG_HIGH_LEVEL
        assert(false);
#endif
        exit(ERROR_INVALID_MAPPER_OUTPUT);
      }
      if (create_composite)
      {
        // Return out early marking that we didn't make any instances
        return false;
      }
      else
      {
        // Now process the results
        // First see if we should re-use any instances
        for (std::set<Memory>::const_iterator mit = to_reuse.begin();
              mit != to_reuse.end(); mit++)
        {
          // Make sure it is a valid choice
          if (valid_memories.find(*mit) == valid_memories.end())
            continue;
          MaterializedView *best = NULL;
          FieldMask best_mask;
          int num_valid_fields = -1;
          for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
                valid_views.begin(); it != valid_views.end(); it++)
          {
            if (it->first->is_deferred_view())
              continue;
#ifdef DEBUG_HIGH_LEVEL
            assert(it->first->as_instance_view()->is_materialized_view());
#endif
            MaterializedView *current = 
              it->first->as_instance_view()->as_materialized_view();
            if (current->get_location() != (*mit))
              continue;
            int valid_fields = FieldMask::pop_count(it->second);
            if (valid_fields > num_valid_fields)
            {
              num_valid_fields = valid_fields;
              best = current;
              best_mask = it->second;
            }
          }
          if (best != NULL)
          {
            FieldMask need_update = closing_mask - best_mask;
            if (!!need_update)
              update_views[best] = need_update;
            // Add it to the list of close targets
            closer.add_target(best);
          }
        }
        // Now see if want to try to create any new instances
        for (unsigned idx = 0; idx < to_create.size(); idx++)
        {
          // Try making an instance in memory
          MaterializedView *new_view = 
            create_instance(to_create[idx], 
                            closer.info.req.privilege_fields, 
                            blocking_factor,
                            closer.info.op->get_mappable()->get_depth(),
                            closer.info.op);
          if (new_view != NULL)
          {
            // Update all the fields
            update_views[new_view] = closing_mask;
            closer.add_target(new_view);
            // Make sure to tell our state we created a new instance
            state->record_created_instance(new_view);
            // If we only needed to make one, then we are done
            if (create_one)
              break;
          }
        }
      }
      // Check to see if have targets
      return (!closer.needs_targets());
    }

    //--------------------------------------------------------------------------
    bool RegionTreeNode::siphon_physical_children(PhysicalCloser &closer,
                                              PhysicalState *state,
                                              const FieldMask &closing_mask,
                                      const std::set<ColorPoint> &next_children,
                                              bool &create_composite)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, SIPHON_PHYSICAL_CHILDREN_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      // First check, if all the fields are disjoint, then we're done
      if (state->children.valid_fields * closing_mask)
        return true;
      // Otherwise go through all of the children and 
      // see which ones we need to clean up
      bool changed = false;
      std::vector<ColorPoint> to_delete;
      for (LegionMap<ColorPoint,FieldMask>::aligned::iterator 
            it = state->children.open_children.begin(); 
            it != state->children.open_children.end(); it++)
      {
        if (!close_physical_child(closer, state, closing_mask, it->first,
                   it->second, next_children, create_composite, changed))
          return false;
        if (!it->second)
          to_delete.push_back(it->first);
      }
      // Delete any empty children
      if (!to_delete.empty())
      {
        for (std::vector<ColorPoint>::const_iterator it = to_delete.begin();
              it != to_delete.end(); it++)
        {
          state->children.open_children.erase(*it);
        }
      }
      // Rebuild the valid mask if anything changed
      if (changed)
      {
        FieldMask next_valid;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
              state->children.open_children.begin(); it !=
              state->children.open_children.end(); it++)
        {
          next_valid |= it->second;
        }
        state->children.valid_fields = next_valid;
      }
      return true;
    }

    //--------------------------------------------------------------------------
    bool RegionTreeNode::close_physical_child(PhysicalCloser &closer,
                                              PhysicalState *state,
                                              const FieldMask &closing_mask,
                                              const ColorPoint &target_child,
                                              FieldMask &child_mask,
                                      const std::set<ColorPoint> &next_children,
                                          bool &create_composite, bool &changed)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, CLOSE_PHYSICAL_CHILD_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      FieldMask close_mask = child_mask & closing_mask;
      // Check field disjointness
      if (!close_mask)
        return true;
      // Check for child disjointness
      if (!next_children.empty())
      {
        bool all_disjoint = true;
        for (std::set<ColorPoint>::const_iterator it = 
              next_children.begin(); it != next_children.end(); it++)
        {
          if (!are_children_disjoint(target_child, *it))
          {
            all_disjoint = false;
            break;
          }
        }
        if (all_disjoint)
          return true;
      }
      // First check to see if the closer needs to make physical
      // instance targets in order to perform the close operation
      LegionMap<LogicalView*,FieldMask>::aligned space_views;
      LegionMap<LogicalView*,FieldMask>::aligned valid_views;
      LegionMap<MaterializedView*,FieldMask>::aligned update_views;
      if (closer.needs_targets())
      {
        // Have the closer make targets and return false indicating
        // we could not successfully perform the close operation
        // if he fails to make them. When making close targets pick
        // them for the full traversal mask so that other close
        // operations can reuse the same physical instances.
        find_valid_instance_views(closer.info.ctx, state, 
                                  closer.info.traversal_mask, 
                                  closer.info.traversal_mask,
                                  closer.info.version_info,
                                  true/*needs space*/, space_views);
        // This doesn't matter so always mark it false for now
        if (!select_close_targets(closer, state, closer.info.traversal_mask, 
                          space_views, update_views, create_composite))
        {
          // We failed to close, time to return
          return false;
        }
        else
        {
          // We succeeded, so get the set of valid views
          // for issuing update copies
          find_valid_instance_views(closer.info.ctx, state, 
                                    closer.info.traversal_mask,
                                    closer.info.traversal_mask,
                                    closer.info.version_info,
                                    false/*needs space*/, valid_views);
        }
      }
      // Need to get this value before the iterator is invalidated
      RegionTreeNode *child_node = get_tree_child(target_child);
      if (!update_views.empty())
      {
        // Issue any update copies, and then release any
        // valid view references that we are holding
        for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator 
              it = update_views.begin(); it != update_views.end(); it++)
        {
          issue_update_copies(closer.info, it->first, 
                              it->second, valid_views, &closer);
        }
        update_views.clear();
      }
      // Now we're ready to perform the close operation
      closer.close_tree_node(child_node, close_mask);
      // Update the child field mask and mark that we changed something
      child_mask -= close_mask;
      changed = true;
      // Reacquire our lock on the state upon returning
      return true;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::create_composite_instance(ContextID ctx_id,
                                            const std::set<ColorPoint> &targets,
                                               bool leave_open, 
                                      const std::set<ColorPoint> &next_children,
                                               const FieldMask &closing_mask,
                                               VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef LEGION_SPY
      log_run.error("Unfortunately Legion Spy doesn't support composite "
                    "instance analysis yet");
      assert(false); // TODO: Teach Legion Spy to analyze composite instances
#endif
      PhysicalState *state = get_physical_state(ctx_id, version_info);
      FieldMask dirty_mask, complete_mask; 
      const bool capture_children = !is_region();
      LegionMap<ColorPoint,FieldMask>::aligned complete_children;
      CompositeCloser closer(ctx_id, version_info, leave_open);
      CompositeNode *root = closer.get_composite_node(this, NULL/*parent*/);
      for (std::set<ColorPoint>::const_iterator it = targets.begin(); 
            it != targets.end(); it++)
      {
        FieldMask child_complete;
        close_physical_child(closer, root, state, 
                             closing_mask, (*it), 
                             next_children, dirty_mask, child_complete);
        if (!child_complete)
          continue;
        if (capture_children)
        {
          LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
            complete_children.find(*it);
          if (finder == complete_children.end())
            complete_children[(*it)] = child_complete;
          else
            finder->second |= child_complete;
        }
        else
          complete_mask |= child_complete;
      }
      FieldMask capture_mask = dirty_mask;
      // If we are a region, then we can avoid closing any fields
      // for which we know we had complete children
      if (!capture_children)
      {
        // This is a region so we can avoid capturing data for
        // any fields which have complete children
        capture_mask -= complete_mask;
      }
      else if ((complete_children.size() == get_num_children()) &&
                is_complete())
      {
        // Otherwise, this is a partition, so if we closed all
        // the children and this is a complete partition check to
        // see which fields we closed all the children so we can
        // count them as complete
        FieldMask complete_mask = capture_mask;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator
              it = complete_children.begin(); it != 
              complete_children.end(); it++)
        {
          complete_mask &= it->second;
        }
        if (!!complete_mask)
          capture_mask -= complete_mask;
      }
      // Update the root node with the appropriate information
      // and then create a composite view
      closer.capture_physical_state(root, this, state, 
                                    capture_mask, dirty_mask);
      closer.create_valid_view(state, root, dirty_mask);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::close_physical_node(CompositeCloser &closer,
                                             CompositeNode *node,
                                             const FieldMask &closing_mask,
                                             FieldMask &dirty_mask,
                                             FieldMask &complete_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, CLOSE_PHYSICAL_NODE_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(node->logical_node == this);
#endif
      // Tell the node to update its parent
      node->update_parent_info(closing_mask);
      // Acquire the state and save any pertinent information in the node
      PhysicalState *state = get_physical_state(closer.ctx,closer.version_info);
      // First close up any children below and see if we are flushing
      // back any dirty data which is complete
      FieldMask dirty_below, complete_below;
      siphon_physical_children(closer, node, state, closing_mask, 
                               dirty_below, complete_below);
      // Only capture state here for things in our close mask which we
      // don't have complete data for below.
      FieldMask capture_mask = closing_mask - complete_below;
      if (!!capture_mask)
        closer.capture_physical_state(node, this, state, 
                                      capture_mask, dirty_mask);
      dirty_mask |= dirty_below;
      complete_mask |= complete_below;
      // If we are leaving this state open, we have to do some clean-up
      // so that it can remain valid, otherwise, if we're not leaving it
      // open then it doesn't matter anyway.
      if (closer.permit_leave_open)
      {
        if (!!dirty_below)
          invalidate_instance_views(state, dirty_below);
        state->dirty_mask -= closing_mask;
        FieldMask reduc_fields = state->reduction_mask & closing_mask;
        if (!!reduc_fields)
          invalidate_reduction_views(state, reduc_fields); 
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::siphon_physical_children(CompositeCloser &closer,
                                                  CompositeNode *node,
                                                  PhysicalState *state,
                                                  const FieldMask &closing_mask,
                                                  FieldMask &dirty_mask,
                                                  FieldMask &complete_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, SIPHON_PHYSICAL_CHILDREN_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
      assert(node->logical_node == this);
#endif
      // First check, if all the fields are disjoint, then we're done
      if (state->children.valid_fields * closing_mask)
        return;
      // Keep track of two sets of fields
      // 1. The set of fields for which all children are complete
      // 2. The set of fields for which any children are complete
      // Optionally in the future we can make this more precise to 
      // track when we've written to enough children to dominate
      // this node and therefore be complete
      FieldMask all_children = closing_mask;
      FieldMask any_children;
      bool changed = false;
      std::vector<ColorPoint> to_delete;
      // Otherwise go through all of the children and 
      // see which ones we need to clean up
      for (LegionMap<ColorPoint,FieldMask>::aligned::iterator it = 
            state->children.open_children.begin(); it !=
            state->children.open_children.end(); it++)
      {
        FieldMask overlap = it->second & closing_mask;
        all_children &= overlap;
        if (!overlap)
          continue;
        FieldMask child_complete;
        std::set<ColorPoint> empty_next_children;
        close_physical_child(closer, node, state, overlap, 
                             it->first, empty_next_children, 
                             dirty_mask, child_complete);
        all_children &= child_complete;
        any_children |= child_complete;
        changed = true;
        it->second -= overlap;
        if (!it->second)
          to_delete.push_back(it->first);
      }
      // If this is a region, if any of our children were complete
      // then we can also be considered complete
      if (is_region())
        complete_mask |= any_children;
      // Otherwise, if this is a partition, we have complete data
      // if all children were complete, we are complete, and 
      // we touched all of the children
      else if (!!all_children && is_complete() &&
               (state->children.open_children.size() == get_num_children()))
        complete_mask |= all_children;
      // Delete any closed children
      if (!to_delete.empty())
      {
        for (std::vector<ColorPoint>::const_iterator it = to_delete.begin();
              it != to_delete.end(); it++)
        {
          state->children.open_children.erase(*it);
        }
      }
      // Rebuild the valid mask if anything changed
      if (changed)
      {
        FieldMask next_valid;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              state->children.open_children.begin(); it !=
              state->children.open_children.end(); it++)
        {
          next_valid |= it->second;
        }
        state->children.valid_fields = next_valid;
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::close_physical_child(CompositeCloser &closer,
                                              CompositeNode *node,
                                              PhysicalState *state,
                                              const FieldMask &closing_mask,
                                              const ColorPoint &target_child,
                                      const std::set<ColorPoint> &next_children,
                                              FieldMask &dirty_mask,
                                              FieldMask &complete_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, CLOSE_PHYSICAL_CHILD_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
      assert(node->logical_node == this);
#endif
      // See if we can find the child
      LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
        state->children.open_children.find(target_child);
      if (finder == state->children.open_children.end())
        return;
      // Check field disjointness
      if (finder->second * closing_mask)
        return;
      // Check for child disjointness
      if (!next_children.empty())
      {
        bool all_disjoint = true;
        for (std::set<ColorPoint>::const_iterator it = 
              next_children.begin(); it != next_children.end(); it++)
        {
          if (!are_children_disjoint(finder->first, *it))
          {
            all_disjoint = false;
            break;
          }
        }
        if (all_disjoint)
          return;
      }
      FieldMask close_mask = finder->second & closing_mask;
      // Need to get this value before the iterator is invalidated
      RegionTreeNode *child_node = get_tree_child(finder->first);
      CompositeNode *child_composite = 
                                closer.get_composite_node(child_node, node);
      child_node->close_physical_node(closer, child_composite, 
                                      close_mask, dirty_mask, complete_mask);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::open_physical_child(ContextID ctx_id,
                                             const ColorPoint &child_color,
                                             const FieldMask &open_mask,
                                             VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
      PhysicalState *state = get_physical_state(ctx_id, version_info);
      state->children.valid_fields |= open_mask;
      LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
                          state->children.open_children.find(child_color);
      if (finder == state->children.open_children.end())
        state->children.open_children[child_color] = open_mask;
      else
        finder->second |= open_mask;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::siphon_physical_children(ReductionCloser &closer,
                                                  PhysicalState *state)
    //--------------------------------------------------------------------------
    {
      if (state->children.valid_fields * closer.close_mask)
        return;
      for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
            state->children.open_children.begin(); it !=
            state->children.open_children.end(); it++)
      {
        if (closer.close_mask * it->second)
          continue;
        RegionTreeNode *child = get_tree_child(it->first);
        child->close_physical_node(closer);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::close_physical_node(ReductionCloser &closer)
    //--------------------------------------------------------------------------
    {
      PhysicalState *state = get_physical_state(closer.ctx,closer.version_info);
      closer.issue_close_reductions(this, state);
      siphon_physical_children(closer, state);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::find_valid_instance_views(ContextID ctx,
                                                   PhysicalState *state,
                                                   const FieldMask &valid_mask,
                                                   const FieldMask &space_mask,
                                                   VersionInfo &version_info,
                                                   bool needs_space,
                        LegionMap<LogicalView*,FieldMask>::aligned &valid_views)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FIND_VALID_INSTANCE_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      if (!version_info.is_upper_bound_node(this))
      {
        RegionTreeNode *parent = get_parent();
        if (parent != NULL)
        {
          FieldMask up_mask = valid_mask - state->dirty_mask;
          if (!!up_mask || needs_space)
          {
            PhysicalState *parent_state = 
              parent->get_physical_state(ctx, version_info);
            LegionMap<LogicalView*,FieldMask>::aligned local_valid;
            parent->find_valid_instance_views(ctx, parent_state, up_mask, 
                      space_mask, version_info, needs_space, local_valid);
            // Get the subview for this level
            for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator 
                  it = local_valid.begin(); it != local_valid.end(); it++)
            {
              LogicalView *local_view = it->first->get_subview(get_color());
              valid_views[local_view] = it->second;
            }
          }
        }
      }
      // Now figure out which of our valid views we can add
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            state->valid_views.begin(); it != state->valid_views.end(); it++)
      {
        // If we need the physical instances to be at least as big as the
        // needed fields, check that first
        if (needs_space)
        {
          if (it->first->is_deferred_view())
            continue;
#ifdef DEBUG_HIGH_LEVEL
          assert(it->first->as_instance_view()->is_materialized_view());
#endif
          MaterializedView *current = 
            it->first->as_instance_view()->as_materialized_view();
          if (!!(space_mask - current->get_physical_mask()))
            continue;
        }
        // If we're looking for instances with space, we want the instances
        // even if they have no valid fields, otherwise if we're not looking
        // for instances with enough space, we can exit out early if they
        // don't have any valid fields
        FieldMask overlap = valid_mask & it->second;
        if (!needs_space && !overlap)
          continue;
        // Check to see if we need to merge the field views.
        LegionMap<LogicalView*,FieldMask>::aligned::iterator finder = 
          valid_views.find(it->first);
        if (finder == valid_views.end())
          valid_views[it->first] = overlap;
        else
          finder->second |= overlap;
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::find_valid_reduction_views(ContextID ctx,
                                                    PhysicalState *state,
                                                    ReductionOpID redop,
                                                    const FieldMask &valid_mask,
                                                    VersionInfo &version_info,
                                          std::set<ReductionView*> &valid_views)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FIND_VALID_REDUCTION_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      if (!version_info.is_upper_bound_node(this))
      {
        RegionTreeNode *parent = get_parent();
        if (parent != NULL)
        {
          // See if we can continue going up the tree
          FieldMask up_mask = valid_mask - state->dirty_mask;
          if (!!up_mask)
          {
            // Acquire the parent state in non-exclusive mode
            PhysicalState *parent_state = 
              parent->get_physical_state(ctx, version_info);
            parent->find_valid_reduction_views(ctx, parent_state, redop, 
                                    up_mask, version_info, valid_views);
          }
        }
      }
      for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it = 
            state->reduction_views.begin(); it != 
            state->reduction_views.end(); it++)
      {
        if (it->first->get_redop() != redop)
          continue;
        FieldMask uncovered = valid_mask - it->second;
        if (!uncovered && (valid_views.find(it->first) == valid_views.end()))
        {
          valid_views.insert(it->first);
        }
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::pull_valid_instance_views(ContextID ctx,
                                                   PhysicalState *state,
                                                   const FieldMask &mask,
                                                   bool needs_space,
                                                   VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, PULL_VALID_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      LegionMap<LogicalView*,FieldMask>::aligned new_valid_views;
      find_valid_instance_views(ctx, state, mask, mask, version_info,
                                needs_space, new_valid_views);
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            new_valid_views.begin(); it != new_valid_views.end(); it++)
      {
        update_valid_views(state, it->second, false/*dirty*/, it->first);
      }
    }
    
    //--------------------------------------------------------------------------
    void RegionTreeNode::find_copy_across_instances(const MappableInfo &info,
                                                    MaterializedView *target,
                 LegionMap<MaterializedView*,FieldMask>::aligned &src_instances,
                LegionMap<DeferredView*,FieldMask>::aligned &deferred_instances)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FIND_COPY_ACROSS_INSTANCES_CALL);
#endif
      LegionMap<LogicalView*,FieldMask>::aligned valid_views;
      PhysicalState *state = get_physical_state(info.ctx, info.version_info); 
      find_valid_instance_views(info.ctx, state, info.traversal_mask,
                                info.traversal_mask, info.version_info,
                                false/*needs space*/, valid_views);
      // Now tease them apart into src and composite views and sort
      // them based on the target memory
      FieldMask copy_mask = info.traversal_mask;
      sort_copy_instances(info, target, copy_mask, valid_views,
                          src_instances, deferred_instances);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::issue_update_copies(const MappableInfo &info,
                                             MaterializedView *dst,
                                             FieldMask copy_mask,
             const LegionMap<LogicalView*,FieldMask>::aligned &valid_instances,
                                             CopyTracker *tracker /*= NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ISSUE_UPDATE_COPIES_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(!!copy_mask);
      assert(dst->logical_node == this);
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_instances.begin(); it != valid_instances.end(); it++)
      {
        assert(it->first->logical_node == this);
      }
#endif
      // Quick check to see if we are done early
      {
        LegionMap<LogicalView*,FieldMask>::aligned::const_iterator finder = 
          valid_instances.find(dst);
        if ((finder != valid_instances.end()) &&
            !(copy_mask - finder->second))
          return;
      }

      // To facilitate optimized copies in the low-level runtime, 
      // we gather all the information needed to issue gather copies 
      // from multiple instances into the data structures below, we then 
      // issue the copy when we're done and update the destination instance.
      LegionMap<LogicalView*,FieldMask>::aligned copy_instances = 
                                                            valid_instances;
      LegionMap<MaterializedView*,FieldMask>::aligned src_instances;
      LegionMap<DeferredView*,FieldMask>::aligned deferred_instances;
      // This call destroys copy_instances and also updates copy_mask
      sort_copy_instances(info, dst, copy_mask, copy_instances, 
                          src_instances, deferred_instances);

      // Now we can issue the copy operation to the low-level runtime
      if (!src_instances.empty())
      {
        // Get all the preconditions for each of the different instances
        LegionMap<Event,FieldMask>::aligned preconditions;
        FieldMask update_mask; 
        for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator 
              it = src_instances.begin(); it != src_instances.end(); it++)
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(!!it->second);
#endif
          it->first->find_copy_preconditions(0/*redop*/, true/*reading*/,
                                             it->second, info.version_info,
                                             preconditions);
          update_mask |= it->second;
        }
        // Now do the destination
        dst->find_copy_preconditions(0/*redop*/, false/*reading*/,
                                     update_mask, info.version_info,
                                     preconditions);

        LegionMap<Event,FieldMask>::aligned postconditions;
        if (!has_component_domains())
        {
          Event domain_pre;
          std::set<Domain> copy_domain;
          copy_domain.insert(get_domain(domain_pre));
          issue_grouped_copies(context, info, dst, preconditions, update_mask,
                               domain_pre, copy_domain, src_instances, 
                               info.version_info, postconditions, tracker);
        }
        else
        {
          Event domain_pre;
          const std::set<Domain> &component_domains = 
                                            get_component_domains(domain_pre);
          issue_grouped_copies(context, info, dst, preconditions, update_mask,
                               domain_pre, component_domains, src_instances,
                               info.version_info, postconditions, tracker);
        }

        // Tell the destination about all of the copies that were done
        for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
              postconditions.begin(); it != postconditions.end(); it++)
        {
          dst->add_copy_user(0/*redop*/, it->first, info.version_info, 
                             it->second, false/*reading*/);
        }
      }
      // If we still have fields that need to be updated and there
      // are composite instances then we need to issue updates copies
      // for those fields from the composite instances
      if (!deferred_instances.empty())
      {
        for (LegionMap<DeferredView*,FieldMask>::aligned::const_iterator it =
              deferred_instances.begin(); it !=
              deferred_instances.end(); it++)
        {
          it->first->issue_deferred_copies(info, dst, it->second, tracker);
        }
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::sort_copy_instances(const MappableInfo &info,
                                             MaterializedView *dst,
                                             FieldMask &copy_mask,
                     LegionMap<LogicalView*,FieldMask>::aligned &copy_instances,
                 LegionMap<MaterializedView*,FieldMask>::aligned &src_instances,
                LegionMap<DeferredView*,FieldMask>::aligned &deferred_instances)
    //--------------------------------------------------------------------------
    {
      // No need to call the mapper if there is only one valid instance
      if (copy_instances.size() == 1)
      {
        const std::pair<LogicalView*,FieldMask> &src_info = 
          *(copy_instances.begin());
        FieldMask op_mask = copy_mask & src_info.second;
        if (!!op_mask)
        {
          LogicalView *src = src_info.first;
          // No need to do anything if src and destination are the same
          // Also check for the same instance which can occur in the case
          // of deferred instances
          if (src != dst)
          {
            if (src->is_deferred_view())
            {
              DeferredView *current = src->as_deferred_view();
              deferred_instances[current] = op_mask;
            }
            else
            {
#ifdef DEBUG_HIGH_LEVEL
              assert(src->is_instance_view());
              assert(src->as_instance_view()->is_materialized_view());
#endif
              MaterializedView *current = 
                src->as_instance_view()->as_materialized_view();
              // If they are the same instance (which can happen with
              // composite views) then we don't need to do it
              if (current->manager->get_instance() != 
                  dst->manager->get_instance())
                src_instances[current] = op_mask;
            }
          }
        }
      }
      else if (!copy_instances.empty())
      {
        bool copy_ready = false;
        // Ask the mapper to put everything in order
        std::set<Memory> available_memories;
        LegionMap<DeferredView*,FieldMask>::aligned available_deferred;
        for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
              copy_instances.begin(); it != copy_instances.end(); it++)
        {
          if (it->first->is_deferred_view())
          {
            DeferredView *current = it->first->as_deferred_view();
            LegionMap<DeferredView*,FieldMask>::aligned::iterator finder = 
              available_deferred.find(current);
            if (finder == available_deferred.end())
              available_deferred[current] = it->second;
            else
              finder->second |= it->second;
          }
          else
          {
            available_memories.insert(
                it->first->as_instance_view()->get_location());
          }
        }
        std::vector<Memory> chosen_order;
        context->runtime->invoke_mapper_rank_copy_sources(info.local_proc,
                                                        info.op->get_mappable(),
                                                          available_memories,
                                                          dst->get_location(),
                                                          chosen_order);
        for (std::vector<Memory>::const_iterator mit = chosen_order.begin();
              !copy_ready && (mit != chosen_order.end()); mit++)
        {
          available_memories.erase(*mit);
          std::vector<LogicalView*> to_erase;
          // Go through all the valid instances and issue copies
          // from instances in the given memory
          for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
                copy_instances.begin(); it != copy_instances.end(); it++)
          {
            if (it->first->is_deferred_view())
              continue;
#ifdef DEBUG_HIGH_LEVEL
            assert(it->first->is_instance_view());
            assert(it->first->as_instance_view()->as_materialized_view());
#endif
            MaterializedView *current_view = 
              it->first->as_instance_view()->as_materialized_view();
            if ((*mit) != current_view->get_location())
              continue;
            // Check to see if there are any valid fields in the copy mask
            FieldMask op_mask = copy_mask & it->second;
            if (!!op_mask)
            {
              // No need to do anything if they are the same instance
              if ((dst != current_view) || (dst->manager->get_instance() != 
                                    current_view->manager->get_instance()))
              {
                LegionMap<MaterializedView*,FieldMask>::aligned::iterator 
                  finder = src_instances.find(current_view);
                if (finder == src_instances.end())
                  src_instances[current_view] = op_mask;
                else
                  finder->second |= op_mask;
              }
              // Update the copy mask
              copy_mask -= op_mask;
              if (!copy_mask)
              {
                copy_ready = true;
                break;
              }
            }
            to_erase.push_back(it->first);
          }
          // Erase any instances we considered and used
          for (unsigned idx = 0; idx < to_erase.size(); idx++)
            copy_instances.erase(to_erase[idx]);
        }
        // Now do any remaining memories not put in order by the mapper
        for (std::set<Memory>::const_iterator mit = 
              available_memories.begin(); !copy_ready && (mit != 
              available_memories.end()); mit++)
        {
          std::vector<LogicalView*> to_erase;
          for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
                copy_instances.begin(); it != copy_instances.end(); it++)
          {
            if (it->first->is_deferred_view())
              continue;
#ifdef DEBUG_HIGH_LEVEL
            assert(it->first->is_instance_view());
            assert(it->first->as_instance_view()->is_materialized_view());
#endif
            MaterializedView *current_view = 
              it->first->as_instance_view()->as_materialized_view();
            if ((*mit) != current_view->get_location())
              continue;
            // Check to see if there are any valid fields in the copy mask
            FieldMask op_mask = copy_mask & it->second;
            if (!!op_mask)
            {
              // No need to do anything if they are the same instance
              if ((dst != current_view) || (dst->manager->get_instance() != 
                                     current_view->manager->get_instance()))
              {
                LegionMap<MaterializedView*,FieldMask>::aligned::iterator 
                  finder = src_instances.find(current_view);
                if (finder == src_instances.end())
                  src_instances[current_view] = op_mask;
                else
                  finder->second |= op_mask;
              }
              // Update the copy mask
              copy_mask -= op_mask;
              if (!copy_mask)
              {
                copy_ready = true;
                break;
              }
            }
            to_erase.push_back(it->first);
          }
          // Erase any instances we've checked
          for (unsigned idx = 0; idx < to_erase.size(); idx++)
            copy_instances.erase(to_erase[idx]);
        }
        // Lastly, if we are still not done, see if we have
        // any composite instances to issue copies from
        for (LegionMap<DeferredView*,FieldMask>::aligned::const_iterator cit =
              available_deferred.begin(); !copy_ready && (cit !=
              available_deferred.end()); cit++)
        {
          FieldMask op_mask = copy_mask & cit->second;
          if (!!op_mask)
          {
            // No need to look for duplicates, we know this is
            // the first time this data structure can be touched
            deferred_instances[cit->first] = op_mask;
            copy_mask -= op_mask;
            if (!copy_mask)
            {
              copy_ready = true;
              break;
            }
          }
        }
      }
      // Otherwise all the fields have no current data so they are
      // by defintiion up to date
    }

    //--------------------------------------------------------------------------
    /*static*/ void RegionTreeNode::issue_grouped_copies(
                                                      RegionTreeForest *context,
                                                      const MappableInfo &info,
                                                         MaterializedView *dst,
                             LegionMap<Event,FieldMask>::aligned &preconditions,
                                       const FieldMask &update_mask,
                                       Event copy_domains_precondition,
                                       const std::set<Domain> &copy_domains,
           const LegionMap<MaterializedView*,FieldMask>::aligned &src_instances,
                                       const VersionInfo &src_version_info,
                            LegionMap<Event,FieldMask>::aligned &postconditions,
                                                CopyTracker *tracker /*= NULL*/)
    //--------------------------------------------------------------------------
    {
      // Now let's build maximal sets of fields which have
      // identical event preconditions. Use a list so our
      // iterators remain valid under insertion and push back
      LegionList<EventSet>::aligned precondition_sets;
      compute_event_sets(update_mask, preconditions, precondition_sets);
      // Now that we have our precondition sets, it's time
      // to issue the distinct copies to the low-level runtime
      // Issue a copy for each of the different precondition sets
      for (LegionList<EventSet>::aligned::iterator pit = 
            precondition_sets.begin(); pit != 
            precondition_sets.end(); pit++)
      {
        EventSet &pre_set = *pit;
        // Build the src and dst fields vectors
        std::vector<Domain::CopySrcDstField> src_fields;
        std::vector<Domain::CopySrcDstField> dst_fields;
        LegionMap<MaterializedView*,FieldMask>::aligned update_views;
        for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator 
              it = src_instances.begin(); it != src_instances.end(); it++)
        {
          FieldMask op_mask = pre_set.set_mask & it->second;
          if (!!op_mask)
          {
            it->first->copy_from(op_mask, src_fields);
            dst->copy_to(op_mask, dst_fields);
            update_views[it->first] = op_mask;
          }
        }
#ifdef DEBUG_HIGH_LEVEL
        assert(!src_fields.empty());
        assert(!dst_fields.empty());
        assert(src_fields.size() == dst_fields.size());
#endif
        // Add the copy domain precondition if it exists
        if (copy_domains_precondition.exists())
          pre_set.preconditions.insert(copy_domains_precondition);
        // Now that we've got our offsets ready, we
        // can now issue the copy to the low-level runtime
        Event copy_pre = Event::merge_events(pre_set.preconditions);
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        if (!copy_pre.exists())
        {
          UserEvent new_copy_pre = UserEvent::create_user_event();
          new_copy_pre.trigger();
          copy_pre = new_copy_pre;
        }
#endif
#ifdef LEGION_LOGGING
        LegionLogging::log_event_dependences(
            Processor::get_executing_processor(), 
            pre_set.preconditions, copy_pre);
#endif
#ifdef LEGION_SPY
        LegionSpy::log_event_dependences(pre_set.preconditions, copy_pre);
#endif
        std::set<Event> post_events;
        for (std::set<Domain>::const_iterator it = copy_domains.begin();
              it != copy_domains.end(); it++)
        {
          post_events.insert(context->issue_copy(*it, info.op, src_fields,
                                                 dst_fields, copy_pre));
        }
        Event copy_post = Event::merge_events(post_events);
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        if (!copy_post.exists())
        {
          UserEvent new_copy_post = UserEvent::create_user_event();
          new_copy_post.trigger();
          copy_post = new_copy_post;
        }
#endif
        // Save the copy post in the post conditions
        if (copy_post.exists())
        {
          if (tracker != NULL)
            tracker->add_copy_event(copy_post);
          // Register copy post with the source views
          // Note it is up to the caller to make sure the event
          // gets registered with the destination
          for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator 
                it = update_views.begin(); it != update_views.end(); it++)
          {
            it->first->add_copy_user(0/*redop*/, copy_post, src_version_info,
                                     it->second, true/*reading*/);
          }
          postconditions[copy_post] = pre_set.set_mask;
        }
#if defined(LEGION_SPY) || defined(LEGION_LOGGING)
        IndexSpace copy_index_space =
                        dst->logical_node->as_region_node()->row_source->handle;
        for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator 
              it = update_views.begin(); it != update_views.end(); it++)
        {
#ifdef LEGION_LOGGING
          {
            std::set<FieldID> copy_fields;
            RegionNode *manager_node = dst->manager->region_node;
            manager_node->column_source->to_field_set(it->second,
                                                      copy_fields);
            LegionLogging::log_lowlevel_copy(
                Processor::get_executing_processor(),
                it->first->manager->get_instance(),
                dst->manager->get_instance(),
                copy_index_space.get_id(),
                manager_node->column_source->handle,
                manager_node->handle.tree_id,
                copy_pre, copy_post, copy_fields, 0/*redop*/);
          }
#endif
#ifdef LEGION_SPY
          RegionNode *manager_node = dst->manager->region_node;
          char *string_mask = 
            manager_node->column_source->to_string(it->second);
          {
            std::set<FieldID> field_set;
            manager_node->column_source->to_field_set(it->second, field_set);
            LegionSpy::log_copy_operation(
                it->first->manager->get_instance().id,
                dst->manager->get_instance().id,
                copy_index_space.get_id(),
                manager_node->column_source->handle.id,
                manager_node->handle.tree_id, copy_pre, copy_post,
                0/*redop*/, field_set);
          }
          free(string_mask);
#endif
        }
#endif
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void RegionTreeNode::compute_event_sets(FieldMask update_mask, 
                      const LegionMap<Event,FieldMask>::aligned &preconditions,
                      LegionList<EventSet>::aligned &precondition_sets)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<Event,FieldMask>::aligned::const_iterator pit = 
            preconditions.begin(); pit != preconditions.end(); pit++)
      {
        bool inserted = false;
        // Also keep track of which fields have updates
        // but don't have any preconditions
        update_mask -= pit->second;
        FieldMask remaining = pit->second;
        // Insert this event into the precondition sets 
        for (LegionList<EventSet>::aligned::iterator it = 
              precondition_sets.begin(); it != precondition_sets.end(); it++)
        {
          // Easy case, check for equality
          if (remaining == it->set_mask)
          {
            it->preconditions.insert(pit->first);
            inserted = true;
            break;
          }
          FieldMask overlap = remaining & it->set_mask;
          // Easy case, they are disjoint so keep going
          if (!overlap)
            continue;
          // Moderate case, we are dominated, split into two sets
          // reusing existing set and making a new set
          if (overlap == remaining)
          {
            // Leave the existing set and make it the difference 
            it->set_mask -= overlap;
            precondition_sets.push_back(EventSet(overlap));
            EventSet &last = precondition_sets.back();
            last.preconditions = it->preconditions;
            last.preconditions.insert(pit->first);
            inserted = true;
            break;
          }
          // Moderate case, we dominate the existing set
          if (overlap == it->set_mask)
          {
            // Add ourselves to the existing set and then
            // keep going for the remaining fields
            it->preconditions.insert(pit->first);
            remaining -= overlap;
            // Can't consider ourselves added yet
            continue;
          }
          // Hard case, neither dominates, compute three
          // distinct sets of fields, keep left one in
          // place and reduce scope, add new one at the
          // end for overlap, continue iterating for right one
          it->set_mask -= overlap;
          const std::set<Event> &temp_preconditions = it->preconditions;
          it = precondition_sets.insert(it, EventSet(overlap));
          it->preconditions = temp_preconditions;
          it->preconditions.insert(pit->first);
          remaining -= overlap;
          continue;
        }
        if (!inserted)
        {
          precondition_sets.push_back(EventSet(remaining));
          EventSet &last = precondition_sets.back();
          last.preconditions.insert(pit->first);
        }
      }
      // For any fields which need copies but don't have
      // any preconditions, but them in their own set.
      // Put it on the front because it is the copy with
      // no preconditions so it can start right away!
      if (!!update_mask)
        precondition_sets.push_front(EventSet(update_mask));
    }

    //--------------------------------------------------------------------------
    Event RegionTreeNode::perform_copy_operation(Operation *op,
                                                 Event precondition,
                         const std::vector<Domain::CopySrcDstField> &src_fields,
                         const std::vector<Domain::CopySrcDstField> &dst_fields)
    //--------------------------------------------------------------------------
    {
      if (has_component_domains())
      {
        Event domain_pre;
        const std::set<Domain> &component_domains = 
                                  get_component_domains(domain_pre);
        if (domain_pre.exists())
          precondition = Event::merge_events(precondition, domain_pre);
        std::set<Event> result_events;
        for (std::set<Domain>::const_iterator it = component_domains.begin();
              it != component_domains.end(); it++)
          result_events.insert(context->issue_copy(*it, op, src_fields, 
                                                   dst_fields, precondition));
        return Event::merge_events(result_events);
      }
      else
      {
        Event domain_pre;
        Domain copy_domain = get_domain(domain_pre);
        if (domain_pre.exists())
          precondition = Event::merge_events(precondition, domain_pre);
        return context->issue_copy(copy_domain, op, src_fields, 
                                   dst_fields, precondition);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::issue_update_reductions(LogicalView *target,
                                                 const FieldMask &mask,
                                                const VersionInfo &version_info,
                                                 Processor local_proc,
           const LegionMap<ReductionView*,FieldMask>::aligned &valid_reductions,
                                                 Operation *op,
                                                 CopyTracker *tracker/*= NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ISSUE_UPDATE_REDUCTIONS_CALL);
#endif
      // Handle the special case where the target is a composite view
      if (target->is_deferred_view())
      {
        DeferredView *def_view = target->as_deferred_view();
        // Save all the reductions to the composite target
        for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it =
              valid_reductions.begin(); it != valid_reductions.end(); it++)
        {
          FieldMask copy_mask = mask & it->second;
          if (!copy_mask)
            continue;
          def_view->update_reduction_views(it->first, copy_mask);
        }
        // Once we're done saving the reductions we are finished
        return;
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(target->is_instance_view());
#endif
      InstanceView *inst_target = target->as_instance_view();
      // Go through all of our reduction instances and issue reductions
      // to the target instances
      for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it = 
            valid_reductions.begin(); it != valid_reductions.end(); it++)
      {
        // Doesn't need to reduce to itself
        if (inst_target == (it->first))
          continue;
        FieldMask copy_mask = mask & it->second;
        if (!!copy_mask)
        {
#ifdef DEBUG_HIGH_LEVEL
          // all fields in the reduction instances should be used
          assert(!(it->second - copy_mask));
#endif
          // Then we have a reduction to perform
          it->first->perform_reduction(inst_target, copy_mask, version_info,
                                       local_proc, op, tracker);
        }
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::invalidate_instance_views(PhysicalState *state,
                                                  const FieldMask &invalid_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, INVALIDATE_INSTANCE_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      std::vector<LogicalView*> to_delete;
      for (LegionMap<LogicalView*,FieldMask>::aligned::iterator it = 
            state->valid_views.begin(); it != state->valid_views.end(); it++)
      {
        it->second -= invalid_mask;
        if (!it->second)
          to_delete.push_back(it->first);
      }
      for (std::vector<LogicalView*>::const_iterator it = to_delete.begin();
            it != to_delete.end(); it++)
      {
        state->valid_views.erase(*it);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::invalidate_reduction_views(PhysicalState *state,
                                                  const FieldMask &invalid_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, INVALIDATE_REDUCTION_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      std::vector<ReductionView*> to_delete;
      for (LegionMap<ReductionView*,FieldMask>::aligned::iterator it = 
            state->reduction_views.begin(); it != 
            state->reduction_views.end(); it++)
      {
        it->second -= invalid_mask;
        if (!it->second)
          to_delete.push_back(it->first);
      }
      for (std::vector<ReductionView*>::const_iterator it = to_delete.begin();
            it != to_delete.end(); it++)
      {
        state->reduction_views.erase(*it);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::update_valid_views(PhysicalState *state, 
                                            const FieldMask &valid_mask,
                                            bool dirty, LogicalView *new_view)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, UPDATE_VALID_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
      if (!new_view->is_deferred_view())
      {
        assert(new_view->is_instance_view());
        assert(new_view->as_instance_view()->is_materialized_view());
        assert(!(valid_mask - new_view->as_instance_view()->
                as_materialized_view()->manager->layout->allocated_fields));
      }
      assert(new_view->logical_node == this);
#endif
      if (dirty)
      {
        invalidate_instance_views(state, valid_mask); 
        state->dirty_mask |= valid_mask;
      }
      LegionMap<LogicalView*,FieldMask>::aligned::iterator finder = 
        state->valid_views.find(new_view);
      if (finder == state->valid_views.end())
      {
        // New valid view, update everything accordingly
        state->valid_views[new_view] = valid_mask;
      }
      else
      {
        // It already existed update the valid mask
        finder->second |= valid_mask;
      }
    } 

    //--------------------------------------------------------------------------
    void RegionTreeNode::update_valid_views(PhysicalState *state,
                                            const FieldMask &valid_mask,
                                            const FieldMask &dirty_mask,
                                     const std::vector<LogicalView*> &new_views)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, UPDATE_VALID_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      if (!!dirty_mask)
      {
        invalidate_instance_views(state, dirty_mask); 
        state->dirty_mask |= dirty_mask;
      }
      for (std::vector<LogicalView*>::const_iterator it = new_views.begin();
            it != new_views.end(); it++)
      {
        LegionMap<LogicalView*,FieldMask>::aligned::iterator finder = 
          state->valid_views.find(*it);
        if (finder == state->valid_views.end())
        {
          // New valid view, update everything accordingly
          state->valid_views[*it] = valid_mask;
        }
        else
        {
          // It already existed update the valid mask
          finder->second |= valid_mask;
        }
#ifdef DEBUG_HIGH_LEVEL
        finder = state->valid_views.find(*it);
        if (!(*it)->is_deferred_view())
        {
          assert((*it)->is_instance_view());
          assert((*it)->as_instance_view()->is_materialized_view());
          assert(!(finder->second - (*it)->as_instance_view()->
                as_materialized_view()->manager->layout->allocated_fields));
        }
#endif
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::update_valid_views(PhysicalState *state,
                                            const FieldMask &valid_mask,
                                            const FieldMask &dirty_mask,
                                const std::vector<MaterializedView*> &new_views)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, UPDATE_VALID_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      if (!!dirty_mask)
      {
        invalidate_instance_views(state, dirty_mask); 
        state->dirty_mask |= dirty_mask;
      }
      for (std::vector<MaterializedView*>::const_iterator it = 
            new_views.begin(); it != new_views.end(); it++)
      {
        LegionMap<LogicalView*,FieldMask>::aligned::iterator finder = 
          state->valid_views.find(*it);
        if (finder == state->valid_views.end())
        {
          // New valid view, update everything accordingly
          state->valid_views[*it] = valid_mask;
        }
        else
        {
          // It already existed update the valid mask
          finder->second |= valid_mask;
        }
#ifdef DEBUG_HIGH_LEVEL
        finder = state->valid_views.find(*it);
        assert(!(finder->second - (*it)->manager->layout->allocated_fields));
#endif
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::update_reduction_views(PhysicalState *state,
                                                const FieldMask &valid_mask,
                                                ReductionView *new_view) 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, UPDATE_REDUCTION_VIEWS_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(state->node == this);
#endif
      LegionMap<ReductionView*,FieldMask>::aligned::iterator finder = 
        state->reduction_views.find(new_view);
      if (finder == state->reduction_views.end())
      {
        state->reduction_views[new_view] = valid_mask;
      }
      else
      {
        finder->second |= valid_mask;
      }
      state->reduction_mask |= valid_mask;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::flush_reductions(const FieldMask &valid_mask,
                                          ReductionOpID redop,
                                          const MappableInfo &info,
                                          CopyTracker *tracker /*= NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FLUSH_REDUCTIONS_CALL);
#endif
      // Go through the list of reduction views and see if there are
      // any that don't mesh with the current user and therefore need
      // to be flushed.
      FieldMask flush_mask;
      LegionMap<LogicalView*,FieldMask>::aligned valid_views;
      LegionMap<ReductionView*,FieldMask>::aligned reduction_views;
      PhysicalState *state = get_physical_state(info.ctx, info.version_info); 
      for (LegionMap<ReductionView*,FieldMask>::aligned::const_iterator it =
            state->reduction_views.begin(); it != 
            state->reduction_views.end(); it++)
      {
        // Skip reductions that have the same reduction op ID
        if (it->first->get_redop() == redop)
          continue;
        FieldMask overlap = valid_mask & it->second;
        if (!overlap)
          continue;
        flush_mask |= overlap; 
        reduction_views.insert(*it);
      }
      // Now get any physical instances to flush to
      if (!!flush_mask)
      {
        find_valid_instance_views(info.ctx, state, flush_mask, flush_mask, 
                    info.version_info, false/*needs space*/, valid_views);
      }
      if (!!flush_mask)
      {
#ifdef DEBUG_HIGH_LEVEL
        FieldMask update_mask;
#endif
        // Iterate over all the valid instances and issue any reductions
        // to the target that need to be done
        for (LegionMap<LogicalView*,FieldMask>::aligned::iterator it = 
              valid_views.begin(); it != valid_views.end(); it++)
        {
          FieldMask overlap = flush_mask & it->second; 
          issue_update_reductions(it->first, overlap, info.version_info,
                    info.local_proc, reduction_views, info.op, tracker);
          // Save the overlap fields
          it->second = overlap;
#ifdef DEBUG_HIGH_LEVEL
          update_mask |= overlap;
#endif
        }
#ifdef DEBUG_HIGH_LEVEL
        // We should have issued reduction operations to at least
        // one place for every single reduction field.
        assert(update_mask == flush_mask);
#endif
        // Now update our physical state
        // Update the valid views.  Don't mark them dirty since we
        // don't want to accidentally invalidate some of our other
        // instances which get updated later in the loop.  Note this
        // is safe since we're updating all the instances for each
        // reduction field.
        for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
              valid_views.begin(); it != valid_views.end(); it++)
        {
          update_valid_views(state, it->second, false/*dirty*/, it->first);
        }
        // Update the dirty mask since we didn't do it when updating
        // the valid instance views do it now
        state->dirty_mask |= flush_mask;
        // Then invalidate all the reduction views that we flushed
        invalidate_reduction_views(state, flush_mask);
      }
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::initialize_physical_state(ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, INITIALIZE_PHYSICAL_STATE_CALL);
#endif
      if (!version_managers.has_entry(ctx))
        return;
      VersionManager *manager = version_managers.lookup_entry(ctx, this);
      manager->check_init();
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::invalidate_physical_state(ContextID ctx)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, INVALIDATE_PHYSICAL_STATE_CALL);
#endif
      if (!version_managers.has_entry(ctx))
        return;
      VersionManager *manager = version_managers.lookup_entry(ctx, this);
      manager->clear(); 
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::detach_instance_views(ContextID ctx,
                                               const FieldMask &detach_mask,
                                               PhysicalManager *target)
    //--------------------------------------------------------------------------
    {
      if (!version_managers.has_entry(ctx))
        return;
      VersionManager *manager = version_managers.lookup_entry(ctx, this);
      manager->detach_instance(detach_mask, target);
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::clear_physical_states(const FieldMask &clear_mask)
    //--------------------------------------------------------------------------
    {
#ifdef UNIMPLEMENTED_VERSIONING
      for (size_t idx = 0; idx < physical_states.max_entries(); idx++)
      {
        if (physical_states.has_entry(idx))
        {
          PhysicalState *state = acquire_physical_state(idx, true/*exclusive*/);
          invalidate_physical_state(state, clear_mask, true/*force*/);
          release_physical_state(state);
        }
      }
#else
      assert(false);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::add_persistent_view(ContextID ctx, 
                                             MaterializedView *persistent_view)
    //--------------------------------------------------------------------------
    {
      VersionManager *manager = version_managers.lookup_entry(ctx, this);
      manager->add_persistent_view(persistent_view);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeNode::register_logical_view(LogicalView *view)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      if (logical_views.find(view->did) != logical_views.end())
        return false;
      logical_views[view->did] = view;
      return true;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::unregister_logical_view(LogicalView *view)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      logical_views.erase(view->did);
    }

    //--------------------------------------------------------------------------
    LogicalView* RegionTreeNode::find_view(DistributedID did) 
    //--------------------------------------------------------------------------
    {
      // See if we can find it locally
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        LegionMap<DistributedID,LogicalView*,
                  LOGICAL_VIEW_ALLOC>::tracked::const_iterator finder = 
          logical_views.find(did);
        if (finder != logical_views.end())
          return finder->second;
      }
      RegionTreeNode *parent = get_parent();
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      // Otherwise go up the tree
      LogicalView *parent_view = parent->find_view(did);
      return parent_view->get_subview(get_color());
    }

    //--------------------------------------------------------------------------
    VersionState* RegionTreeNode::find_remote_version_state(ContextID ctx,
                  VersionID vid, DistributedID did, AddressSpaceID source,
                  const FieldMask &mask, bool initialize)
    //--------------------------------------------------------------------------
    {
      VersionManager *manager = version_managers.lookup_entry(ctx, this);
      return manager->find_remote_version_state(vid, did, source,
                                                mask, initialize);
    }

    //--------------------------------------------------------------------------
    bool RegionTreeNode::register_physical_manager(PhysicalManager *manager)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      if (physical_managers.find(manager->did) != physical_managers.end())
        return false;
      physical_managers[manager->did] = manager;
      return true;
    }

    //--------------------------------------------------------------------------
    void RegionTreeNode::unregister_physical_manager(PhysicalManager *manager)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      physical_managers.erase(manager->did);
    }

    //--------------------------------------------------------------------------
    PhysicalManager* RegionTreeNode::find_manager(DistributedID did)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      LegionMap<DistributedID,PhysicalManager*,
                PHYSICAL_MANAGER_ALLOC>::tracked::const_iterator finder = 
                  physical_managers.find(did);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != physical_managers.end());
#endif
      return finder->second;
    }

    //--------------------------------------------------------------------------
    template<AllocationType ALLOC, bool RECORD, bool HAS_SKIP, bool TRACK_DOM>
    /*static*/ FieldMask RegionTreeNode::perform_dependence_checks(
      const LogicalUser &user, 
      typename LegionList<LogicalUser, ALLOC>::track_aligned &prev_users,
      const FieldMask &check_mask, const FieldMask &open_below,
      bool validates_regions, Operation *to_skip /*= NULL*/, 
      GenerationID skip_gen /* = 0*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(user.op->runtime->forest, 
                        PERFORM_DEPENDENCE_CHECKS_CALL);
#endif
      FieldMask dominator_mask = check_mask;
      // It's not actually sound to assume we dominate something
      // if we don't observe any users of those fields.  Therefore
      // also keep track of the fields that we observe.  We'll use this
      // at the end when computing the final dominator mask.
      FieldMask observed_mask;
      // For domination, we only need to observe fields that
      // are open below, therefore, any fields which are not open
      // below can already be recorded as observed.
      if (TRACK_DOM)
        observed_mask = check_mask - open_below;
      FieldMask user_check_mask = user.field_mask & check_mask;
      const bool tracing = user.op->is_tracing();
      for (typename LegionList<LogicalUser, ALLOC>::track_aligned::iterator 
            it = prev_users.begin(); it != prev_users.end(); /*nothing*/)
      {
        if (HAS_SKIP && (to_skip == it->op) && (skip_gen == it->gen))
        {
          it++;
          continue;
        }
        FieldMask overlap = user_check_mask & it->field_mask;
        if (!!overlap)
        {
          if (TRACK_DOM)
            observed_mask |= overlap;
          DependenceType dtype = check_dependence_type(it->usage, user.usage);
          bool validate = validates_regions;
          switch (dtype)
          {
            case NO_DEPENDENCE:
              {
                // No dependence so remove bits from the dominator mask
                dominator_mask -= it->field_mask;
                break;
              }
            case ANTI_DEPENDENCE:
            case ATOMIC_DEPENDENCE:
            case SIMULTANEOUS_DEPENDENCE:
            case PROMOTED_DEPENDENCE:
              {
                // Mark that these kinds of dependences are not allowed
                // to validate region inputs
                validate = false;
                // No break so we register dependences just like
                // a true dependence
              }
            case TRUE_DEPENDENCE:
              {
#ifdef LEGION_LOGGING
                if (dtype != PROMOTED_DEPENDENCE)
                  LegionLogging::log_mapping_dependence(
                      Processor::get_executing_processor(),
                      user.op->get_parent()->get_unique_task_id(),
                      it->uid, it->idx, user.uid, user.idx, dtype);
#endif
#ifdef LEGION_SPY
                if (dtype != PROMOTED_DEPENDENCE)
                  LegionSpy::log_mapping_dependence(
                      user.op->get_parent()->get_unique_task_id(),
                      it->uid, it->idx, user.uid, user.idx, dtype);
#endif
                if (RECORD)
                  user.op->record_logical_dependence(*it);
                // Do this after the logging since we might 
                // update the iterator.
                // If we can validate a region record which of our
                // predecessors regions we are validating, otherwise
                // just register a normal dependence
                if (user.op->register_region_dependence(user.idx, it->op, 
                                                        it->gen, it->idx,
                                                        dtype, validate,
                                                        overlap))
                {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY)
                  // Now we can prune it from the list and continue
                  it = prev_users.erase(it);
#else
                  it++;
#endif
                  continue;
                }
                else
                {
                  // hasn't commited, reset timeout and continue
                  it->timeout = LogicalUser::TIMEOUT;
                  it++;
                  continue;
                }
                break;
              }
            default:
              assert(false); // should never get here
          }
        }
        // If we didn't register any kind of dependence, check
        // to see if the timeout has expired.  Note that it is
        // unsound to do this if we are tracing so don't perform
        // the check in that case.
        if (!tracing)
        {
          if (it->timeout <= 0)
          {
            // Timeout has expired.  Check whether the operation
            // has committed. If it has prune it from the list.
            // Otherwise reset its timeout and continue.
            if (it->op->is_operation_committed(it->gen))
            {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY)
              it = prev_users.erase(it);
#else
              // Can't prune things early for these cases
              it->timeout = LogicalUser::TIMEOUT;
              it++;
#endif
            }
            else
            {
              // Operation hasn't committed, reset timeout
              it->timeout = LogicalUser::TIMEOUT;
              it++;
            }
          }
          else
          {
            // Timeout hasn't expired, decrement it and continue
            it->timeout--;
            it++;
          }
        }
        else
          it++; // Tracing so no timeouts
      }
      // The result of this computation is the dominator mask.
      // It's only sound to say that we dominate fields that
      // we actually observed users for so intersect the dominator 
      // mask with the observed mask
      if (TRACK_DOM)
        return (dominator_mask & observed_mask);
      else
        return dominator_mask;
    }

    //--------------------------------------------------------------------------
    template<AllocationType ALLOC>
    /*static*/ void RegionTreeNode::perform_closing_checks(
        LogicalCloser &closer, 
        typename LegionList<LogicalUser, ALLOC>::track_aligned &users, 
        const FieldMask &check_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(closer.user.op->runtime->forest, 
                        PERFORM_CLOSING_CHECKS_CALL);
#endif
      // Since we are performing a close operation on the region
      // tree data structure, we know that we need to register
      // mapping dependences on all of the operations in the 
      // current epoch since close operations must be serialized
      // with respect to mappings.  Finally we have to upgrade the
      // privilege to read-write to ensure that anyone that comes
      // later also records mapping dependences on the users.
      const FieldMask user_check_mask = closer.user.field_mask & check_mask; 
      for (typename LegionList<LogicalUser, ALLOC>::track_aligned::iterator 
            it = users.begin(); it != users.end(); /*nothing*/)
      {
        FieldMask overlap = user_check_mask & it->field_mask;
        if (!overlap)
        {
          it++;
          continue;
        }
        // Skip any users of the same op, we know they won't be dependences
        if ((it->op == closer.user.op) && (it->gen == closer.user.gen))
        {
          it->field_mask -= overlap; 
          if (!it->field_mask)
          {
            it->op->remove_mapping_reference(it->gen);
            it = users.erase(it);
          }
          else
            it++;
          continue;
        }
        DependenceType dtype = check_dependence_type(it->usage, 
                                                     closer.user.usage);
        // We can 
#ifdef LEGION_LOGGING
        if ((dtype != NO_DEPENDENCE) && (dtype != PROMOTED_DEPENDENCE))
          LegionLogging::log_mapping_dependence(
              Processor::get_executing_processor(),
              closer.user.op->get_parent()->get_unique_task_id(),
              it->uid, it->idx, closer.user.uid, closer.user.idx, dtype);
#endif
#ifdef LEGION_SPY
        if ((dtype != NO_DEPENDENCE) && (dtype != PROMOTED_DEPENDENCE))
          LegionSpy::log_mapping_dependence(
              closer.user.op->get_parent()->get_unique_task_id(),
              it->uid, it->idx, closer.user.uid, closer.user.idx, dtype);
#endif
        // Register the dependence 
        if (closer.user.op->register_region_dependence(closer.user.idx, 
                                                       it->op, it->gen, 
                                                       it->idx, dtype,
                                                       closer.validates,
                                                       overlap))
        {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY)
          it = users.erase(it);
          continue;
#endif
        }
        else
        {
          // it hasn't committed, reset timeout
          it->timeout = LogicalUser::TIMEOUT;
        }
        
        if (closer.capture_users)
        {
          // Record that we closed this user
          // Update the field mask and the privilege
          closer.closed_users.push_back(*it);
          LogicalUser &closed_user = closer.closed_users.back();
          closed_user.field_mask = overlap;
          closed_user.usage.privilege = 
            (PrivilegeMode)((int)closed_user.usage.privilege | PROMOTED);
          // Remove the closed set of fields from this user
          it->field_mask -= overlap;
          // If it's empty, remove it from the list and let
          // the mapping reference go up the tree with it
          // Otherwise add a new mapping reference
          if (!it->field_mask)
            it = users.erase(it);
          else
          {
            it->op->add_mapping_reference(it->gen);
            it++;
          }
        }
        else
        {
          // Remove the closed set of fields from this user
          it->field_mask -= overlap;
          // Otherwise, if we can remote it, then remove it's
          // mapping reference from the logical tree.
          if (!it->field_mask)
          {
            it->op->remove_mapping_reference(it->gen);
            it = users.erase(it);
          }
          else
            it++;
        }
      }
    }

    /////////////////////////////////////////////////////////////
    // Region Node 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RegionNode::RegionNode(LogicalRegion r, PartitionNode *par,
                           IndexSpaceNode *row_src, FieldSpaceNode *col_src,
                           RegionTreeForest *ctx)
      : RegionTreeNode(ctx, col_src), handle(r), 
        parent(par), row_source(row_src)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RegionNode::RegionNode(const RegionNode &rhs)
      : RegionTreeNode(NULL, NULL), handle(LogicalRegion::NO_REGION), 
        parent(NULL), row_source(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    RegionNode::~RegionNode(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    RegionNode& RegionNode::operator=(const RegionNode &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* RegionNode::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<RegionNode,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void RegionNode::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    bool RegionNode::has_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      return (color_map.find(c) != color_map.end());
    }

    //--------------------------------------------------------------------------
    bool RegionNode::has_color(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // Ask the row source since it eagerly instantiates
      return row_source->has_child(c);
    }

    //--------------------------------------------------------------------------
    PartitionNode* RegionNode::get_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // check to see if we have it, if not try to make it
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<ColorPoint,PartitionNode*>::const_iterator finder = 
          color_map.find(c);
        if (finder != color_map.end())
          return finder->second;
      }
      // If we get here we didn't immediately have it so try
      // to make it through the proper channels
      IndexPartNode *index_part = row_source->get_child(c);
#ifdef DEBUG_HIGH_LEVEL
      assert(index_part != NULL);
#endif
      LogicalPartition part_handle(handle.tree_id, index_part->handle,
                                   handle.field_space);
      return context->create_node(part_handle, this);
    }

    //--------------------------------------------------------------------------
    void RegionNode::add_child(PartitionNode *child)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(color_map.find(child->row_source->color) == color_map.end());
#endif
      color_map[child->row_source->color] = child;
      valid_map[child->row_source->color] = child;
    }

    //--------------------------------------------------------------------------
    void RegionNode::remove_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      valid_map.erase(c);
    }

    //--------------------------------------------------------------------------
    void RegionNode::add_creation_source(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      creation_set.add(source);
    }

    //--------------------------------------------------------------------------
    void RegionNode::destroy_node(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        parent->remove_child(row_source->color);
      AutoLock n_lock(node_lock);
      destruction_set.add(source);
    }

    //--------------------------------------------------------------------------
    unsigned RegionNode::get_depth(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->depth;
    }

    //--------------------------------------------------------------------------
    const ColorPoint& RegionNode::get_color(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->color;
    }

    //--------------------------------------------------------------------------
    IndexTreeNode* RegionNode::get_row_source(void) const
    //--------------------------------------------------------------------------
    {
      return row_source;
    }

    //--------------------------------------------------------------------------
    RegionTreeID RegionNode::get_tree_id(void) const
    //--------------------------------------------------------------------------
    {
      return handle.get_tree_id();
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* RegionNode::get_parent(void) const
    //--------------------------------------------------------------------------
    {
      return parent;
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* RegionNode::get_tree_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      return get_child(c);
    }

    //--------------------------------------------------------------------------
    bool RegionNode::are_children_disjoint(const ColorPoint &c1, 
                                           const ColorPoint &c2)
    //--------------------------------------------------------------------------
    {
      return row_source->are_disjoint(c1, c2);
    }

    //--------------------------------------------------------------------------
    bool RegionNode::are_all_children_disjoint(void)
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    void RegionNode::instantiate_children(void)
    //--------------------------------------------------------------------------
    {
      std::set<ColorPoint> all_colors;
      row_source->get_colors(all_colors);
      // This may look like it does nothing, but it checks to see
      // if we have instantiated all the child nodes
      for (std::set<ColorPoint>::const_iterator it = all_colors.begin(); 
            it != all_colors.end(); it++)
        get_child(*it);
    }

    //--------------------------------------------------------------------------
    bool RegionNode::is_region(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    RegionNode* RegionNode::as_region_node(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<RegionNode*>(this);
    }

    //--------------------------------------------------------------------------
    PartitionNode* RegionNode::as_partition_node(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    bool RegionNode::visit_node(PathTraverser *traverser)
    //--------------------------------------------------------------------------
    {
      return traverser->visit_region(this);
    }

    //--------------------------------------------------------------------------
    bool RegionNode::visit_node(NodeTraverser *traverser)
    //--------------------------------------------------------------------------
    {
      bool continue_traversal = traverser->visit_region(this);
      if (continue_traversal)
      {
        const bool break_early = traverser->break_early();
        if (traverser->force_instantiation)
        {
          // If we are forcing instantiation, then grab the set of 
          // colors from the row source and use them to instantiate children
          std::set<ColorPoint> children_colors;
          row_source->get_child_colors(children_colors, 
                                       traverser->visit_only_valid());
          for (std::set<ColorPoint>::const_iterator it = 
                children_colors.begin(); it != children_colors.end(); it++)
          {
            bool result = get_child(*it)->visit_node(traverser);
            continue_traversal = continue_traversal && result;
            if (!result && break_early)
              break;
          }
        }
        else
        {
          std::map<ColorPoint,PartitionNode*> children;
          // Need to hold the lock when reading from 
          // the color map or the valid map
          if (traverser->visit_only_valid())
          {
            AutoLock n_lock(node_lock,1,false/*exclusive*/);
            children = valid_map;
          }
          else
          {
            AutoLock n_lock(node_lock,1,false/*exclusive*/);
            children = color_map;
          }
          for (std::map<ColorPoint,PartitionNode*>::const_iterator it = 
                children.begin(); it != children.end(); it++)
          {
            bool result = it->second->visit_node(traverser);
            continue_traversal = continue_traversal && result;
            if (!result && break_early)
              break;
          }
        }
      }
      return continue_traversal;
    }

    //--------------------------------------------------------------------------
    bool RegionNode::has_component_domains(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->has_component_domains();
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& RegionNode::get_component_domains_blocking(
                                                                     void) const
    //--------------------------------------------------------------------------
    {
      return row_source->get_component_domains_blocking();
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& RegionNode::get_component_domains(
                                                             Event &ready) const
    //--------------------------------------------------------------------------
    {
      return row_source->get_component_domains(ready);
    }

    //--------------------------------------------------------------------------
    const Domain& RegionNode::get_domain_blocking(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->get_domain_blocking();
    }

    //--------------------------------------------------------------------------
    const Domain& RegionNode::get_domain(Event &precondition) const
    //--------------------------------------------------------------------------
    {
      return row_source->get_domain(precondition);
    }

    //--------------------------------------------------------------------------
    const Domain& RegionNode::get_domain_no_wait(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->get_domain_no_wait();
    }

    //--------------------------------------------------------------------------
    bool RegionNode::is_complete(void)
    //--------------------------------------------------------------------------
    {
      // For now just assume that regions are never complete
      return false;
    }

    //--------------------------------------------------------------------------
    bool RegionNode::intersects_with(RegionTreeNode *other)
    //--------------------------------------------------------------------------
    {
      if (other->is_region())
        return row_source->intersects_with(
                  other->as_region_node()->row_source);
      else
        return row_source->intersects_with(
                  other->as_partition_node()->row_source);
    }

    //--------------------------------------------------------------------------
    bool RegionNode::dominates(RegionTreeNode *other)
    //--------------------------------------------------------------------------
    {
      if (other->is_region())
        return row_source->dominates(other->as_region_node()->row_source);
      else
        return row_source->dominates(other->as_partition_node()->row_source);
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& RegionNode::get_intersection_domains(
                                                          RegionTreeNode *other)
    //--------------------------------------------------------------------------
    {
      if (other->is_region())
        return row_source->get_intersection_domains(
                                           other->as_region_node()->row_source);
      else
        return row_source->get_intersection_domains(
                                        other->as_partition_node()->row_source);
    }

    //--------------------------------------------------------------------------
    size_t RegionNode::get_num_children(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->get_num_children();
    }

    //--------------------------------------------------------------------------
    InterCloseOp* RegionNode::create_close_op(Operation *creator,
                                              const FieldMask &closing_mask,
                                              bool leave_open,
                                            const std::set<ColorPoint> &targets,
                                              const VersionInfo &close_info,
                                              const VersionInfo &version_info,
                                              const RestrictInfo &restrict_info,
                                              const TraceInfo &trace_info)
    //--------------------------------------------------------------------------
    {
      InterCloseOp *op = context->runtime->get_available_inter_close_op(false);
      // Construct a reigon requirement for this operation
      // All privileges are based on the parent logical region
      RegionRequirement req(handle, READ_WRITE, EXCLUSIVE, 
                            trace_info.req.parent);
      // Compute the set of fields that we need
      column_source->get_field_set(closing_mask, 
                                   trace_info.req.privilege_fields,
                                   req.privilege_fields);
      // Now initialize the operation
      op->initialize(creator->get_parent(), req, targets, leave_open, 
                     trace_info.trace, trace_info.req_idx, 
                     close_info, version_info, restrict_info, 
                     closing_mask, creator);
      return op;
    }

    //--------------------------------------------------------------------------
    bool RegionNode::perform_close_operation(const MappableInfo &info,
                                             const FieldMask &closing_mask,
                                            const std::set<ColorPoint> &targets,
                                             const MappingRef &target_region,
                                             VersionInfo &version_info,
                                             bool leave_open, 
                                      const std::set<ColorPoint> &next_children,
                                             Event &closed,
                                             bool &create_composite)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!create_composite); // should always start off like this
#endif
      PhysicalCloser closer(info, leave_open, handle);
      if (target_region.has_ref())
      {
        LogicalView *view = target_region.get_view();
#ifdef DEBUG_HIGH_LEVEL
        assert(!view->is_deferred_view());
        assert(view->is_instance_view());
#endif
        InstanceView *inst_view = view->as_instance_view();
#ifdef DEBUG_HIGH_LEVEL
        assert(inst_view->is_materialized_view());
#endif
        closer.add_target(inst_view->as_materialized_view());
      }
      bool success = true;
      bool changed = false;
      PhysicalState *state = get_physical_state(info.ctx, version_info);
      for (std::set<ColorPoint>::const_iterator it = targets.begin(); 
            it != targets.end(); it++)
      {
        LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
          state->children.open_children.find(*it);
        if (finder == state->children.open_children.end())
          continue;
        bool result = close_physical_child(closer, state, closing_mask,
                                           (*it), finder->second,
                                           next_children, 
                                           create_composite, changed);
        if (!result || create_composite)
        {
          success = false;
          break;
        }
        if (!finder->second)
          state->children.open_children.erase(finder);
      }
      // If anything changed, rebuild the field mask
      if (changed)
      {
        FieldMask next_valid;
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
              state->children.open_children.begin(); it !=
              state->children.open_children.end(); it++)
        {
          next_valid |= it->second;
        }
        state->children.valid_fields = next_valid;
      }
      if (success)
      {
        closer.update_node_views(this, state);
        // Now flush any reductions which need to be closed
        if (!!state->reduction_mask)
        {
          FieldMask flush_reduction_mask = state->reduction_mask & closing_mask;
          if (!!flush_reduction_mask)
            flush_reductions(flush_reduction_mask, 0/*redop*/, info, &closer);
        }
        closed = closer.get_termination_event();
      }
      return success;
    }

    //--------------------------------------------------------------------------
    MaterializedView* RegionNode::create_instance(Memory target_mem,
                                                const std::set<FieldID> &fields,
                                                size_t blocking_factor,
                                                unsigned depth,
                                                Operation *op)
    //--------------------------------------------------------------------------
    {
      DistributedID did = context->runtime->get_available_distributed_id(false);
      UniqueID op_id = (op == NULL) ? 0 : op->get_unique_op_id();
      InstanceManager *manager = column_source->create_instance(target_mem,
                                      row_source->get_domain_blocking(),
                                      fields, blocking_factor, depth, this, 
                                      did, op_id);
      // See if we made the instance
      MaterializedView *result = NULL;
      if (manager != NULL)
      {
        result = manager->create_top_view(depth);
#ifdef DEBUG_HIGH_LEVEL
        assert(result != NULL);
#endif
#ifdef LEGION_LOGGING
        LegionLogging::log_physical_instance(
            Processor::get_executing_processor(),
            manager->get_instance(), manager->memory,
            handle.index_space, handle.field_space, handle.tree_id);
#endif
#ifdef LEGION_SPY
        LegionSpy::log_physical_instance(manager->get_instance().id,
            manager->memory.id, handle.index_space.id,
            handle.field_space.id, handle.tree_id);
        for (std::set<FieldID>::const_iterator it = fields.begin();
             it != fields.end(); ++it)
          LegionSpy::log_instance_field(manager->get_instance().id, *it);
#endif
      }
      else
        context->runtime->free_distributed_id(did);
      return result;
    }

    //--------------------------------------------------------------------------
    ReductionView* RegionNode::create_reduction(Memory target_mem, FieldID fid,
                                                bool reduction_list,
                                                ReductionOpID redop,
                                                Operation *op)
    //--------------------------------------------------------------------------
    {
      DistributedID did = context->runtime->get_available_distributed_id(false);
      UniqueID op_id = (op == NULL) ? 0 : op->get_unique_op_id();
      ReductionManager *manager = column_source->create_reduction(target_mem,
                                      row_source->get_domain_blocking(),
                                      fid, reduction_list, this, redop, 
                                      did, op_id);
      ReductionView *result = NULL;
      if (manager != NULL)
      {
        result = manager->create_view(); 
#ifdef DEBUG_HIGH_LEVEL
        assert(result != NULL);
#endif
#ifdef LEGION_LOGGING
        LegionLogging::log_physical_instance(
            Processor::get_executing_processor(),
            manager->get_instance(), manager->memory,
            handle.index_space, handle.field_space, handle.tree_id,
            redop, !reduction_list, manager->get_pointer_space());
#endif
#ifdef LEGION_SPY
        Domain ptr_space = manager->get_pointer_space();
        LegionSpy::log_physical_reduction(manager->get_instance().id,
            manager->memory.id, handle.index_space.id,
            handle.field_space.id, handle.tree_id, !reduction_list,
            ptr_space.exists() ? ptr_space.get_index_space().id : 0);
        LegionSpy::log_instance_field(manager->get_instance().id, fid);
#endif
      }
      else
        context->runtime->free_distributed_id(did);
      return result;
    }

    //--------------------------------------------------------------------------
    void RegionNode::send_node(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have it in our creation set in which
      // case we are done otherwise keep going up
      bool continue_up = false;
      bool send_deletion = false;
      {
        AutoLock n_lock(node_lock); 
        if (!creation_set.contains(target))
        {
          continue_up = true;
          creation_set.add(target);
        }
        if (!destruction_set.contains(target))
        {
          send_deletion = true;
          destruction_set.add(target);
        }
      }
      if (continue_up)
      {
        if (parent != NULL)
        {
          // Send the parent node first
          parent->send_node(target);
          AutoLock n_lock(node_lock);
          for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
                semantic_info.begin(); it != semantic_info.end(); it++)
          {
            it->second.node_mask.add(target);
            Serializer rez;
            {
              RezCheck z(rez);
              rez.serialize(handle);
              rez.serialize(it->first);
              rez.serialize(it->second.node_mask);
              rez.serialize(it->second.size);
              rez.serialize(it->second.buffer, it->second.size);
            }
            context->runtime->send_logical_region_semantic_info(target, rez);
          }
        }
        else
        {
          // We've made it to the top, send this node
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(handle);
            rez.serialize<size_t>(semantic_info.size());
            for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
                  semantic_info.begin(); it != semantic_info.end(); it++)
            {
              rez.serialize(it->first);
              rez.serialize(it->second.size);
              rez.serialize(it->second.buffer, it->second.size);
              it->second.node_mask.add(target);
            }
          }
          context->runtime->send_logical_region_node(target, rez);
        }
      }
      if (send_deletion)
      {
        context->runtime->send_logical_region_destruction(handle, target);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void RegionNode::handle_node_creation(RegionTreeForest *context,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      LogicalRegion handle;
      derez.deserialize(handle);

      RegionNode *node = context->create_node(handle, NULL/*parent*/);
#ifdef DEBUG_HIGH_LEVEL
      assert(node != NULL);
#endif
      node->add_creation_source(source);
      size_t num_semantic;
      derez.deserialize(num_semantic);
      if (num_semantic > 0)
      {
        NodeSet source_mask;
        source_mask.add(source);
        source_mask.add(context->runtime->address_space);
        for (unsigned idx = 0; idx < num_semantic; idx++)
        {
          SemanticTag tag;
          derez.deserialize(tag);
          size_t buffer_size;
          derez.deserialize(buffer_size);
          const void *buffer = derez.get_current_pointer();
          derez.advance_pointer(buffer_size);
          node->attach_semantic_information(tag, source_mask, 
                                            buffer, buffer_size);
        }
      }
    } 

    //--------------------------------------------------------------------------
    void RegionNode::remap_region(ContextID ctx, MaterializedView *view,
                                  const FieldMask &user_mask, 
                                  VersionInfo &version_info, 
                                  FieldMask &needed_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REMAP_REGION_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(view != NULL);
#endif
      PhysicalState *state = get_physical_state(ctx, version_info); 
      // We've already pre-mapped so we've pulled down
      // all the valid instance views.  Check to see if we
      // the target views is already there with the right
      // set of valid fields.
      LegionMap<LogicalView*,FieldMask>::aligned::const_iterator finder = 
        state->valid_views.find(view);
      if (finder == state->valid_views.end())
        needed_mask = user_mask;
      else
        needed_mask = user_mask - finder->second;
    }

    //--------------------------------------------------------------------------
    InstanceRef RegionNode::register_region(const MappableInfo &info,
                                            Event term_event,
                                            const RegionUsage &usage,
                                            const FieldMask &user_mask,
                                            LogicalView *view,
                                            const FieldMask &needed_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REGISTER_REGION_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(view != NULL);
      assert(view->is_instance_view());
#endif
      InstanceView *inst_view = view->as_instance_view();
      // This mirrors the if-else statement in MappingTraverser::visit_region
      // for handling the different instance and reduction cases
      if (!inst_view->is_reduction_view())
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(inst_view->is_materialized_view());
#endif
        MaterializedView *new_view = inst_view->as_materialized_view();
        // Issue updates for any fields which needed to be brought up
        // to date with the current versions of those fields
        // (assuming we are not write discard)
        bool needs_update_views = true;
        PhysicalState *state = get_physical_state(info.ctx, info.version_info);
        if (!IS_WRITE_ONLY(info.req) && !!needed_fields) 
        {
          LegionMap<LogicalView*,FieldMask>::aligned valid_views;
          find_valid_instance_views(info.ctx, state, needed_fields, 
           needed_fields, info.version_info, false/*needs space*/, valid_views);
          issue_update_copies(info, new_view, needed_fields, valid_views);
        }

        // If we mapped the region close up any partitions
        // below that might have valid data that we need for
        // this instance.  We only need to do this for 
        // non-read-only tasks, since the read-only close
        // operations happened during the pre-mapping step.
        if (!IS_READ_ONLY(info.req))
        {
          if (!IS_WRITE_ONLY(info.req))
          {
            PhysicalCloser closer(info, false/*leave open*/, handle);
            closer.add_target(new_view);
            // Mark the dirty mask with our bits since we're 
            closer.update_dirty_mask(user_mask);
            // writing and the closer will 
            bool create_composite = false;
            std::set<ColorPoint> empty_next_children;
#ifdef DEBUG_HIGH_LEVEL
            bool result = 
#endif
            siphon_physical_children(closer, state, user_mask,
                                     empty_next_children, 
                                     create_composite);
#ifdef DEBUG_HIGH_LEVEL
            assert(result); // should always succeed
            assert(!create_composite);
#endif
            // Now update the valid views and the dirty mask
            closer.update_node_views(this, state);
            // flush any reductions that we need to do
            FieldMask reduction_overlap = user_mask &
                                          state->reduction_mask;
            if (!!reduction_overlap)
              flush_reductions(reduction_overlap, 0/*redop*/, info); 
          }
          else
          {
            // Remove any open children for these fields, we don't
            // need to traverse down the tree because we already
            // advanced those version numbers logically so we don't
            // need to both updating the version states
            if (!(user_mask * state->children.valid_fields))
            {
              std::vector<ColorPoint> to_delete; 
              for (LegionMap<ColorPoint,FieldMask>::aligned::iterator 
                    it = state->children.open_children.begin(); 
                    it != state->children.open_children.end(); it++)
              {
                it->second -= user_mask;
                if (!it->second)
                  to_delete.push_back(it->first);
              }
              if (!to_delete.empty())
              {
                if (to_delete.size() != state->children.open_children.size())
                {
                  for (std::vector<ColorPoint>::const_iterator it = 
                        to_delete.begin(); it != to_delete.end(); it++)
                  {
                    state->children.open_children.erase(*it);  
                  }
                }
                else
                  state->children.open_children.clear();
              }
              state->children.valid_fields -= user_mask;
            }
            // Remove any overlapping reducitons
            FieldMask reduction_overlap = user_mask &
                                          state->reduction_mask;
            if (!!reduction_overlap)
              invalidate_reduction_views(state, reduction_overlap);
            // This is write-only so update the valid views on the
            // state with the new instance view
            update_valid_views(state, user_mask, 
                               true/*dirty*/, new_view);
          }
        }
        else if (needs_update_views)
        {
          // Otherwise just issue a non-dirty update for the user fields
          update_valid_views(state, user_mask,
                             false/*dirty*/, new_view);
        }
        // Now add ourselves as a user of this region
        return new_view->add_user(usage, term_event,
                                  user_mask, info.version_info);
      }
      else
      {
#ifdef DEBUG_HIGH_LEVEL
        // should never have needed fields for reductions
        assert(!needed_fields); 
        assert(inst_view->is_reduction_view());
#endif
        ReductionView *new_view = inst_view->as_reduction_view();
        PhysicalState *state = get_physical_state(info.ctx,
                                                  info.version_info);
        // If there was a needed close for this reduction then
        // it was performed as part of the premapping operation
        update_reduction_views(state, user_mask, new_view);
        // Now we can add ourselves as a user of this region
        return new_view->add_user(usage, term_event,
                                  user_mask, info.version_info);
      }
    }

    //--------------------------------------------------------------------------
    InstanceRef RegionNode::seed_state(ContextID ctx, Event term_event,
                                       const RegionUsage &usage,
                                       const FieldMask &user_mask,
                                       InstanceView *new_view,
                                       Processor local_proc)
    //--------------------------------------------------------------------------
    {
      version_managers.lookup_entry(ctx,this)->initialize_state(new_view, 
                                                term_event, usage, user_mask);
      return InstanceRef(Event::NO_EVENT, new_view);
    } 

    //--------------------------------------------------------------------------
    Event RegionNode::close_state(const MappableInfo &info, 
                                  Event term_event, RegionUsage &usage, 
                                  const FieldMask &user_mask,
                                  const InstanceRef &target)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, CLOSE_PHYSICAL_STATE_CALL);
#endif
      InstanceView *inst_view = target.get_instance_view();
      if (inst_view->is_reduction_view())
      {
        ReductionView *target_view = inst_view->as_reduction_view();
        // Flush any reductions from this level, and then flush any
        // from farther down in the tree
        PhysicalState *state = get_physical_state(info.ctx, info.version_info);
        ReductionCloser closer(info.ctx, target_view, user_mask, 
                               info.version_info, info.local_proc, info.op);
        closer.issue_close_reductions(this, state);
        siphon_physical_children(closer, state);
        // Important trick: switch the user to read-only so it picks
        // up dependences on all the reductions applied ot this instance
        usage.privilege = READ_ONLY;
        InstanceRef result = target_view->add_user(usage, term_event,
                                                  user_mask, info.version_info);
#ifdef DEBUG_HIGH_LEVEL
        assert(result.has_ref());
#endif
        return result.get_ready_event();
      }
      else
      {
        MaterializedView *target_view = inst_view->as_materialized_view();
        PhysicalState *state = get_physical_state(info.ctx, info.version_info);
        // Figure out what fields we need to update
        FieldMask update_mask = user_mask;
        LegionMap<LogicalView*,FieldMask>::aligned::const_iterator finder = 
          state->valid_views.find(target_view);
        if (finder != state->valid_views.end())
          update_mask -= finder->second;
        // All we should actually have to do here is just register
        // our region because any actualy close operations that would
        // need to be done would have been issued as part of the 
        // logical analysis.
        InstanceRef result = register_region(info, term_event, usage, user_mask,
                                             target_view, update_mask);
#ifdef DEBUG_HIGH_LEVEL
        assert(result.has_ref());
#endif
        return result.get_ready_event(); 
      }
    }

    //--------------------------------------------------------------------------
    void RegionNode::find_field_descriptors(ContextID ctx, Event term_event,
                                            const RegionUsage &usage,
                                            const FieldMask &user_mask,
                                            unsigned fid_idx, Processor proc,
                                  std::vector<FieldDataDescriptor> &field_data,
                                            std::set<Event> &preconditions,
                                            VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
      PhysicalState *state = get_physical_state(ctx, version_info);
      // First pull down any valid instance views
      pull_valid_instance_views(ctx, state, user_mask, 
                                false/*need space*/, version_info);
      // Now go through the list of valid instances and see if we can find
      // one that satisfies the field that we need.
      DeferredView *deferred_view = NULL;
      for (LegionMap<LogicalView*,FieldMask>::track_aligned::const_iterator
            it = state->valid_views.begin(); 
            it != state->valid_views.end(); it++)
      {
        // Check to see if the instance is valid for our target field
        if (it->second.is_set(fid_idx))
        {
          // See if this is a composite view or not
          if (it->first->is_instance_view())
          {
#ifdef DEBUG_HIGH_LEVEL
            assert(it->first->as_instance_view()->is_materialized_view());
#endif
            MaterializedView *view = 
              it->first->as_instance_view()->as_materialized_view(); 
            // Record the instance and its information
            field_data.push_back(FieldDataDescriptor());
            view->set_descriptor(field_data.back(), fid_idx);
            // Register ourselves as user of this instance
            InstanceRef ref = view->add_user(usage, term_event,
                                             user_mask, version_info);  
            Event ready_event = ref.get_ready_event();
            if (ready_event.exists())
              preconditions.insert(ready_event);
            // We found an actual instance so we are done
            deferred_view = NULL;
            break;
          }
          else
          {
            // Save it as a composite view and keep going
#ifdef DEBUG_HIGH_LEVEL
            assert(it->first->is_deferred_view());
            // There should be at most one composite view for this field
            assert(deferred_view == NULL);
#endif
            deferred_view = it->first->as_deferred_view();
          }
        }
      }
      if (deferred_view != NULL)
      {
        // If this is a fill view, we either need to make a new physical
        // instance or apply it to all the existing physical instances
        if (!deferred_view->is_composite_view())
          assert(false); // TODO: implement this
        deferred_view->find_field_descriptors(term_event, usage, 
                                              user_mask, fid_idx, proc, 
                                              field_data, preconditions);
      }
    }

    //--------------------------------------------------------------------------
    void RegionNode::fill_fields(ContextID ctx, const FieldMask &fill_mask,
                                 const void *value, size_t value_size,
                                 VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
      // Make the fill instance
      DistributedID did = context->runtime->get_available_distributed_id(false);
      FillView::FillViewValue *fill_value = 
        new FillView::FillViewValue(value, value_size);
      FillView *fill_view = 
        legion_new<FillView>(context, did, context->runtime->address_space,
                             context->runtime->address_space, this, 
                             true/*register now*/, fill_value);
      // Now update the physical state
      PhysicalState *state = get_physical_state(ctx, version_info);
      update_valid_views(state, fill_mask, true/*dirty*/, fill_view);
    }

    //--------------------------------------------------------------------------
    InstanceRef RegionNode::attach_file(ContextID ctx, 
                                        const FieldMask &attach_mask,
                                        const RegionRequirement &req,
                                        AttachOp *attach_op,
                                        VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
      // Create a new instance view based on the file
      InstanceManager *manager = 
        column_source->create_file_instance(req.privilege_fields,
                                            attach_mask, this, attach_op);
      // Wrap it in a view
      MaterializedView *view = manager->create_top_view(row_source->depth);
#ifdef DEBUG_HIGH_LEVEL
      assert(view != NULL);
#endif
      // Update the physical state with the new instance
      PhysicalState *state = get_physical_state(ctx, version_info);
      update_valid_views(state, attach_mask, false/*dirty*/, view);
      // Return the resulting instance
      return InstanceRef(Event::NO_EVENT, view);
    }

    //--------------------------------------------------------------------------
    void RegionNode::detach_file(ContextID ctx, const FieldMask &detach_mask,
                                 PhysicalManager *detach_target)
    //--------------------------------------------------------------------------
    {
      // Detach any instance views from this node down
      PhysicalDetacher detacher(ctx, detach_mask, detach_target);
      visit_node(&detacher);
#ifdef OLD_LEGION_PROF
      LegionProf::register_instance_deletion(detach_target->get_instance().id);
#endif
    }

    //--------------------------------------------------------------------------
    void RegionNode::send_semantic_info(const NodeSet &targets,
                                        SemanticTag tag,
                                        const void *buffer, size_t size,
                                        const NodeSet &current)
    //--------------------------------------------------------------------------
    {
      // Package up the message first
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(handle);
        rez.serialize(tag);
        rez.serialize(current);
        rez.serialize(size);
        rez.serialize(buffer, size);
      }
      // Then send the messages
      SendSemanticInfoFunctor<LOGICAL_REGION_SEMANTIC>
                                  functor(context->runtime, rez);
      targets.map(functor);
    }

    //--------------------------------------------------------------------------
    /*static*/ void RegionNode::handle_semantic_info(RegionTreeForest *forest,
                                                     Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      LogicalRegion handle;
      derez.deserialize(handle);
      SemanticTag tag;
      derez.deserialize(tag);
      NodeSet sources;
      derez.deserialize(sources);
      size_t size;
      derez.deserialize(size);
      const void *buffer = derez.get_current_pointer();
      derez.advance_pointer(size);
      forest->attach_semantic_information(handle, tag, sources, buffer, size);
    }

    //--------------------------------------------------------------------------
    void RegionNode::print_logical_context(ContextID ctx, 
                                           TreeStateLogger *logger,
                                           const FieldMask &capture_mask) 
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), logger->get_depth());
            break;
          }
        case 1:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], logger->get_depth());
            break;
          }
        case 2:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d) at "
                        "depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], logger->get_depth());
            break;
          }
        case 3:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d,%d) at "
                        "depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], logger->get_depth());
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (logical_states.has_entry(ctx))
      {
        LogicalState &state = get_logical_state(ctx);
        print_logical_state(state, capture_mask, to_traverse, logger);  
      }
      else
      {
        logger->log("No state");
      }
      logger->log("");
      if (!to_traverse.empty())
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,PartitionNode*>::const_iterator finder = 
            color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->print_logical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }

    //--------------------------------------------------------------------------
    void RegionNode::print_physical_context(ContextID ctx, 
                                            TreeStateLogger *logger,
                                            const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), logger->get_depth());
            break;
          }
        case 1:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], logger->get_depth());
            break;
          }
        case 2:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d) at "
                        "depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], logger->get_depth());
            break;
          }
        case 3:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d,%d) at "
                        "depth %d", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], logger->get_depth());
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (version_managers.has_entry(ctx))
      {
        VersionManager *manager = version_managers.lookup_entry(ctx, this);
        manager->print_physical_state(this, capture_mask, to_traverse, logger);
      }
      else
      {
        logger->log("No state");
      }
      logger->log("");
      if (!to_traverse.empty())
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,PartitionNode*>::const_iterator finder = 
            color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->print_physical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }

    //--------------------------------------------------------------------------
    void RegionNode::print_logical_state(LogicalState &state,
                                         const FieldMask &capture_mask,
                         LegionMap<ColorPoint,FieldMask>::aligned &to_traverse,
                                         TreeStateLogger *logger)
    //--------------------------------------------------------------------------
    {
      // Open Field States 
      {
        logger->log("Open Field States (%ld)", state.field_states.size());
        logger->down();
        for (std::list<FieldState>::const_iterator it = 
              state.field_states.begin(); it != 
              state.field_states.end(); it++)
        {
          it->print_state(logger, capture_mask);
          if (it->valid_fields * capture_mask)
            continue;
          for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator 
                cit = it->open_children.begin(); cit != 
                it->open_children.end(); cit++)
          {
            FieldMask overlap = cit->second & capture_mask;
            if (!overlap)
              continue;
            if (to_traverse.find(cit->first) == to_traverse.end())
              to_traverse[cit->first] = overlap;
            else
              to_traverse[cit->first] |= overlap;
          }
        }
        logger->up();
      }
    }
    
#ifdef DEBUG_HIGH_LEVEL
    //--------------------------------------------------------------------------
    void RegionNode::dump_logical_context(ContextID ctx, 
                                          TreeStateLogger *logger,
                                          const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), logger->get_depth(), this);
            break;
          }
        case 1:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], logger->get_depth(), this);
            break;
          }
        case 2:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d) at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], logger->get_depth(), this);
            break;
          }
        case 3:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d,%d) at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], logger->get_depth(), this);
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (logical_states.has_entry(ctx))
        print_logical_state(get_logical_state(ctx), capture_mask,
                            to_traverse, logger);
      else
        logger->log("No state");
      logger->log("");
      if (!to_traverse.empty())
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =  
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,PartitionNode*>::const_iterator finder = 
            color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->dump_logical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }

    //--------------------------------------------------------------------------
    void RegionNode::dump_physical_context(ContextID ctx,
                                           TreeStateLogger *logger,
                                           const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), logger->get_depth(), this);
            break;
          }
        case 1:
          {
            logger->log("Region Node (%x,%d,%d) Color %d at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], logger->get_depth(), this);
            break;
          }
        case 2:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d) at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], logger->get_depth(), this);
            break;
          }
        case 3:
          {
            logger->log("Region Node (%x,%d,%d) Color (%d,%d,%d) at "
                        "depth %d (%p)", 
              handle.index_space.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], logger->get_depth(), this);
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (version_managers.has_entry(ctx))
      {
        VersionManager *manager = version_managers.lookup_entry(ctx, this);
        manager->print_physical_state(this, capture_mask, to_traverse, logger);
      }
      else
        logger->log("No state");
      logger->log("");
      if (!to_traverse.empty())
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,PartitionNode*>::const_iterator finder = 
            color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->dump_physical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }
#endif

    /////////////////////////////////////////////////////////////
    // Partition Node 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PartitionNode::PartitionNode(LogicalPartition p, RegionNode *par,
                                 IndexPartNode *row_src, 
                                 FieldSpaceNode *col_src,
                                 RegionTreeForest *ctx)
      : RegionTreeNode(ctx, col_src), handle(p), 
        parent(par), row_source(row_src)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PartitionNode::PartitionNode(const PartitionNode &rhs)
      : RegionTreeNode(NULL, NULL), handle(LogicalPartition::NO_PART),
        parent(NULL), row_source(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    PartitionNode::~PartitionNode(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    PartitionNode& PartitionNode::operator=(const PartitionNode &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* PartitionNode::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<PartitionNode,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void PartitionNode::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::has_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock,1,false/*exclusive*/);
      return (color_map.find(c) != color_map.end());
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::has_color(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // Ask the row source because it eagerly instantiates
      return row_source->has_child(c);
    }

    //--------------------------------------------------------------------------
    RegionNode* PartitionNode::get_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // check to see if we have it, if not try to make it
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        std::map<ColorPoint,RegionNode*>::const_iterator finder = 
          color_map.find(c);
        if (finder != color_map.end())
          return finder->second;
      }
      // If we get here we didn't immediately have it so try
      // to make it through the proper channels
      IndexSpaceNode *index_node = row_source->get_child(c);
#ifdef DEBUG_HIGH_LEVEL
      assert(index_node != NULL);
#endif
      LogicalRegion reg_handle(handle.tree_id, index_node->handle,
                               handle.field_space);
      return context->create_node(reg_handle, this);
    }

    //--------------------------------------------------------------------------
    void PartitionNode::add_child(RegionNode *child)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
#ifdef DEBUG_HIGH_LEVEL
      assert(color_map.find(child->row_source->color) == color_map.end());
#endif
      color_map[child->row_source->color] = child;
      valid_map[child->row_source->color] = child;
    }

    //--------------------------------------------------------------------------
    void PartitionNode::remove_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      valid_map.erase(c);
    }

    //--------------------------------------------------------------------------
    void PartitionNode::add_creation_source(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      AutoLock n_lock(node_lock);
      creation_set.add(source);
    }

    //--------------------------------------------------------------------------
    void PartitionNode::destroy_node(AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        parent->remove_child(row_source->color);
      AutoLock n_lock(node_lock);
      destruction_set.add(source);
    }

    //--------------------------------------------------------------------------
    unsigned PartitionNode::get_depth(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->depth;
    }

    //--------------------------------------------------------------------------
    const ColorPoint& PartitionNode::get_color(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->color;
    }

    //--------------------------------------------------------------------------
    IndexTreeNode* PartitionNode::get_row_source(void) const
    //--------------------------------------------------------------------------
    {
      return row_source;
    }

    //--------------------------------------------------------------------------
    RegionTreeID PartitionNode::get_tree_id(void) const
    //--------------------------------------------------------------------------
    {
      return handle.get_tree_id();
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* PartitionNode::get_parent(void) const
    //--------------------------------------------------------------------------
    {
      return parent;
    }

    //--------------------------------------------------------------------------
    RegionTreeNode* PartitionNode::get_tree_child(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      return get_child(c);
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::are_children_disjoint(const ColorPoint &c1, 
                                              const ColorPoint &c2)
    //--------------------------------------------------------------------------
    {
      return row_source->are_disjoint(c1, c2);
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::are_all_children_disjoint(void)
    //--------------------------------------------------------------------------
    {
      return row_source->is_disjoint();
    }

    //--------------------------------------------------------------------------
    void PartitionNode::instantiate_children(void)
    //--------------------------------------------------------------------------
    {
      std::set<ColorPoint> all_colors;
      row_source->get_colors(all_colors);
      // This may look like it does nothing, but it checks to see
      // if we have instantiated all the child nodes
      for (std::set<ColorPoint>::const_iterator it = all_colors.begin(); 
            it != all_colors.end(); it++)
        get_child(*it);
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::is_region(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    RegionNode* PartitionNode::as_region_node(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    PartitionNode* PartitionNode::as_partition_node(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<PartitionNode*>(this);
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::visit_node(PathTraverser *traverser)
    //--------------------------------------------------------------------------
    {
      return traverser->visit_partition(this);
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::visit_node(NodeTraverser *traverser)
    //--------------------------------------------------------------------------
    {
      bool continue_traversal = traverser->visit_partition(this);
      if (continue_traversal)
      {
        const bool break_early = traverser->break_early();
        if (traverser->force_instantiation)
        {
          for (Domain::DomainPointIterator itr(row_source->color_space); 
                itr; itr++)
          {
            ColorPoint child_color(itr.p);
            bool result = get_child(child_color)->visit_node(traverser);
            continue_traversal = continue_traversal && result;
            if (!result && break_early)
              break;
          }
        }
        else
        {
          std::map<ColorPoint,RegionNode*> children;
          // Need to hold the lock when reading from 
          // the color map or the valid map
          if (traverser->visit_only_valid())
          {
            AutoLock n_lock(node_lock,1,false/*exclusive*/);
            children = valid_map;
          }
          else
          {
            AutoLock n_lock(node_lock,1,false/*exclusive*/);
            children = color_map;
          }
          for (std::map<ColorPoint,RegionNode*>::const_iterator it = 
                children.begin(); it != children.end(); it++)
          {
            bool result = it->second->visit_node(traverser);
            continue_traversal = continue_traversal && result;
            if (!result && break_early)
              break;
          }
        }
      }
      return continue_traversal;
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::has_component_domains(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return parent->has_component_domains();    
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& PartitionNode::get_component_domains_blocking(
                                                                     void) const
    //--------------------------------------------------------------------------
    {
 #ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return parent->get_component_domains_blocking();     
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& PartitionNode::get_component_domains(
                                                             Event &ready) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return parent->get_component_domains(ready);
    }

    //--------------------------------------------------------------------------
    const Domain& PartitionNode::get_domain_blocking(void) const
    //--------------------------------------------------------------------------
    {
 #ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif     
      return parent->get_domain_blocking();
    }

    //--------------------------------------------------------------------------
    const Domain& PartitionNode::get_domain(Event &precondition) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return parent->get_domain(precondition);
    }

    //--------------------------------------------------------------------------
    const Domain& PartitionNode::get_domain_no_wait(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return parent->get_domain_no_wait();
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::is_complete(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return row_source->is_complete();
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::intersects_with(RegionTreeNode *other)
    //--------------------------------------------------------------------------
    {
      if (other->is_region())
        return row_source->intersects_with(
                    other->as_region_node()->row_source);
      else
        return row_source->intersects_with(
                    other->as_partition_node()->row_source);
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::dominates(RegionTreeNode *other)
    //--------------------------------------------------------------------------
    {
      if (other->is_region())
        return row_source->dominates(other->as_region_node()->row_source);
      else
        return row_source->dominates(other->as_partition_node()->row_source);
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& PartitionNode::get_intersection_domains(
                                                          RegionTreeNode *other)
    //--------------------------------------------------------------------------
    {
      if (other->is_region())
        return row_source->get_intersection_domains(
                                           other->as_region_node()->row_source);
      else
        return row_source->get_intersection_domains(
                                        other->as_partition_node()->row_source);
    }

    //--------------------------------------------------------------------------
    size_t PartitionNode::get_num_children(void) const
    //--------------------------------------------------------------------------
    {
      return row_source->get_num_children();
    }

    //--------------------------------------------------------------------------
    InterCloseOp* PartitionNode::create_close_op(Operation *creator,
                                                 const FieldMask &closing_mask,
                                                 bool leave_open,
                                            const std::set<ColorPoint> &targets,
                                                 const VersionInfo &close_info,
                                                 const VersionInfo &ver_info,
                                                 const RestrictInfo &res_info,
                                                 const TraceInfo &trace_info)
    //--------------------------------------------------------------------------
    {
      InterCloseOp *op = context->runtime->get_available_inter_close_op(false);
      // Construct a region requirement for this operation
      // Make it a projection requirement so we walk to a partition
      RegionRequirement req(handle, 0/*projection id */,
                            READ_WRITE, EXCLUSIVE, trace_info.req.parent);
      // Compute the set of fields that we need
      column_source->get_field_set(closing_mask, 
                                   trace_info.req.privilege_fields,
                                   req.privilege_fields);
      // Now initialize the operation
      op->initialize(creator->get_parent(), req, targets, leave_open, 
                     trace_info.trace, trace_info.req_idx, 
                     close_info, ver_info, res_info, closing_mask, creator);
      return op;
    }

    //--------------------------------------------------------------------------
    bool PartitionNode::perform_close_operation(const MappableInfo &info,
                                                const FieldMask &closing_mask,
                                            const std::set<ColorPoint> &targets,
                                                const MappingRef &target_reg,
                                                VersionInfo &version_info,
                                                bool leave_open, 
                                      const std::set<ColorPoint> &next_children,
                                                Event &closed,
                                                bool &create_composite)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!create_composite); // should always start off this way
#endif
      MaterializedView *target_view = NULL;
      if (target_reg.has_ref())
      {
        LogicalView *view = target_reg.get_view();
#ifdef DEBUG_HIGH_LEVEL
        assert(view->is_instance_view());
#endif
        InstanceView *inst_view = view->as_instance_view();
#ifdef DEBUG_HIGH_LEVEL
        assert(inst_view->is_materialized_view());
#endif
        target_view = inst_view->as_materialized_view();
      }
      // Handle a special case here: if the node we're closing is a partition
      // and we're permitted to leave the partition open, then don't actually
      // close the partition. Instead close to the individual target child
      // that we are trying to close to. This handles the case of leaving 
      // many children in a read-only partition open. Only safe to do
      // this if all the children are disjoint.
      bool success = true;
      if (leave_open && !targets.empty() && row_source->is_disjoint())
      {
        for (std::set<ColorPoint>::const_iterator it = targets.begin();   
              it != targets.end(); it++)
        {
          RegionNode *child_node = get_child(*it);
          PhysicalCloser child_closer(info, true/*leave open*/, 
                                      child_node->handle);
          if (target_view != NULL)
            child_closer.add_target(target_view->get_materialized_subview(*it));
          PhysicalState *child_state = 
            child_node->get_physical_state(info.ctx, version_info);
          std::set<ColorPoint> empty_next_children;
          bool result = child_node->siphon_physical_children(child_closer,
                                             child_state, closing_mask,
                                             empty_next_children,
                                             create_composite);
          // If we succeeded, then update the views
          if (!result || create_composite)
          {
            success = false;
            break;
          }
          else
          {
            child_closer.update_node_views(child_node, child_state);
            closed = child_closer.get_termination_event();
          }
        }
      }
      else
      {
        // Otherwise we are trying to close up this whole partition
        // Close it up to our parent region
        PhysicalCloser closer(info, leave_open, parent->handle);
        if (target_view != NULL)
          closer.add_target(target_view);
        bool changed = false;
        PhysicalState *state = get_physical_state(info.ctx, version_info); 
        for (std::set<ColorPoint>::const_iterator it = targets.begin(); 
              it != targets.end(); it++)
        {
          LegionMap<ColorPoint,FieldMask>::aligned::iterator finder = 
            state->children.open_children.find(*it);
          if (finder == state->children.open_children.end())
            continue;
          bool result = close_physical_child(closer, state, closing_mask,
                                             (*it), finder->second, 
                                             next_children,
                                             create_composite, changed);
          if (!result || create_composite)
          {
            success = false;
            break;
          }
          if (!finder->second)
            state->children.open_children.erase(finder);
        }
        // If anything changed, rebuild the field mask
        {
          FieldMask next_valid;
          for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it = 
                state->children.open_children.begin(); it !=
                state->children.open_children.end(); it++)
          {
            next_valid |= it->second;
          }
          state->children.valid_fields = next_valid;
        }
        // If we succeed, update the physical instance views
        if (success)
        {
          closer.update_node_views(this, state);
          // No need to check for flushed reductions, nobody can be
          // reducing directly to a partition object anyway
          closed = closer.get_termination_event();
        }
      }
      return success;
    }

    //--------------------------------------------------------------------------
    MaterializedView* PartitionNode::create_instance(Memory target_mem,
                                                const std::set<FieldID> &fields,
                                                size_t blocking_factor,
                                                unsigned depth, Operation *op)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      MaterializedView *result = parent->create_instance(target_mem, 
                                                         fields, 
                                                         blocking_factor,
                                                         depth, op);
      if (result != NULL)
      {
        result = result->get_materialized_subview(row_source->color);
      }
      return result;
    }

    //--------------------------------------------------------------------------
    ReductionView* PartitionNode::create_reduction(Memory target_mem, 
                                            FieldID fid, bool reduction_list,
                                            ReductionOpID redop, Operation *op)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(parent != NULL);
#endif
      return parent->create_reduction(target_mem, fid, 
                                      reduction_list, redop, op);
    }

    //--------------------------------------------------------------------------
    void PartitionNode::send_node(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      // Check to see if we have it in our creation set in which
      // case we are done otherwise keep going up
      bool continue_up = false;
      bool send_deletion = false;
      {
        AutoLock n_lock(node_lock); 
        if (!creation_set.contains(target))
        {
          continue_up = true;
          creation_set.add(target);
        }
        if (!destruction_set.contains(target))
        {
          send_deletion = true;
          destruction_set.add(target);
        }
      }
      if (continue_up)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(parent != NULL);
#endif
        // Send the parent node first
        parent->send_node(target);
        AutoLock n_lock(node_lock);
        for (LegionMap<SemanticTag,SemanticInfo>::aligned::iterator it = 
              semantic_info.begin(); it != semantic_info.end(); it++)
        {
          it->second.node_mask.add(target);
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(handle);
            rez.serialize(it->first);
            rez.serialize(it->second.node_mask);
            rez.serialize(it->second.size);
            rez.serialize(it->second.buffer, it->second.size);
          }
          context->runtime->send_logical_partition_semantic_info(target, rez);
        }
      }
      if (send_deletion)
      {
        context->runtime->send_logical_partition_destruction(handle, target);
      }
    }

    //--------------------------------------------------------------------------
    void PartitionNode::send_semantic_info(const NodeSet &targets,
                                           SemanticTag tag,
                                           const void *buffer, size_t size,
                                           const NodeSet &current)
    //--------------------------------------------------------------------------
    {
      // Package up the message first
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(handle);
        rez.serialize(tag);
        rez.serialize(current);
        rez.serialize(size);
        rez.serialize(buffer, size);
      }
      // Then send the messages
      SendSemanticInfoFunctor<LOGICAL_PARTITION_SEMANTIC>
                                            functor(context->runtime, rez);
      targets.map(functor);
    }

    //--------------------------------------------------------------------------
    /*static*/ void PartitionNode::handle_semantic_info(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      LogicalPartition handle;
      derez.deserialize(handle);
      SemanticTag tag;
      derez.deserialize(tag);
      NodeSet sources;
      derez.deserialize(sources);
      size_t size;
      derez.deserialize(size);
      const void *buffer = derez.get_current_pointer();
      derez.advance_pointer(size);
      forest->attach_semantic_information(handle, tag, sources, buffer, size);
    }

    //--------------------------------------------------------------------------
    void PartitionNode::print_logical_context(ContextID ctx,
                                              TreeStateLogger *logger,
                                              const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), row_source->is_disjoint(), 
              logger->get_depth());
            break;
          }
        case 1:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint %d at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->is_disjoint(), 
              logger->get_depth());
            break;
          }
        case 2:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d) "
                        "disjoint %d at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], 
              row_source->is_disjoint(), logger->get_depth());
            break;
          }
        case 3:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d,%d) "
                        "disjoint at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], 
              row_source->is_disjoint(), logger->get_depth());
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (logical_states.has_entry(ctx))
      {
        LogicalState &state = get_logical_state(ctx);
        print_logical_state(state, capture_mask, to_traverse, logger);    
      }
      else
      {
        logger->log("No state");
      }
      logger->log("");
      if (!to_traverse.empty())
      {
        AutoLock n_lock(node_lock,1,false/*exclusive*/);
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,RegionNode*>::const_iterator finder = 
            color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->print_logical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }

    //--------------------------------------------------------------------------
    void PartitionNode::print_physical_context(ContextID ctx,
                                               TreeStateLogger *logger,
                                               const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), 
              row_source->is_disjoint(), logger->get_depth());
            break;
          }
        case 1:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint %d at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->is_disjoint(), logger->get_depth());
            break;
          }
        case 2:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d) "
                        "disjoint %d at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], 
              row_source->is_disjoint(), logger->get_depth());
            break;
          }
        case 3:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d,%d) "
                        "disjoint at depth %d", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], 
              row_source->is_disjoint(), logger->get_depth());
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (version_managers.has_entry(ctx))
      {
        VersionManager *manager = version_managers.lookup_entry(ctx, this);
        manager->print_physical_state(this, capture_mask, to_traverse, logger);
      }
      else
      {
        logger->log("No state");
      }
      logger->log("");
      if (!to_traverse.empty())
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,RegionNode*>::const_iterator 
            finder = color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->print_physical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }

    //--------------------------------------------------------------------------
    void PartitionNode::print_logical_state(LogicalState &state,
                                        const FieldMask &capture_mask,
                   LegionMap<ColorPoint,FieldMask>::aligned &to_traverse,
                                        TreeStateLogger *logger)
    //--------------------------------------------------------------------------
    {
      // Open Field States
      {
        logger->log("Open Field States (%ld)", state.field_states.size()); 
        logger->down();
        for (std::list<FieldState>::const_iterator it = 
              state.field_states.begin(); it != 
              state.field_states.end(); it++)
        {
          it->print_state(logger, capture_mask);
          if (it->valid_fields * capture_mask)
            continue;
          for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator 
                cit = it->open_children.begin(); cit != 
                it->open_children.end(); cit++)
          {
            FieldMask overlap = cit->second & capture_mask;
            if (!overlap)
              continue;
            if (to_traverse.find(cit->first) == to_traverse.end())
              to_traverse[cit->first] = overlap;
            else
              to_traverse[cit->first] |= overlap;
          }
        }
        logger->up();
      }
    }

#ifdef DEBUG_HIGH_LEVEL
    //--------------------------------------------------------------------------
    void PartitionNode::dump_logical_context(ContextID ctx,
                                             TreeStateLogger *logger,
                                             const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        case 1:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint %d at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        case 2:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d) "
                        "disjoint %d at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        case 3:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d,%d) "
                        "disjoint at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (logical_states.has_entry(ctx))
      {
        LogicalState &state = get_logical_state(ctx);
        print_logical_state(state, capture_mask, to_traverse, logger);
      }
      else
      {
        logger->log("No state");
      }
      logger->log("");
      if (!to_traverse.empty())
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,RegionNode*>::const_iterator finder = 
            color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->dump_logical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }

    //--------------------------------------------------------------------------
    void PartitionNode::dump_physical_context(ContextID ctx,
                                              TreeStateLogger *logger,
                                              const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      switch (row_source->color.get_dim())
      {
        case 0:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color.get_index(), row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        case 1:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color %d "
                        "disjoint %d at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        case 2:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d) "
                        "disjoint %d at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], 
              row_source->color[1], row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        case 3:
          {
            logger->log("Partition Node (" IDFMT ",%d,%d) Color (%d,%d,%d) "
                        "disjoint at depth %d (%p)", 
              handle.index_partition.id, handle.field_space.id,handle.tree_id,
              row_source->color[0], row_source->color[2],
              row_source->color[2], row_source->is_disjoint(), 
              logger->get_depth(), this);
            break;
          }
        default:
          assert(false);
      }
      logger->down();
      LegionMap<ColorPoint,FieldMask>::aligned to_traverse;
      if (version_managers.has_entry(ctx))
      {
        VersionManager *manager = version_managers.lookup_entry(ctx, this);
        manager->print_physical_state(this, capture_mask, to_traverse, logger);
      }
      else
      {
        logger->log("No state");
      }
      logger->log("");
      if (!to_traverse.empty())
      {
        for (LegionMap<ColorPoint,FieldMask>::aligned::const_iterator it =
              to_traverse.begin(); it != to_traverse.end(); it++)
        {
          std::map<ColorPoint,RegionNode*>::const_iterator finder = 
            color_map.find(it->first);
          if (finder != color_map.end())
            finder->second->dump_physical_context(ctx, logger, it->second);
        }
      }
      logger->up();
    }
#endif

    /////////////////////////////////////////////////////////////
    // RegionTreePath 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    RegionTreePath::RegionTreePath(void) 
      : min_depth(0), max_depth(0)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    void RegionTreePath::initialize(unsigned min, unsigned max)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(min <= max);
#endif
      min_depth = min;
      max_depth = max;
      path.resize(max_depth+1);
    }

    //--------------------------------------------------------------------------
    void RegionTreePath::register_child(unsigned depth, 
                                        const ColorPoint &color)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(min_depth <= depth);
      assert(depth <= max_depth);
#endif
      path[depth] = color;
    }

    //--------------------------------------------------------------------------
    void RegionTreePath::clear(void)
    //--------------------------------------------------------------------------
    {
      path.clear();
      min_depth = 0;
      max_depth = 0;
    }

    //--------------------------------------------------------------------------
    bool RegionTreePath::has_child(unsigned depth) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(min_depth <= depth);
      assert(depth <= max_depth);
#endif
      return path[depth].is_valid();
    }

    //--------------------------------------------------------------------------
    const ColorPoint& RegionTreePath::get_child(unsigned depth) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(min_depth <= depth);
      assert(depth <= max_depth);
      assert(has_child(depth));
#endif
      return path[depth];
    }

    //--------------------------------------------------------------------------
    unsigned RegionTreePath::get_path_length(void) const
    //--------------------------------------------------------------------------
    {
      return ((max_depth-min_depth)+1); 
    }

    /////////////////////////////////////////////////////////////
    // FatTreePath 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    FatTreePath::FatTreePath(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    FatTreePath::FatTreePath(const FatTreePath &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    FatTreePath::~FatTreePath(void)
    //--------------------------------------------------------------------------
    {
      // Delete all our children
      for (std::map<ColorPoint,FatTreePath*>::const_iterator it = 
            children.begin(); it != children.end(); it++)
      {
        delete it->second;
      }
      children.clear();
    }

    //--------------------------------------------------------------------------
    FatTreePath& FatTreePath::operator=(const FatTreePath &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void FatTreePath::add_child(const ColorPoint &child_color, 
                                FatTreePath *child)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(children.find(child_color) == children.end());
#endif
      children[child_color] = child;
    }

    //--------------------------------------------------------------------------
    bool FatTreePath::add_child(const ColorPoint &child_color,
                                FatTreePath *child, IndexTreeNode *tree_node)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(children.find(child_color) == children.end());
#endif
      bool overlap = false;
      if (!children.empty())
      {
        if (tree_node->is_index_space_node())
        {
          IndexSpaceNode *node = tree_node->as_index_space_node();
          for (std::map<ColorPoint,FatTreePath*>::const_iterator it = 
                children.begin(); it != children.end(); it++)
          {
            if ((it->first == child_color) || 
                !node->are_disjoint(it->first, child_color))
            {
              overlap = true;
              break;
            }
          }
        }
        else
        {
          IndexPartNode *node = tree_node->as_index_part_node();
          for (std::map<ColorPoint,FatTreePath*>::const_iterator it = 
                children.begin(); it != children.end(); it++)
          {
            if ((it->first == child_color) || 
                !node->are_disjoint(it->first, child_color))
            {
              overlap = true;
              break;
            }
          }
        }
      }
      children[child_color] = child;
      return overlap;
    }

    /////////////////////////////////////////////////////////////
    // Layout Description 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LayoutDescription::LayoutDescription(const FieldMask &mask, const Domain &d,
                                         size_t bf, FieldSpaceNode *own)
      : allocated_fields(mask), blocking_factor(bf), 
        volume(compute_layout_volume(d)), owner(own)
    //--------------------------------------------------------------------------
    {
      layout_lock = Reservation::create_reservation();
    }

    //--------------------------------------------------------------------------
    LayoutDescription::LayoutDescription(const LayoutDescription &rhs)
      : allocated_fields(rhs.allocated_fields), 
        blocking_factor(rhs.blocking_factor), 
        volume(rhs.volume), owner(rhs.owner)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    LayoutDescription::~LayoutDescription(void)
    //--------------------------------------------------------------------------
    {
      memoized_offsets.clear();
      layout_lock.destroy_reservation();
      layout_lock = Reservation::NO_RESERVATION;
    }

    //--------------------------------------------------------------------------
    LayoutDescription& LayoutDescription::operator=(
                                                   const LayoutDescription &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* LayoutDescription::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<LayoutDescription,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::compute_copy_offsets(const FieldMask &copy_mask,
                                                 PhysicalInstance instance,
                                   std::vector<Domain::CopySrcDstField> &fields)
    //--------------------------------------------------------------------------
    {
      uint64_t hash_key = copy_mask.get_hash_key();
      size_t added_offset_count = 0;
      bool found_in_cache = false;
      // First check to see if we've memoized this result 
      {
        AutoLock o_lock(layout_lock,1,false/*exclusive*/);
        std::map<FIELD_TYPE,LegionVector<OffsetEntry>::aligned >::const_iterator
          finder = memoized_offsets.find(hash_key);
        if (finder != memoized_offsets.end())
        {
          for (LegionVector<OffsetEntry>::aligned::const_iterator it = 
                finder->second.begin(); it != finder->second.end(); it++)
          {
            if (it->offset_mask == copy_mask)
            {
              fields.insert(fields.end(),it->offsets.begin(),it->offsets.end());
              found_in_cache = true;
              added_offset_count = it->offsets.size();
              break;
            }
          }
        }
      }
      if (found_in_cache)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert(added_offset_count <= fields.size());
#endif
        // Go through and fill in all the annonymous instances
        for (unsigned idx = fields.size() - added_offset_count;
              idx < fields.size(); idx++)
        {
          fields[idx].inst = instance;
        }
        // Now we're done
        return;
      }
      // It is absolutely imperative that these infos be added in
      // the order in which they appear in the field mask so that 
      // they line up in the same order with the source/destination infos
      // (depending on the calling context of this function)
#ifdef DEBUG_HIGH_LEVEL
      int pop_count = 0;
#endif
      std::vector<Domain::CopySrcDstField> local;
      for (std::map<unsigned,FieldID>::const_iterator it = 
            field_indexes.begin(); it != field_indexes.end(); it++)
      {
        if (copy_mask.is_set(it->first))
        {
          std::map<FieldID,Domain::CopySrcDstField>::const_iterator finder = 
            field_infos.find(it->second);
#ifdef DEBUG_HIGH_LEVEL
          assert(finder != field_infos.end());
          pop_count++;
#endif
          fields.push_back(finder->second);
          // Because instances are annonymous in layout descriptions
          // we have to fill them in as we add them to fields
          fields.back().inst = instance;
          local.push_back(finder->second);
        }
      }
#ifdef DEBUG_HIGH_LEVEL
      // Make sure that we added exactly the number of infos as
      // there were fields set in the bit mask
      assert(pop_count == FieldMask::pop_count(copy_mask));
#endif
      // Add this to the results
      AutoLock o_lock(layout_lock);
      std::map<FIELD_TYPE,LegionVector<OffsetEntry>::aligned >::iterator
        finder = memoized_offsets.find(hash_key);
      if (finder == memoized_offsets.end())
        memoized_offsets[hash_key].push_back(OffsetEntry(copy_mask,local));
      else
        finder->second.push_back(OffsetEntry(copy_mask,local));
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::compute_copy_offsets(
                                   const std::vector<FieldID> &copy_fields, 
                                   PhysicalInstance instance,
                                   std::vector<Domain::CopySrcDstField> &fields)
    //--------------------------------------------------------------------------
    {
      for (std::vector<FieldID>::const_iterator it = copy_fields.begin();
            it != copy_fields.end(); it++)
      {
        std::map<FieldID,Domain::CopySrcDstField>::const_iterator
          finder = field_infos.find(*it);
#ifdef DEBUG_HIGH_LEVEL
        assert(finder != field_infos.end());
#endif
        fields.push_back(finder->second);
        // Since instances are annonymous in layout descriptions we
        // have to fill them in when we add the field info
        fields.back().inst = instance;
      }
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::add_field_info(FieldID fid, unsigned index,
                                           size_t offset, size_t field_size)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(field_infos.find(fid) == field_infos.end());
      assert(field_indexes.find(index) == field_indexes.end());
#endif
      // Use annonymous instances when creating these field infos since
      // we specifying layouts independently of any one instance
      field_infos[fid] = Domain::CopySrcDstField(PhysicalInstance::NO_INST,
                                                 offset, field_size);
      field_indexes[index] = fid;
#ifdef DEBUG_HIGH_LEVEL
      assert(offset_size_map.find(offset) == offset_size_map.end());
#endif
      offset_size_map[offset] = field_size;
    }

    //--------------------------------------------------------------------------
    const Domain::CopySrcDstField& LayoutDescription::find_field_info(
                                                              FieldID fid) const
    //--------------------------------------------------------------------------
    {
      std::map<FieldID,Domain::CopySrcDstField>::const_iterator finder = 
        field_infos.find(fid);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != field_infos.end());
#endif
      return finder->second;
    }

    //--------------------------------------------------------------------------
    size_t LayoutDescription::get_layout_size(void) const
    //--------------------------------------------------------------------------
    {
      size_t result = 0;
      // Add up all the field sizes
      for (std::map<FieldID,Domain::CopySrcDstField>::const_iterator it = 
            field_infos.begin(); it != field_infos.end(); it++)
      {
        result += (it->second.size);
      }
      result *= volume;
      return result;
    }

    //--------------------------------------------------------------------------
    bool LayoutDescription::match_shape(const size_t field_size) const
    //--------------------------------------------------------------------------
    {
      if (field_infos.size() != 1)
        return false;
      if (field_infos.begin()->second.size != field_size)
        return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool LayoutDescription::match_shape(const std::vector<size_t> &field_sizes,
                                        const size_t bf) const
    //--------------------------------------------------------------------------
    {
      if (field_sizes.size() != field_infos.size())
        return false;
      if (blocking_factor != bf)
        return false;
      unsigned offset = 0;
      for (std::vector<size_t>::const_iterator it = field_sizes.begin();
            it != field_sizes.end(); it++)
      {
        std::map<unsigned,unsigned>::const_iterator finder = 
          offset_size_map.find(offset);
        // Check to make sure we have the same offset
        if (finder == offset_size_map.end())
          return false;
        // Check that the sizes are the same for the offset
        if (finder->second != (*it))
          return false;
        offset += (*it);
      }
      return true;
    }

    //--------------------------------------------------------------------------
    bool LayoutDescription::match_layout(const FieldMask &mask,
                                         const size_t vl, const size_t bf) const
    //--------------------------------------------------------------------------
    {
      if (blocking_factor != bf)
        return false;
      if (volume != vl)
        return false;
      if (allocated_fields != mask)
        return false;
      return true;
    }

    //--------------------------------------------------------------------------
    bool LayoutDescription::match_layout(const FieldMask &mask, const Domain &d,
                                         const size_t bf) const
    //--------------------------------------------------------------------------
    {
      return match_layout(mask, compute_layout_volume(d), bf);
    }

    //--------------------------------------------------------------------------
    bool LayoutDescription::match_layout(LayoutDescription *rhs) const
    //--------------------------------------------------------------------------
    {
      return match_layout(rhs->allocated_fields, rhs->volume, 
                          rhs->blocking_factor);
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::set_descriptor(FieldDataDescriptor &desc,
                                           unsigned fid_idx) const
    //--------------------------------------------------------------------------
    {
      std::map<unsigned,FieldID>::const_iterator idx_finder = 
        field_indexes.find(fid_idx);
#ifdef DEBUG_HIGH_LEVEL
      assert(idx_finder != field_indexes.end());
#endif
      std::map<FieldID,Domain::CopySrcDstField>::const_iterator finder = 
        field_infos.find(idx_finder->second);
#ifdef DEBUG_HIGH_LEVEL
      assert(finder != field_infos.end());
#endif
      desc.field_offset = finder->second.offset;
      desc.field_size = finder->second.size;
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::pack_layout_description(Serializer &rez,
                                                    AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      RezCheck z(rez);
      // Do a quick check to see if the target already has the layout
      // We don't need to hold a lock here since if we lose the race
      // we will just send the layout twice and everything will be
      // resolved on the far side
      if (known_nodes.contains(target))
      {
        rez.serialize<bool>(true);
        // If it is already on the remote node, then we only
        // need to the necessary information to identify it
        rez.serialize(allocated_fields);
        rez.serialize(blocking_factor);
      }
      else
      {
        rez.serialize<bool>(false);
        rez.serialize(allocated_fields);
        rez.serialize(blocking_factor);
        rez.serialize<size_t>(field_infos.size());
#ifdef DEBUG_HIGH_LEVEL
        assert(field_infos.size() == field_indexes.size());
#endif
        for (std::map<unsigned,FieldID>::const_iterator it = 
              field_indexes.begin(); it != field_indexes.end(); it++)
        {
          std::map<FieldID,Domain::CopySrcDstField>::const_iterator finder = 
            field_infos.find(it->second);
#ifdef DEBUG_HIGH_LEVEL
          assert(finder != field_infos.end());
#endif
          rez.serialize(it->second);
          rez.serialize(finder->second.offset);
          rez.serialize(finder->second.size);
        }
      }
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::unpack_layout_description(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_fields;
      derez.deserialize(num_fields);
      for (unsigned idx = 0; idx < num_fields; idx++)
      {
        FieldID fid;
        derez.deserialize(fid);
        unsigned index = owner->get_field_index(fid);
        field_indexes[index] = fid;
        unsigned offset, size;
        derez.deserialize(offset);
        derez.deserialize(size);
        field_infos[fid] = 
          Domain::CopySrcDstField(PhysicalInstance::NO_INST, offset, size);
#ifdef DEBUG_HIGH_LEVEL
        assert(offset_size_map.find(offset) == offset_size_map.end());
#endif
        offset_size_map[offset] = size;
      }
    }

    //--------------------------------------------------------------------------
    void LayoutDescription::update_known_nodes(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      // Hold the lock to get serial access to this data structure
      AutoLock l_lock(layout_lock);
      known_nodes.add(target);
    }

    //--------------------------------------------------------------------------
    /*static*/ LayoutDescription* LayoutDescription::
      handle_unpack_layout_description(Deserializer &derez,
                                 AddressSpaceID source, RegionNode *region_node)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      bool has_local;
      derez.deserialize(has_local);
      FieldSpaceNode *field_space_node = region_node->column_source;
      LayoutDescription *result = NULL;
      FieldMask mask;
      derez.deserialize(mask);
      field_space_node->transform_field_mask(mask, source);
      size_t blocking_factor;
      derez.deserialize(blocking_factor);
      if (has_local)
      {
        // If we have a local layout, then we should be able to find it
        result = field_space_node->find_layout_description(mask,  
                                            region_node->get_domain_blocking(),
                                            blocking_factor);
      }
      else
      {
        // Otherwise create a new layout description, 
        // unpack it, and then try registering it with
        // the field space node
        result = new LayoutDescription(mask, region_node->get_domain_blocking(),
                                       blocking_factor, field_space_node);
        result->unpack_layout_description(derez);
        result = field_space_node->register_layout_description(result);
      }
#ifdef DEBUG_HIGH_LEVEL
      assert(result != NULL);
#endif
      // Record that the sender already has this layout
      // Only do this after we've registered the instance
      result->update_known_nodes(source);
      return result;
    }

    //--------------------------------------------------------------------------
    /*static*/ size_t LayoutDescription::compute_layout_volume(const Domain &d)
    //--------------------------------------------------------------------------
    {
      if (d.get_dim() == 0)
      {
        const LowLevel::ElementMask &mask = 
          d.get_index_space().get_valid_mask();
        return mask.get_num_elmts();
      }
      else
        return d.get_volume();
    }

    /////////////////////////////////////////////////////////////
    // PhysicalManager 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    PhysicalManager::PhysicalManager(RegionTreeForest *ctx, DistributedID did,
                                     AddressSpaceID owner_space,
                                     AddressSpaceID local_space,
                                     Memory mem, RegionNode *node,
                                     PhysicalInstance inst, bool register_now)
      : DistributedCollectable(ctx->runtime, did, owner_space, 
                               local_space, register_now), 
        context(ctx), memory(mem), region_node(node), instance(inst)
    //--------------------------------------------------------------------------
    {
      if (register_now)
        region_node->register_physical_manager(this);
      // If we are not the owner, add a resource reference
      if (!is_owner())
        add_base_resource_ref(REMOTE_DID_REF);
    }

    //--------------------------------------------------------------------------
    PhysicalManager::~PhysicalManager(void)
    //--------------------------------------------------------------------------
    {
      region_node->unregister_physical_manager(this);
      // If we're the owner remove our resource references
      if (is_owner())
      {
        UpdateReferenceFunctor<RESOURCE_REF_KIND,false/*add*/> functor(this);
        map_over_remote_instances(functor);
      }
      if (is_owner() && instance.exists())
      {
        log_leak.warning("Leaking physical instance " IDFMT " in memory"
                               IDFMT "",
                               instance.id, memory.id);
      }
    }

    //--------------------------------------------------------------------------
    void PhysicalManager::notify_active(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      if (is_owner())
        assert(instance.exists());
#endif
      // If we are not the owner, send a reference
      if (!is_owner())
        send_remote_gc_update(owner_space, 1/*count*/, true/*add*/);
    }

    //--------------------------------------------------------------------------
    void PhysicalManager::notify_valid(void)
    //--------------------------------------------------------------------------
    {
      // No need to do anything
#ifdef DEBUG_HIGH_LEVEL
      if (is_owner())
        assert(instance.exists());
#endif
      // If we are not the owner, send a reference
      if (!is_owner())
        send_remote_valid_update(owner_space, 1/*count*/, true/*add*/);
    }

    /////////////////////////////////////////////////////////////
    // InstanceManager 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    InstanceManager::InstanceManager(RegionTreeForest *ctx, DistributedID did,
                                     AddressSpaceID owner_space, 
                                     AddressSpaceID local_space,
                                     Memory mem, PhysicalInstance inst,
                                     RegionNode *node, LayoutDescription *desc, 
                                     Event u_event, unsigned dep, 
                                     bool reg_now, InstanceFlag flags)
      : PhysicalManager(ctx, did, owner_space, local_space, mem, 
                        node, inst, reg_now), layout(desc), use_event(u_event), 
        depth(dep), instance_flags(flags)
    //--------------------------------------------------------------------------
    {
      // Tell the runtime so it can update the per memory data structures
      context->runtime->allocate_physical_instance(this);
      // Add a reference to the layout
      layout->add_reference();
#ifdef LEGION_GC
      log_garbage.info("GC Instance Manager %ld " IDFMT " " IDFMT " ",
                        did, inst.id, mem.id);
#endif
    }

    //--------------------------------------------------------------------------
    InstanceManager::InstanceManager(const InstanceManager &rhs)
      : PhysicalManager(NULL, 0, 0, 0, Memory::NO_MEMORY,
                        NULL, PhysicalInstance::NO_INST, false), 
        layout(NULL), use_event(Event::NO_EVENT), depth(0)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    InstanceManager::~InstanceManager(void)
    //--------------------------------------------------------------------------
    {
      // Tell the runtime this instance no longer exists
      // If we were the owner we already did this when we
      // garbage collected the physical instance
      if (!is_owner())
        context->runtime->free_physical_instance(this);

      if (layout->remove_reference())
        delete layout;
    }

    //--------------------------------------------------------------------------
    InstanceManager& InstanceManager::operator=(const InstanceManager &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic>
      InstanceManager::get_accessor(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(instance.exists());
#endif
      return instance.get_accessor();
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic>
      InstanceManager::get_field_accessor(FieldID fid) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(instance.exists());
      assert(layout != NULL);
#endif
      const Domain::CopySrcDstField &info = layout->find_field_info(fid);
      Accessor::RegionAccessor<Accessor::AccessorType::Generic> temp = 
        instance.get_accessor();
      return temp.get_untyped_field_accessor(info.offset, info.size);
    }

    //--------------------------------------------------------------------------
    bool InstanceManager::is_reduction_manager(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    InstanceManager* InstanceManager::as_instance_manager(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<InstanceManager*>(this);
    }

    //--------------------------------------------------------------------------
    ReductionManager* InstanceManager::as_reduction_manager(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    size_t InstanceManager::get_instance_size(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(layout != NULL);
#endif
      return layout->get_layout_size();
    }

    //--------------------------------------------------------------------------
    void InstanceManager::notify_inactive(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, GARBAGE_COLLECT_CALL);
#endif
      if (is_owner())
      {
        // Always call this up front to see if we need to reclaim the
        // physical instance from the runtime because we recycled it.
        // Note we can do this here and not worry about a race with
        // notify_invalid because we are guaranteed they are called
        // sequentially by the state machine in the distributed
        // collectable implementation.
        //bool reclaimed = context->runtime->reclaim_physical_instance(this);
        // Now tell the runtime that this instance will no longer exist
        context->runtime->free_physical_instance(this);
        AutoLock gc(gc_lock);
#ifdef DEBUG_HIGH_LEVEL
        assert(instance.exists());
#endif
        // Do the deletion for this instance
        // If either of these conditions were true, then we
        // should actually delete the physical instance.
        log_garbage.debug("Garbage collecting physical instance " IDFMT
                              " in memory " IDFMT " in address space %d",
                              instance.id, memory.id, owner_space);
#ifdef OLD_LEGION_PROF
        LegionProf::register_instance_deletion(instance.id);
#endif
#ifndef DISABLE_GC
        instance.destroy(use_event);
#endif
        // Mark that this instance has been garbage collected
        instance = PhysicalInstance::NO_INST;
      }
      else // Remove our gc reference
        send_remote_gc_update(owner_space, 1/*count*/, false/*add*/);
    }


#ifdef DEBUG_HIGH_LEVEL
    //--------------------------------------------------------------------------
    void InstanceManager::notify_valid(void)
    //--------------------------------------------------------------------------
    {
      assert(instance.exists());
      PhysicalManager::notify_valid();
    }
#endif

    //--------------------------------------------------------------------------
    void InstanceManager::notify_invalid(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, NOTIFY_INVALID_CALL);
#endif
      if (!is_owner()) // If we are not the owner, remove our valid reference
        send_remote_valid_update(owner_space, 1/*count*/, false/*add*/);
    }

    //--------------------------------------------------------------------------
    MaterializedView* InstanceManager::create_top_view(unsigned depth)
    //--------------------------------------------------------------------------
    {
      DistributedID view_did = 
        context->runtime->get_available_distributed_id(false);
      MaterializedView *result = legion_new<MaterializedView>(context, view_did,
                                                context->runtime->address_space,
                                                context->runtime->address_space,
                                                region_node, this,
                                            ((MaterializedView*)NULL/*parent*/),
                                                depth, true/*register now*/);
      return result;
    }

    //--------------------------------------------------------------------------
    void InstanceManager::compute_copy_offsets(const FieldMask &copy_mask,
                                  std::vector<Domain::CopySrcDstField> &fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(layout != NULL);
#endif
      // Pass in our physical instance so the layout knows how to specialize
      layout->compute_copy_offsets(copy_mask, instance, fields);
    }

    //--------------------------------------------------------------------------
    void InstanceManager::compute_copy_offsets(
                                  const std::vector<FieldID> &copy_fields,
                                  std::vector<Domain::CopySrcDstField> &fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(layout != NULL);
#endif
      // Pass in our physical instance so the layout knows how to specialize
      layout->compute_copy_offsets(copy_fields, instance, fields);
    }

    //--------------------------------------------------------------------------
    void InstanceManager::set_descriptor(FieldDataDescriptor &desc,
                                         unsigned fid_idx) const
    //--------------------------------------------------------------------------
    {
      // Fill in the information about our instance
      desc.inst = instance;
      // Ask the layout to fill in the information about field offset and size
      layout->set_descriptor(desc, fid_idx);
    }

    //--------------------------------------------------------------------------
    DistributedID InstanceManager::send_manager(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      if (!has_remote_instance(target))
      {
        // No need to take the lock, duplicate sends are alright
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(owner_space);
          rez.serialize(memory);
          rez.serialize(instance);
          rez.serialize(region_node->handle);
          rez.serialize(use_event);
          rez.serialize(depth);
          rez.serialize(instance_flags);
          layout->pack_layout_description(rez, target);
        }
        context->runtime->send_instance_manager(target, rez);
        update_remote_instances(target);
        // Finally we can update our known nodes
        // It's only safe to do this after the message
        // has been sent
        layout->update_known_nodes(target);
      }
      return did;
    }

    //--------------------------------------------------------------------------
    /*static*/ void InstanceManager::handle_send_manager(Runtime *runtime, 
                                     AddressSpaceID source, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      AddressSpaceID owner_space;
      derez.deserialize(owner_space);
      Memory mem;
      derez.deserialize(mem);
      PhysicalInstance inst;
      derez.deserialize(inst);
      LogicalRegion handle;
      derez.deserialize(handle);
      Event use_event;
      derez.deserialize(use_event);
      unsigned depth;
      derez.deserialize(depth);
      InstanceFlag flags;
      derez.deserialize(flags);
      RegionNode *target_node = runtime->forest->get_node(handle);
      LayoutDescription *layout = 
        LayoutDescription::handle_unpack_layout_description(derez, source, 
                                                            target_node);
      InstanceManager *inst_manager = legion_new<InstanceManager>(
                                        runtime->forest, did, owner_space,
                                        runtime->address_space, mem, inst, 
                                        target_node, layout, use_event,
                                        depth, false/*reg now*/, flags);
      if (!target_node->register_physical_manager(inst_manager))
        legion_delete(inst_manager);
      else
      {
        inst_manager->register_with_runtime();
        inst_manager->update_remote_instances(source);
      }
    }

    //--------------------------------------------------------------------------
    bool InstanceManager::match_instance(size_t field_size, 
                                         const Domain &dom) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(layout != NULL);
#endif
      // First check to see if the domains are the same
      if (region_node->get_domain_blocking() != dom)
        return false;
      return layout->match_shape(field_size);
    }

    //--------------------------------------------------------------------------
    bool InstanceManager::match_instance(const std::vector<size_t> &field_sizes,
                                         const Domain &dom, 
                                         const size_t bf) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(layout != NULL);
#endif
      // First check to see if the domains are the same
      if (region_node->get_domain_blocking() != dom)
        return false;
      return layout->match_shape(field_sizes, bf);
    }

    //--------------------------------------------------------------------------
    bool InstanceManager::is_attached_file(void) const
    //--------------------------------------------------------------------------
    {
      return (instance_flags & ATTACH_FILE_FLAG);
    }

    /////////////////////////////////////////////////////////////
    // ReductionManager 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ReductionManager::ReductionManager(RegionTreeForest *ctx, DistributedID did,
                                       AddressSpaceID owner_space, 
                                       AddressSpaceID local_space,
                                       Memory mem, PhysicalInstance inst, 
                                       RegionNode *node, ReductionOpID red, 
                                       const ReductionOp *o, bool reg_now)
      : PhysicalManager(ctx, did, owner_space, local_space, mem, 
                        node, inst, reg_now), 
        op(o), redop(red)
    //--------------------------------------------------------------------------
    { 
    }

    //--------------------------------------------------------------------------
    ReductionManager::~ReductionManager(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool ReductionManager::is_reduction_manager(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    InstanceManager* ReductionManager::as_instance_manager(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    ReductionManager* ReductionManager::as_reduction_manager(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<ReductionManager*>(this);
    }

    //--------------------------------------------------------------------------
    void ReductionManager::notify_inactive(void)
    //--------------------------------------------------------------------------
    {
      if (is_owner())
      {
        context->runtime->free_physical_instance(this);
        AutoLock gc(gc_lock);
#ifdef DEBUG_HIGH_LEVEL
        assert(instance.exists());
#endif
        log_garbage.debug("Garbage collecting reduction instance " IDFMT
                                " in memory " IDFMT " in address space %d",
                                instance.id, memory.id, owner_space);
#ifdef OLD_LEGION_PROF
        LegionProf::register_instance_deletion(instance.id);
#endif
#ifndef DISABLE_GC
        instance.destroy();
#endif
        instance = PhysicalInstance::NO_INST;
      }
      else // If we are not the owner remove our gc reference
        send_remote_gc_update(owner_space, 1/*count*/, false/*add*/);
    }

    //--------------------------------------------------------------------------
    void ReductionManager::notify_invalid(void)
    //--------------------------------------------------------------------------
    {
      // For right now we'll do nothing
      // There doesn't seem like much point in recycling reduction instances
      // If we are not the owner remove our valid reference
      if (!is_owner())
        send_remote_valid_update(owner_space, 1/*count*/, false/*add*/);
    }

    //--------------------------------------------------------------------------
    DistributedID ReductionManager::send_manager(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      if (!has_remote_instance(target))
      {
        // NO need to take the lock, duplicate sends are alright
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(owner_space);
          rez.serialize(memory);
          rez.serialize(instance);
          rez.serialize(redop);
          rez.serialize(region_node->handle);
          rez.serialize<bool>(is_foldable());
          rez.serialize(get_pointer_space());
          rez.serialize(get_use_event());
        }
        // Now send the message
        context->runtime->send_reduction_manager(target, rez);
        update_remote_instances(target);
      }
      return did;
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReductionManager::handle_send_manager(Runtime *runtime, 
                                     AddressSpaceID source, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      AddressSpaceID owner_space;
      derez.deserialize(owner_space);
      Memory mem;
      derez.deserialize(mem);
      PhysicalInstance inst;
      derez.deserialize(inst);
      ReductionOpID redop;
      derez.deserialize(redop);
      LogicalRegion handle;
      derez.deserialize(handle);
      bool foldable;
      derez.deserialize(foldable);
      Domain ptr_space;
      derez.deserialize(ptr_space);
      Event use_event;
      derez.deserialize(use_event);

      RegionNode *target_node = runtime->forest->get_node(handle);
      const ReductionOp *op = Runtime::get_reduction_op(redop);
      if (foldable)
      {
        FoldReductionManager *manager = 
                        legion_new<FoldReductionManager>(runtime->forest, did,
                                            owner_space, runtime->address_space,
                                            mem, inst, target_node, redop, op,
                                            use_event, false/*register now*/);
        if (!target_node->register_physical_manager(manager))
          legion_delete(manager);
        else
        {
          manager->register_with_runtime();
          manager->update_remote_instances(source);
        }
      }
      else
      {
        ListReductionManager *manager = 
                        legion_new<ListReductionManager>(runtime->forest, did,
                                            owner_space, runtime->address_space,
                                            mem, inst, target_node, redop, op,
                                            ptr_space, false/*register now*/);
        if (!target_node->register_physical_manager(manager))
          legion_delete(manager);
        else
        {
          manager->register_with_runtime();
          manager->update_remote_instances(source);
        }
      }
    }

    //--------------------------------------------------------------------------
    ReductionView* ReductionManager::create_view(void)
    //--------------------------------------------------------------------------
    {
      DistributedID view_did = 
        context->runtime->get_available_distributed_id(false);
      ReductionView *result = legion_new<ReductionView>(context, view_did,
                                                context->runtime->address_space,
                                                context->runtime->address_space,
                                                region_node, this, true/*reg*/);
      return result;
    }

    /////////////////////////////////////////////////////////////
    // ListReductionManager 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ListReductionManager::ListReductionManager(RegionTreeForest *ctx, 
                                               DistributedID did,
                                               AddressSpaceID owner_space, 
                                               AddressSpaceID local_space,
                                               Memory mem, 
                                               PhysicalInstance inst, 
                                               RegionNode *node,
                                               ReductionOpID red,
                                               const ReductionOp *o, 
                                               Domain dom, bool reg_now)
      : ReductionManager(ctx, did, owner_space, local_space, mem, 
                         inst, node, red, o, reg_now), ptr_space(dom)
    //--------------------------------------------------------------------------
    {
      // Tell the runtime so it can update the per memory data structures
      context->runtime->allocate_physical_instance(this);
#ifdef LEGION_GC
      log_garbage.info("GC List Reduction Manager %ld " IDFMT " " IDFMT " ",
                        did, inst.id, mem.id);
#endif
    }

    //--------------------------------------------------------------------------
    ListReductionManager::ListReductionManager(const ListReductionManager &rhs)
      : ReductionManager(NULL, 0, 0, 0, Memory::NO_MEMORY,
                         PhysicalInstance::NO_INST, NULL, 0, NULL, false),
        ptr_space(Domain::NO_DOMAIN)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    ListReductionManager::~ListReductionManager(void)
    //--------------------------------------------------------------------------
    {
      // Free up our pointer space
      ptr_space.get_index_space().destroy();
      // Tell the runtime that this instance no longer exists
      // If we were the owner we already did this when we garbage
      // collected the physical instance
      if (!is_owner())
        context->runtime->free_physical_instance(this);
    }

    //--------------------------------------------------------------------------
    ListReductionManager& ListReductionManager::operator=(
                                                const ListReductionManager &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic>
      ListReductionManager::get_accessor(void) const
    //--------------------------------------------------------------------------
    {
      // TODO: Implement this 
      assert(false);
      return instance.get_accessor();
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic>
      ListReductionManager::get_field_accessor(FieldID fid) const
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return instance.get_accessor();
    }

    //--------------------------------------------------------------------------
    size_t ListReductionManager::get_instance_size(void) const
    //--------------------------------------------------------------------------
    {
      size_t result = op->sizeof_rhs;
      if (ptr_space.get_dim() == 0)
      {
        const LowLevel::ElementMask &mask = 
          ptr_space.get_index_space().get_valid_mask();
        result *= mask.get_num_elmts();
      }
      else
        result *= ptr_space.get_volume();
      return result;
    }
    
    //--------------------------------------------------------------------------
    bool ListReductionManager::is_foldable(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    void ListReductionManager::find_field_offsets(const FieldMask &reduce_mask,
                                  std::vector<Domain::CopySrcDstField> &fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(instance.exists());
#endif
      // Assume that it's all the fields for right now
      // but offset by the pointer size
      fields.push_back(
          Domain::CopySrcDstField(instance, sizeof(ptr_t), op->sizeof_rhs));
    }

    //--------------------------------------------------------------------------
    Event ListReductionManager::issue_reduction(Operation *op,
        const std::vector<Domain::CopySrcDstField> &src_fields,
        const std::vector<Domain::CopySrcDstField> &dst_fields,
        Domain space, Event precondition, bool reduction_fold, bool precise)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(instance.exists());
#endif
      if (precise)
      {
        Domain::CopySrcDstField idx_field(instance, 0/*offset*/, sizeof(ptr_t));
        return context->issue_indirect_copy(space, op, idx_field, redop, 
                                            reduction_fold, src_fields, 
                                            dst_fields, precondition);
      }
      else
      {
        // TODO: teach the low-level runtime how to issue
        // partial reduction copies from a given space
        assert(false);
        return Event::NO_EVENT;
      }
    }

    //--------------------------------------------------------------------------
    Domain ListReductionManager::get_pointer_space(void) const
    //--------------------------------------------------------------------------
    {
      return ptr_space;
    }

    //--------------------------------------------------------------------------
    bool ListReductionManager::is_list_manager(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    ListReductionManager* ListReductionManager::as_list_manager(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<ListReductionManager*>(this);
    }

    //--------------------------------------------------------------------------
    FoldReductionManager* ListReductionManager::as_fold_manager(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    Event ListReductionManager::get_use_event(void) const
    //--------------------------------------------------------------------------
    {
      return Event::NO_EVENT;
    }

    /////////////////////////////////////////////////////////////
    // FoldReductionManager 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    FoldReductionManager::FoldReductionManager(RegionTreeForest *ctx, 
                                               DistributedID did,
                                               AddressSpaceID owner_space, 
                                               AddressSpaceID local_space,
                                               Memory mem,
                                               PhysicalInstance inst, 
                                               RegionNode *node,
                                               ReductionOpID red,
                                               const ReductionOp *o,
                                               Event u_event,
                                               bool register_now)
      : ReductionManager(ctx, did, owner_space, local_space, mem, 
                         inst, node, red, o, register_now), use_event(u_event)
    //--------------------------------------------------------------------------
    {
      // Tell the runtime so it can update the per memory data structures
      context->runtime->allocate_physical_instance(this);
#ifdef LEGION_GC
      log_garbage.info("GC Fold Reduction Manager %ld " IDFMT " " IDFMT " ",
                        did, inst.id, mem.id);
#endif
    }

    //--------------------------------------------------------------------------
    FoldReductionManager::FoldReductionManager(const FoldReductionManager &rhs)
      : ReductionManager(NULL, 0, 0, 0, Memory::NO_MEMORY,
                         PhysicalInstance::NO_INST, NULL, 0, NULL, false),
        use_event(Event::NO_EVENT)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    FoldReductionManager::~FoldReductionManager(void)
    //--------------------------------------------------------------------------
    {
      // Tell the runtime that this instance no longer exists
      // If we were the owner we already did this when we garbage
      // collected the physical instance
      if (!is_owner())
        context->runtime->free_physical_instance(this);
    }

    //--------------------------------------------------------------------------
    FoldReductionManager& FoldReductionManager::operator=(
                                                const FoldReductionManager &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic>
      FoldReductionManager::get_accessor(void) const
    //--------------------------------------------------------------------------
    {
      return instance.get_accessor();
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic>
      FoldReductionManager::get_field_accessor(FieldID fid) const
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return instance.get_accessor();
    }

    //--------------------------------------------------------------------------
    size_t FoldReductionManager::get_instance_size(void) const
    //--------------------------------------------------------------------------
    {
      size_t result = op->sizeof_rhs;
      const Domain &d = region_node->row_source->get_domain_blocking();
      if (d.get_dim() == 0)
      {
        const LowLevel::ElementMask &mask = 
          d.get_index_space().get_valid_mask();
        result *= mask.get_num_elmts();
      }
      else
        result *= d.get_volume();
      return result;
    }
    
    //--------------------------------------------------------------------------
    bool FoldReductionManager::is_foldable(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    void FoldReductionManager::find_field_offsets(const FieldMask &reduce_mask,
                                  std::vector<Domain::CopySrcDstField> &fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(instance.exists());
#endif
      // Assume that its all the fields for now
      // until we find a different way to do reductions on a subset of fields
      fields.push_back(
          Domain::CopySrcDstField(instance, 0/*offset*/, op->sizeof_rhs));
    }

    //--------------------------------------------------------------------------
    Event FoldReductionManager::issue_reduction(Operation *op,
        const std::vector<Domain::CopySrcDstField> &src_fields,
        const std::vector<Domain::CopySrcDstField> &dst_fields,
        Domain space, Event precondition, bool reduction_fold, bool precise)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(instance.exists());
#endif
      // Doesn't matter if this one is precise or not
      return context->issue_reduction_copy(space, op, redop, reduction_fold,
                                         src_fields, dst_fields, precondition);
    }

    //--------------------------------------------------------------------------
    Domain FoldReductionManager::get_pointer_space(void) const
    //--------------------------------------------------------------------------
    {
      return Domain::NO_DOMAIN;
    }

    //--------------------------------------------------------------------------
    bool FoldReductionManager::is_list_manager(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    ListReductionManager* FoldReductionManager::as_list_manager(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    FoldReductionManager* FoldReductionManager::as_fold_manager(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<FoldReductionManager*>(this);
    }

    //--------------------------------------------------------------------------
    Event FoldReductionManager::get_use_event(void) const
    //--------------------------------------------------------------------------
    {
      return use_event;
    }

    /////////////////////////////////////////////////////////////
    // LogicalView 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    LogicalView::LogicalView(RegionTreeForest *ctx, DistributedID did,
                             AddressSpaceID own_addr, AddressSpace loc_space,
                             RegionTreeNode *node, bool register_now)
      : DistributedCollectable(ctx->runtime, did, own_addr, 
                               loc_space, false/*register with runtime*/), 
        context(ctx), logical_node(node), 
        view_lock(Reservation::create_reservation()) 
    //--------------------------------------------------------------------------
    {
      if (register_now)
        logical_node->register_logical_view(this); 
    }

    //--------------------------------------------------------------------------
    LogicalView::~LogicalView(void)
    //--------------------------------------------------------------------------
    {
      view_lock.destroy_reservation();
      view_lock = Reservation::NO_RESERVATION;
      logical_node->unregister_logical_view(this);
    }

    //--------------------------------------------------------------------------
    /*static*/ void LogicalView::delete_logical_view(LogicalView *view)
    //--------------------------------------------------------------------------
    {
      if (view->is_instance_view())
      {
        InstanceView *inst_view = view->as_instance_view();
        if (inst_view->is_materialized_view())
          legion_delete(inst_view->as_materialized_view());
        else if (inst_view->is_reduction_view())
          legion_delete(inst_view->as_reduction_view());
        else
          assert(false);
      }
      else if (view->is_deferred_view())
      {
        DeferredView *deferred_view = view->as_deferred_view();
        if (deferred_view->is_composite_view())
          legion_delete(deferred_view->as_composite_view());
        else if (deferred_view->is_fill_view())
          legion_delete(deferred_view->as_fill_view());
        else
          assert(false);
      }
      else
        assert(false);
    }

    //--------------------------------------------------------------------------
    void LogicalView::send_remote_registration(void)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!is_owner());
#endif
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(destruction_event);
        const bool is_region = logical_node->is_region();
        rez.serialize<bool>(is_region);
        if (is_region)
          rez.serialize(logical_node->as_region_node()->handle);
        else
          rez.serialize(logical_node->as_partition_node()->handle);
      }
      runtime->send_view_remote_registration(owner_space, rez);
    }

    //--------------------------------------------------------------------------
    void LogicalView::send_remote_valid_update(AddressSpaceID target,
                                               unsigned count, bool add)
    //--------------------------------------------------------------------------
    {
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(count);
        rez.serialize(add);
        const bool is_region = logical_node->is_region();
        rez.serialize<bool>(is_region);
        if (is_region)
          rez.serialize(logical_node->as_region_node()->handle);
        else
          rez.serialize(logical_node->as_partition_node()->handle);
      }
      runtime->send_view_remote_valid_update(target, rez);
    }

    //--------------------------------------------------------------------------
    void LogicalView::send_remote_gc_update(AddressSpaceID target,
                                            unsigned count, bool add)
    //--------------------------------------------------------------------------
    {
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(count);
        rez.serialize(add);
        const bool is_region = logical_node->is_region();
        rez.serialize<bool>(is_region);
        if (is_region)
          rez.serialize(logical_node->as_region_node()->handle);
        else
          rez.serialize(logical_node->as_partition_node()->handle);
      }
      runtime->send_view_remote_gc_update(target, rez);
    }

    //--------------------------------------------------------------------------
    void LogicalView::send_remote_resource_update(AddressSpaceID target,
                                                  unsigned count, bool add)
    //--------------------------------------------------------------------------
    {
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        rez.serialize(count);
        rez.serialize(add);
        const bool is_region = logical_node->is_region();
        rez.serialize<bool>(is_region);
        if (is_region)
          rez.serialize(logical_node->as_region_node()->handle);
        else
          rez.serialize(logical_node->as_partition_node()->handle);
      }
      runtime->send_view_remote_resource_update(target, rez);
    }

    //--------------------------------------------------------------------------
    /*static*/ void LogicalView::handle_view_remote_registration(
           RegionTreeForest *forest, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      Event destroy_event;
      derez.deserialize(destroy_event);
      bool is_region;
      derez.deserialize<bool>(is_region);
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        forest->get_node(handle)->find_view(did)->register_remote_instance(
                                                      source, destroy_event);
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        forest->get_node(handle)->find_view(did)->register_remote_instance(
                                                      source, destroy_event);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void LogicalView::handle_view_remote_valid_update(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      unsigned count;
      derez.deserialize(count);
      bool add;
      derez.deserialize(add);
      bool is_region;
      derez.deserialize(is_region);
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        LogicalView *target = forest->get_node(handle)->find_view(did);
        if (add)
          target->add_base_valid_ref(REMOTE_DID_REF, count);
        else if (target->remove_base_valid_ref(REMOTE_DID_REF, count))
          delete target;
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        LogicalView *target = forest->get_node(handle)->find_view(did);
        if (add)
          target->add_base_valid_ref(REMOTE_DID_REF, count);
        else if (target->remove_base_valid_ref(REMOTE_DID_REF, count))
          delete target;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void LogicalView::handle_view_remote_gc_update(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      unsigned count;
      derez.deserialize(count);
      bool add;
      derez.deserialize(add);
      bool is_region;
      derez.deserialize(is_region);
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        LogicalView *target = forest->get_node(handle)->find_view(did);
        if (add)
          target->add_base_gc_ref(REMOTE_DID_REF, count);
        else if (target->remove_base_gc_ref(REMOTE_DID_REF, count))
          delete target;
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        LogicalView *target = forest->get_node(handle)->find_view(did);
        if (add)
          target->add_base_gc_ref(REMOTE_DID_REF, count);
        else if (target->remove_base_gc_ref(REMOTE_DID_REF, count))
          delete target;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void LogicalView::handle_view_remote_resource_update(
                                  RegionTreeForest *forest, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      unsigned count;
      derez.deserialize(count);
      bool add;
      derez.deserialize(add);
      bool is_region;
      derez.deserialize(is_region);
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        LogicalView *target = forest->get_node(handle)->find_view(did);
        if (add)
          target->add_base_resource_ref(REMOTE_DID_REF, count);
        else if (target->remove_base_resource_ref(REMOTE_DID_REF, count))
          delete target;
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        LogicalView *target = forest->get_node(handle)->find_view(did);
        if (add)
          target->add_base_resource_ref(REMOTE_DID_REF, count);
        else if (target->remove_base_resource_ref(REMOTE_DID_REF, count))
          delete target;
      }
    }

    //--------------------------------------------------------------------------
    DistributedID LogicalView::send_view(AddressSpaceID target,
                                         const FieldMask &update_mask)
    //--------------------------------------------------------------------------
    {
      DistributedID result = send_view_base(target);
      send_view_updates(target, update_mask);
      return result;
    }

    //--------------------------------------------------------------------------
    void LogicalView::defer_collect_user(Event term_event) 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, DEFER_COLLECT_USER_CALL);
#endif
      // The runtime will add the gc reference to this view when necessary
      runtime->defer_collect_user(this, term_event);
    }
 
    //--------------------------------------------------------------------------
    /*static*/ void LogicalView::handle_deferred_collect(LogicalView *view,
                                             const std::set<Event> &term_events)
    //--------------------------------------------------------------------------
    {
 #ifdef OLD_LEGION_PROF
      LegionProf::register_event(0, PROF_BEGIN_GC);
#endif
#ifdef LEGION_LOGGING
      LegionLogging::log_timing_event(Processor::get_executing_processor(),
                                      0 /* no unique id */,
                                      BEGIN_GC);
#endif     
      view->collect_users(term_events);
      // Then remove the gc reference on the object
      if (view->remove_base_gc_ref(PENDING_GC_REF))
        delete_logical_view(view);
#ifdef OLD_LEGION_PROF
      LegionProf::register_event(0, PROF_END_GC);
#endif
#ifdef LEGION_LOGGING
      LegionLogging::log_timing_event(Processor::get_executing_processor(),
                                      0 /* no unique id */,
                                      END_GC);
#endif
    }

    /////////////////////////////////////////////////////////////
    // InstanceView 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    InstanceView::InstanceView(RegionTreeForest *ctx, DistributedID did,
                               AddressSpaceID owner_sp, AddressSpaceID local_sp,
                               RegionTreeNode *node, bool register_now)
      : LogicalView(ctx, did, owner_sp, local_sp, node, register_now)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InstanceView::~InstanceView(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    bool InstanceView::is_instance_view(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    bool InstanceView::is_deferred_view(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    InstanceView* InstanceView::as_instance_view(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<InstanceView*>(this);
    }

    //--------------------------------------------------------------------------
    DeferredView* InstanceView::as_deferred_view(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    /////////////////////////////////////////////////////////////
    // MaterializedView 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    MaterializedView::MaterializedView(
                               RegionTreeForest *ctx, DistributedID did,
                               AddressSpaceID own_addr, AddressSpaceID loc_addr,
                               RegionTreeNode *node, InstanceManager *man,
                               MaterializedView *par, unsigned dep,
                               bool register_now, bool persist/* = false*/)
      : InstanceView(ctx, did, own_addr, loc_addr, node, register_now), 
        manager(man), parent(par), depth(dep), persistent_view(persist)
    //--------------------------------------------------------------------------
    {
      // Otherwise the instance lock will get filled in when we are unpacked
#ifdef DEBUG_HIGH_LEVEL
      assert(manager != NULL);
#endif
      // If we are either not a parent or we are a remote parent add 
      // a resource reference to avoid being collected
      if (parent != NULL)
        add_nested_resource_ref(did);
      else 
      {
        manager->add_nested_resource_ref(did);
        // Do remote registration for the top of each remote tree
        if (!is_owner())
        {
          add_base_resource_ref(REMOTE_DID_REF);
          send_remote_registration();
        }
      }
#ifdef LEGION_GC
      log_garbage.info("GC Materialized View %ld %ld", did, manager->did); 
#endif
    }

    //--------------------------------------------------------------------------
    MaterializedView::MaterializedView(const MaterializedView &rhs)
      : InstanceView(NULL, 0, 0, 0, NULL, false),
        manager(NULL), parent(NULL), depth(0)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    MaterializedView::~MaterializedView(void)
    //--------------------------------------------------------------------------
    {
      // Remove our resource references on our children
      // Capture their recycle events in the process
      for (std::map<ColorPoint,MaterializedView*>::const_iterator it = 
            children.begin(); it != children.end(); it++)
      {
        recycle_events.insert(it->second->get_destruction_event());
        if (it->second->remove_nested_resource_ref(did))
          legion_delete(it->second);
      }
      if (parent == NULL)
      {
        if (manager->remove_nested_resource_ref(did))
          delete manager;
        if (is_owner())
        {
          UpdateReferenceFunctor<RESOURCE_REF_KIND,false/*add*/> functor(this);
          map_over_remote_instances(functor);
          // If we are the top the tree on the owner node we can recycle 
          // the distributed ID once our destruction event triggers
          runtime->recycle_distributed_id(did, destruction_event);
        }
      }
      if (!atomic_reservations.empty())
      {
        // If this is the owner view, delete any atomic reservations
        if (is_owner())
        {
          for (std::map<FieldID,Reservation>::iterator it = 
                atomic_reservations.begin(); it != 
                atomic_reservations.end(); it++)
          {
            it->second.destroy_reservation();
          }
        }
        atomic_reservations.clear();
      }
      if (!initial_user_events.empty())
      {
        for (std::set<Event>::const_iterator it = initial_user_events.begin();
              it != initial_user_events.end(); it++)
          filter_local_users(*it);
      }
#if !defined(LEGION_SPY) && !defined(LEGION_LOGGING) && \
      !defined(EVENT_GRAPH_TRACE) && defined(DEBUG_HIGH_LEVEL)
      // Don't forget to remove the initial user if there was one
      // before running these checks
      assert(current_epoch_users.empty());
      assert(previous_epoch_users.empty());
      assert(outstanding_gc_events.empty());
#endif
    }

    //--------------------------------------------------------------------------
    MaterializedView& MaterializedView::operator=(const MaterializedView &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    Memory MaterializedView::get_location(void) const
    //--------------------------------------------------------------------------
    {
      return manager->memory;
    }

    //--------------------------------------------------------------------------
    size_t MaterializedView::get_blocking_factor(void) const
    //--------------------------------------------------------------------------
    {
      return manager->layout->blocking_factor;
    } 

    //--------------------------------------------------------------------------
    const FieldMask& MaterializedView::get_physical_mask(void) const
    //--------------------------------------------------------------------------
    {
      return manager->layout->allocated_fields;
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::is_materialized_view(void) const
    //--------------------------------------------------------------------------
    {
      return true; 
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::is_reduction_view(void) const
    //--------------------------------------------------------------------------
    {
      return false; 
    }

    //--------------------------------------------------------------------------
    MaterializedView* MaterializedView::as_materialized_view(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<MaterializedView*>(this);
    }

    //--------------------------------------------------------------------------
    ReductionView* MaterializedView::as_reduction_view(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::has_space(const FieldMask &space_mask) const
    //--------------------------------------------------------------------------
    {
      return !(manager->layout->allocated_fields * space_mask);
    }

    //--------------------------------------------------------------------------
    LogicalView* MaterializedView::get_subview(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      return get_materialized_subview(c);
    }

    //--------------------------------------------------------------------------
    MaterializedView* MaterializedView::get_materialized_subview(
                                                           const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, GET_SUBVIEW_CALL);
#endif
      // This is the common case
      {
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
        std::map<ColorPoint,MaterializedView*>::const_iterator finder = 
                                                            children.find(c);
        if (finder != children.end())
          return finder->second;
      }
      RegionTreeNode *child_node = logical_node->get_tree_child(c);
      MaterializedView *child_view = legion_new<MaterializedView>(context,
                                            did, owner_space, local_space,
                                            child_node, manager, this, 
                                            depth, false/*don't register*/);
      // Retake the lock and try and add it, see if
      // someone else added the child in the meantime
      bool free_child_view = false;
      MaterializedView *result = child_view;
      {
        AutoLock v_lock(view_lock);
        std::map<ColorPoint,MaterializedView*>::const_iterator finder = 
                                                            children.find(c);
        if (finder != children.end())
        {
          // Guaranteed to succeed
          if (child_view->remove_nested_resource_ref(did))
            free_child_view = true;
          // Change the result
          result = finder->second;
        }
        else
        {
          children[c] = child_view;
          child_node->register_logical_view(child_view);
        }
      }
      if (free_child_view)
        legion_delete(child_view);
      return result;
    }

    //--------------------------------------------------------------------------
    MaterializedView* MaterializedView::get_materialized_parent_view(void) const
    //--------------------------------------------------------------------------
    {
      return parent;
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::is_persistent(void) const
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        return parent->is_persistent();
      return persistent_view;
    }

    //--------------------------------------------------------------------------
    void MaterializedView::PersistenceFunctor::apply(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      if (target != source)
      {
        UserEvent to_trigger = UserEvent::create_user_event();
        Serializer rez;
        {
          RezCheck z(rez);
          parent->pack_remote_ctx_info(rez);
          rez.serialize(handle);
          rez.serialize(did);
          rez.serialize(parent_idx);
          rez.serialize(to_trigger);
        }
        runtime->send_make_persistent(target, rez);
        done_events.insert(to_trigger);
      }
    }

    //--------------------------------------------------------------------------
    void MaterializedView::make_persistent(SingleTask *parent_ctx,
                                           unsigned parent_idx,
                                           AddressSpaceID source, 
                                           UserEvent to_trigger)
    //--------------------------------------------------------------------------
    {
      if (parent == NULL)
      {
        // Mark that we are persistent, then figure out who else needs updates
        if (!persistent_view)
        {
          persistent_view = true;
          RegionTreeContext local_ctx = 
            parent_ctx->find_enclosing_physical_context(parent_idx);
          logical_node->add_persistent_view(local_ctx.get_id(), this);
        }
        if (is_owner())
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(logical_node->is_region());
#endif
          // We're the owner, so send out any notifications to other
          // views to inform them that this instance is now persistent
          std::set<Event> done_events;
          PersistenceFunctor functor(source, runtime, parent_ctx,
                                     logical_node->as_region_node()->handle,
                                     did, parent_idx, done_events);
          map_over_remote_instances(functor);
          to_trigger.trigger(Event::merge_events(done_events));
        }
        else if (source != owner_space)
        {
          // If we are not the owner and the request didn't come from 
          // the owner then, send a request to the owner
          // to make all the views persistent
          Serializer rez;
          {
            RezCheck z(rez);
            parent_ctx->pack_remote_ctx_info(rez);
#ifdef DEBUG_HIGH_LEVEL
            assert(logical_node->is_region());
#endif
            rez.serialize(logical_node->as_region_node()->handle);
            rez.serialize(did);
            rez.serialize(parent_idx);
            rez.serialize(to_trigger);
          }
          runtime->send_make_persistent(owner_space, rez);
        }
        else
        {
          // We've done our registration, so trigger our event
          to_trigger.trigger();
        }
      }
      else
        parent->make_persistent(parent_ctx, parent_idx, source, to_trigger);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::copy_field(FieldID fid,
                              std::vector<Domain::CopySrcDstField> &copy_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, COPY_FIELD_CALL);
#endif
      std::vector<FieldID> local_fields(1,fid);
      manager->compute_copy_offsets(local_fields, copy_fields); 
    }

    //--------------------------------------------------------------------------
    void MaterializedView::copy_to(const FieldMask &copy_mask,
                               std::vector<Domain::CopySrcDstField> &dst_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, COPY_TO_CALL);
#endif
      manager->compute_copy_offsets(copy_mask, dst_fields);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::copy_from(const FieldMask &copy_mask,
                               std::vector<Domain::CopySrcDstField> &src_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, COPY_FROM_CALL);
#endif
      manager->compute_copy_offsets(copy_mask, src_fields);
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::reduce_to(ReductionOpID redop, 
                                     const FieldMask &copy_mask,
                               std::vector<Domain::CopySrcDstField> &dst_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REDUCE_TO_CALL);
#endif
      manager->compute_copy_offsets(copy_mask, dst_fields);
      return false; // not a fold
    }

    //--------------------------------------------------------------------------
    void MaterializedView::reduce_from(ReductionOpID redop,
                                       const FieldMask &reduce_mask,
                               std::vector<Domain::CopySrcDstField> &src_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REDUCE_FROM_CALL);
#endif
      manager->compute_copy_offsets(reduce_mask, src_fields);
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::has_war_dependence(const RegionUsage &usage,
                                              const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, HAS_WAR_DEPENDENCE_CALL);
#endif
      // No WAR dependences for read-only or reduce 
      if (IS_READ_ONLY(usage) || IS_REDUCE(usage))
        return false;
      const ColorPoint &local_color = logical_node->get_color();
      if (has_local_war_dependence(usage, user_mask, ColorPoint(), local_color))
        return true;
      if (parent != NULL)
        return parent->has_war_dependence_above(usage, user_mask, local_color);
      return false;
    } 

    //--------------------------------------------------------------------------
    void MaterializedView::accumulate_events(std::set<Event> &all_events)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ACCUMULATE_EVENTS_CALL);
#endif
      AutoLock v_lock(view_lock,1,false/*exclusive*/);
      all_events.insert(outstanding_gc_events.begin(),
                        outstanding_gc_events.end());
    } 

    //--------------------------------------------------------------------------
    void MaterializedView::add_copy_user(ReductionOpID redop, Event copy_term,
                                         const VersionInfo &version_info,
                                     const FieldMask &copy_mask, bool reading)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ADD_COPY_USER_CALL);
#endif
      // Quick test, we only need to do this if the copy_term event
      // exists, otherwise the user is already done
      if (copy_term.exists())
      {
        RegionUsage usage;
        usage.redop = redop;
        usage.prop = EXCLUSIVE;
        if (reading)
          usage.privilege = READ_ONLY;
        else if (redop > 0)
          usage.privilege = REDUCE;
        else
          usage.privilege = READ_WRITE;
        if ((parent != NULL) && !version_info.is_upper_bound_node(logical_node))
        {
          const ColorPoint &local_color = logical_node->get_color();
          parent->add_copy_user_above(usage, copy_term, local_color,
                                      version_info, copy_mask);
        }
        add_local_copy_user(usage, copy_term, true/*base*/, ColorPoint(),
                            version_info, copy_mask);
      }
    }

    //--------------------------------------------------------------------------
    InstanceRef MaterializedView::add_user(const RegionUsage &usage, 
                                           Event term_event,
                                           const FieldMask &user_mask,
                                           const VersionInfo &version_info) 
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ADD_USER_CALL);
#endif
      std::set<Event> wait_on_events;
      Event start_use_event = manager->get_use_event();
      if (start_use_event.exists())
        wait_on_events.insert(start_use_event);
      if ((parent != NULL) && !version_info.is_upper_bound_node(logical_node))
      {
        const ColorPoint &local_color = logical_node->get_color();
        parent->add_user_above(usage, term_event, local_color,
                               version_info, user_mask, wait_on_events);
      }
      const bool issue_collect = add_local_user(usage, term_event, true/*base*/,
                         ColorPoint(), version_info, user_mask, wait_on_events);
      // Launch the garbage collection task, if it doesn't exist
      // then the user wasn't registered anyway, see add_local_user
      if (issue_collect)
        defer_collect_user(term_event);
      // At this point tasks shouldn't be allowed to wait on themselves
#ifdef DEBUG_HIGH_LEVEL
      if (term_event.exists())
        assert(wait_on_events.find(term_event) == wait_on_events.end());
#endif
      // Make the instance ref
      Event ready_event = Event::merge_events(wait_on_events);
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
      if (!ready_event.exists())
      {
        UserEvent new_ready_event = UserEvent::create_user_event();
        new_ready_event.trigger();
        ready_event = new_ready_event;
      }
#endif
#ifdef LEGION_LOGGING
      LegionLogging::log_event_dependences(
          Processor::get_executing_processor(), wait_on_events, ready_event);
#endif
#ifdef LEGION_SPY
      LegionSpy::log_event_dependences(wait_on_events, ready_event);
#endif
      InstanceRef result(ready_event, this);
      if (IS_ATOMIC(usage))
        find_atomic_reservations(result, user_mask);
      return result;
    }

    //--------------------------------------------------------------------------
    void MaterializedView::add_initial_user(Event term_event,
                                            const RegionUsage &usage,
                                            const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      // No need to take the lock since we are just initializing
      PhysicalUser *user = legion_new<PhysicalUser>(usage, ColorPoint());
      user->add_reference();
      add_current_user(user, term_event, user_mask);
      initial_user_events.insert(term_event);
      // Don't need to actual launch a collection task, destructor
      // will handle this case
      outstanding_gc_events.insert(term_event);
    }
 
    //--------------------------------------------------------------------------
    void MaterializedView::notify_active(void)
    //--------------------------------------------------------------------------
    {
      if (parent == NULL)
        manager->add_nested_gc_ref(did);
      else
        parent->add_nested_gc_ref(did);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::notify_inactive(void)
    //--------------------------------------------------------------------------
    {
      // No need to worry about handling the deletion case since
      // we know we also hold a resource reference and therefore
      // the manager won't be deleted until we are deleted at
      // the earliest
      if (parent == NULL)
        manager->remove_nested_gc_ref(did);
      else if (parent->remove_nested_gc_ref(did))
        delete parent;
    }

    //--------------------------------------------------------------------------
    void MaterializedView::notify_valid(void)
    //--------------------------------------------------------------------------
    {
      // If we are at the top of the tree add a valid reference
      // Otherwise add our valid reference on our parent
      if (parent == NULL)
      {
        if (!is_owner())
          send_remote_valid_update(owner_space, 1/*count*/, true/*add*/);
        manager->add_nested_valid_ref(did);
      }
      else
        parent->add_nested_valid_ref(did);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::notify_invalid(void)
    //--------------------------------------------------------------------------
    {
      // If we are at the top of the tree add a valid reference
      // Otherwise add our valid reference on the parent
      if (parent == NULL)
      {
        if (!is_owner())
          send_remote_valid_update(owner_space, 1/*count*/, false/*add*/);
        manager->remove_nested_valid_ref(did);
      }
      else if (parent->remove_nested_valid_ref(did))
        legion_delete(parent);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::collect_users(const std::set<Event> &term_events)
    //--------------------------------------------------------------------------
    {
      {
        AutoLock v_lock(view_lock);
        // Remove any event users from the current and previous users
        for (std::set<Event>::const_iterator it = term_events.begin();
              it != term_events.end(); it++)
        {
          filter_local_users(*it); 
        }
      }
      if (parent != NULL)
        parent->collect_users(term_events);
    } 

    //--------------------------------------------------------------------------
    DistributedID MaterializedView::send_view_base(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      // See if we already have it
      if (!has_remote_instance(target))
      {
        if (parent == NULL)
        {
          // If we are the parent we have to do the send
          // Send the physical manager first
          DistributedID manager_did = manager->send_manager(target);
#ifdef DEBUG_HIGH_LEVEL
          assert(logical_node->is_region()); // Always regions at the top
#endif
          // Don't take the lock, it's alright to have duplicate sends
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(manager_did);
            rez.serialize(logical_node->as_region_node()->handle);
            rez.serialize(owner_space);
            rez.serialize(depth);
            rez.serialize(persistent_view);
          }
          runtime->send_materialized_view(target, rez);
        }
        else // Ask our parent to do the send
          parent->send_view_base(target);
        // We've now done the send so record it 
        update_remote_instances(target);
      }
      return did;
    }

    //--------------------------------------------------------------------------
    void MaterializedView::send_view_updates(AddressSpaceID target,
                                             const FieldMask &update_mask)
    //--------------------------------------------------------------------------
    {
      std::map<PhysicalUser*,int/*index*/> needed_users;  
      Serializer current_rez, previous_rez;
      unsigned current_events = 0, previous_events = 0;
      // Take the lock in read-only mode
      {
        AutoLock v_lock(view_lock,1,false/*exclusive*/);
        for (LegionMap<Event,EventUsers>::aligned::const_iterator cit = 
              current_epoch_users.begin(); cit != 
              current_epoch_users.end(); cit++)
        {
          FieldMask overlap = cit->second.user_mask & update_mask;
          if (!overlap)
            continue;
          current_events++;
          current_rez.serialize(cit->first);
          const EventUsers &event_users = cit->second;
          if (event_users.single)
          {
            int index = needed_users.size();
            needed_users[event_users.users.single_user] = index;
            event_users.users.single_user->add_reference();
            current_rez.serialize(index);
            current_rez.serialize(overlap);
          }
          else
          {
            Serializer event_rez;
            int count = -1; // start this at negative one
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                  it = event_users.users.multi_users->begin(); it != 
                  event_users.users.multi_users->end(); it++)
            {
              FieldMask overlap2 = it->second & overlap;
              if (!overlap2)
                continue;
              count--; // Make the count negative to disambiguate
              int index = needed_users.size();
              needed_users[it->first] = index;
              it->first->add_reference();
              event_rez.serialize(index);
              event_rez.serialize(overlap2);
            }
            // If there was only one, we can take the normal path
            if ((count == -1) || (count < -2))
              current_rez.serialize(count);
            size_t event_rez_size = event_rez.get_used_bytes();
            current_rez.serialize(event_rez.get_buffer(), event_rez_size);
          }
        }
        for (LegionMap<Event,EventUsers>::aligned::const_iterator pit = 
              previous_epoch_users.begin(); pit != 
              previous_epoch_users.end(); pit++)
        {
          FieldMask overlap = pit->second.user_mask & update_mask;
          if (!overlap)
            continue;
          previous_events++;
          previous_rez.serialize(pit->first);
          const EventUsers &event_users = pit->second;
          if (event_users.single)
          {
            std::map<PhysicalUser*,int>::const_iterator finder = 
              needed_users.find(event_users.users.single_user);
            if (finder == needed_users.end())
            {
              int index = needed_users.size();
              previous_rez.serialize(index);
              needed_users[event_users.users.single_user] = index;
              event_users.users.single_user->add_reference();
            }
            else
              previous_rez.serialize(finder->second);
            previous_rez.serialize(overlap);
          }
          else 
          {
            Serializer event_rez;
            int count = -1; // start this at negative one
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator
                  it = event_users.users.multi_users->begin(); it !=
                  event_users.users.multi_users->end(); it++)
            {
              FieldMask overlap2 = it->second & overlap;
              if (!overlap2)
                continue;
              count--; // Make the count negative to disambiguate
              std::map<PhysicalUser*,int>::const_iterator finder = 
                needed_users.find(it->first);
              if (finder == needed_users.end())
              {
                int index = needed_users.size();
                needed_users[it->first] = index;
                event_rez.serialize(index);
                it->first->add_reference();
              }
              else
                event_rez.serialize(finder->second);
              event_rez.serialize(overlap2);
            }
            // If there was only one user, we can take the normal path
            if ((count == -1) || (count < -2))
              previous_rez.serialize(count);
            size_t event_rez_size = event_rez.get_used_bytes();
            previous_rez.serialize(event_rez.get_buffer(), event_rez_size); 
          }
        }
      }
      // Now build our buffer and send the result
      Serializer rez;
      {
        RezCheck z(rez);
        bool is_region = logical_node->is_region();
        rez.serialize(is_region);
        if (is_region)
          rez.serialize(logical_node->as_region_node()->handle);
        else
          rez.serialize(logical_node->as_partition_node()->handle);
        rez.serialize(did);
        // Pack the needed users first
        rez.serialize<size_t>(needed_users.size());
        for (std::map<PhysicalUser*,int>::const_iterator it = 
              needed_users.begin(); it != needed_users.end(); it++)
        {
          rez.serialize(it->second);
          it->first->pack_user(rez);
          if (it->first->remove_reference())
            legion_delete(it->first);
        }
        // Then pack the current and previous events
        rez.serialize(current_events);
        size_t current_size = current_rez.get_used_bytes();
        rez.serialize(current_rez.get_buffer(), current_size);
        rez.serialize(previous_events);
        size_t previous_size = previous_rez.get_used_bytes();
        rez.serialize(previous_rez.get_buffer(), previous_size);
      }
      runtime->send_materialized_update(target, rez);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::process_update(Deserializer &derez,
                                          AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      size_t num_users;
      derez.deserialize(num_users);
      std::vector<PhysicalUser*> users(num_users);
      FieldSpaceNode *field_node = logical_node->column_source;
      for (unsigned idx = 0; idx < num_users; idx++)
      {
        int index;
        derez.deserialize(index);
        users[index] = PhysicalUser::unpack_user(derez, field_node, 
                                                 source, true/*add ref*/); 
      }
      // We've already added a reference for all users since we'll know
      // that we'll be adding them at least once
      std::vector<bool> need_reference(num_users, false);
      std::deque<Event> collect_events;
      {
        // Hold the lock when updating the view
        AutoLock v_lock(view_lock); 
        unsigned num_current;
        derez.deserialize(num_current);
        for (unsigned idx = 0; idx < num_current; idx++)
        {
          Event current_event;
          derez.deserialize(current_event);
          int index;
          derez.deserialize(index);
          if (index < 0)
          {
            int count = (-index) - 1;
            for (int i = 0; i < count; i++)
            {
              derez.deserialize(index);
#ifdef DEBUG_HIGH_LEVEL
              assert(unsigned(index) < num_users);
#endif
              FieldMask user_mask;
              derez.deserialize(user_mask);
              field_node->transform_field_mask(user_mask, source);
              if (need_reference[index])
                users[index]->add_reference();
              else
                need_reference[index] = true;
              add_current_user(users[index], current_event, user_mask);
            }
          }
          else
          {
#ifdef DEBUG_HIGH_LEVEL
            assert(unsigned(index) < num_users);
#endif
            // Just one user
            FieldMask user_mask;
            derez.deserialize(user_mask);
            field_node->transform_field_mask(user_mask, source);
            if (need_reference[index])
              users[index]->add_reference();
            else
              need_reference[index] = true;
            add_current_user(users[index], current_event, user_mask);
          }
          if (outstanding_gc_events.find(current_event) ==
              outstanding_gc_events.end())
          {
            outstanding_gc_events.insert(current_event);
            collect_events.push_back(current_event);
          }
        }
        unsigned num_previous;
        derez.deserialize(num_previous);
        for (unsigned idx = 0; idx < num_previous; idx++)
        {
          Event previous_event;
          derez.deserialize(previous_event);
          int index;
          derez.deserialize(index);
          if (index < 0)
          {
            int count = (-index) - 1;
            for (int i = 0; i < count; i++)
            {
              derez.deserialize(index);
#ifdef DEBUG_HIGH_LEVEL
              assert(unsigned(index) < num_users);
#endif
              FieldMask user_mask;
              derez.deserialize(user_mask);
              field_node->transform_field_mask(user_mask, source);
              if (need_reference[index])
                users[index]->add_reference();
              else
                need_reference[index] = true;
              add_previous_user(users[index], previous_event, user_mask);
            }
          }
          else
          {
#ifdef DEBUG_HIGH_LEVEL
            assert(unsigned(index) < num_users);
#endif
            // Just one user
            FieldMask user_mask;
            derez.deserialize(user_mask);
            field_node->transform_field_mask(user_mask, source);
            if (need_reference[index])
              users[index]->add_reference();
            else
              need_reference[index] = true;
            add_previous_user(users[index], previous_event, user_mask);
          }
          if (outstanding_gc_events.find(previous_event) ==
              outstanding_gc_events.end())
          {
            outstanding_gc_events.insert(previous_event);
            collect_events.push_back(previous_event);
          }
        }
      }
      if (!collect_events.empty())
      {
        if (parent != NULL)
          parent->update_gc_events(collect_events);
        for (std::deque<Event>::const_iterator it = 
              collect_events.begin(); it != collect_events.end(); it++)
        {
          defer_collect_user(*it); 
        }
      }
#ifdef DEBUG_HIGH_LEVEL
      for (unsigned idx = 0; idx < need_reference.size(); idx++)
        assert(need_reference[idx]);
#endif
    }

    //--------------------------------------------------------------------------
    void MaterializedView::update_gc_events(const std::deque<Event> &gc_events)
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        parent->update_gc_events(gc_events);
      AutoLock v_lock(view_lock);
      for (std::deque<Event>::const_iterator it = gc_events.begin();
            it != gc_events.end(); it++)
      {
        outstanding_gc_events.insert(*it);
      }
    }

    //--------------------------------------------------------------------------
    void MaterializedView::add_user_above(const RegionUsage &usage, 
                                          Event term_event,
                                          const ColorPoint &child_color,
                                          const VersionInfo &version_info,
                                          const FieldMask &user_mask,
                                          std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ADD_USER_ABOVE_CALL);
#endif
      if ((parent != NULL) && !version_info.is_upper_bound_node(logical_node))
      {
        const ColorPoint &local_color = logical_node->get_color();
        parent->add_user_above(usage, term_event, local_color,
                               version_info, user_mask, preconditions);
      }
      add_local_user(usage, term_event, false/*base*/, child_color,
                     version_info, user_mask, preconditions);
      // No need to launch a collect user task, the child takes care of that
    }

    //--------------------------------------------------------------------------
    void MaterializedView::add_copy_user_above(const RegionUsage &usage, 
                                               Event copy_term, 
                                               const ColorPoint &child_color,
                                               const VersionInfo &version_info,
                                               const FieldMask &copy_mask)
    //--------------------------------------------------------------------------
    {
      if ((parent != NULL) && !version_info.is_upper_bound_node(logical_node))
      {
        const ColorPoint &local_color = logical_node->get_color();
        parent->add_copy_user_above(usage, copy_term, local_color,
                                    version_info, copy_mask);
      }
      add_local_copy_user(usage, copy_term, false/*base*/, child_color, 
                          version_info, copy_mask);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::add_local_copy_user(const RegionUsage &usage, 
                                               Event copy_term, bool base_user,
                                               const ColorPoint &child_color,
                                               const VersionInfo &version_info,
                                               const FieldMask &copy_mask)
    //--------------------------------------------------------------------------
    {
      PhysicalUser *user;
      // We currently only use the version information for avoiding
      // WAR dependences on the same version number, so we don't need
      // it if we aren't only reading
      if (IS_READ_ONLY(usage))
        user = legion_new<PhysicalUser>(usage, child_color,
                                        version_info.get_versions(logical_node));
      else
        user = legion_new<PhysicalUser>(usage, child_color);
      user->add_reference();
      bool issue_collect = false;
      {
        AutoLock v_lock(view_lock);
        add_current_user(user, copy_term, copy_mask); 
        if (base_user)
          issue_collect = (outstanding_gc_events.find(copy_term) ==
                            outstanding_gc_events.end());
        outstanding_gc_events.insert(copy_term);
      }
      if (issue_collect)
        defer_collect_user(copy_term);
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::add_local_user(const RegionUsage &usage,
                                          Event term_event, bool base_user,
                                          const ColorPoint &child_color,
                                          const VersionInfo &version_info,
                                          const FieldMask &user_mask,
                                          std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ADD_LOCAL_USER_CALL);
#endif
      std::set<Event> dead_events;
      LegionMap<Event,FieldMask>::aligned filter_previous;
      FieldMask dominated;
      // Hold the lock in read-only mode when doing this part of the analysis
      {
        AutoLock v_lock(view_lock,1,false/*exclusive*/);
        FieldMask observed, non_dominated;
        for (LegionMap<Event,EventUsers>::aligned::const_iterator cit = 
              current_epoch_users.begin(); cit != 
              current_epoch_users.end(); cit++)
        {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY) && \
      !defined(EVENT_GRAPH_TRACE)
          // We're about to do a bunch of expensive tests, 
          // so first do something cheap to see if we can 
          // skip all the tests.
          if (cit->first.has_triggered())
          {
            dead_events.insert(cit->first);
            continue;
          }
#endif
          // No need to check for dependences on ourselves
          if (cit->first == term_event)
            continue;
          // If we arleady recorded a dependence, then we are done
          if (preconditions.find(cit->first) != preconditions.end())
            continue;
          const EventUsers &event_users = cit->second;
          if (event_users.single)
          {
            find_current_preconditions(cit->first, 
                                       event_users.users.single_user,
                                       event_users.user_mask,
                                       usage, user_mask, child_color,
                                       preconditions, observed, non_dominated);
          }
          else
          {
            // Otherwise do a quick test for non-interference on the
            // summary mask and iterate the users if needed
            if (!(user_mask * event_users.user_mask))
            {
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                    it = event_users.users.multi_users->begin(); it !=
                    event_users.users.multi_users->end(); it++)
              {
                // Unlike with the copy analysis, once we record a dependence
                // on an event, we are done, so we can keep going
                if (find_current_preconditions(cit->first,
                                               it->first, it->second,
                                               usage, user_mask, child_color,
                                               preconditions, observed, 
                                               non_dominated))
                  break;
              }
            }
          }
        }
        // See if we have any fields for which we need to do an analysis
        // on the previous fields
        // It's only safe to dominate fields that we observed
        dominated = (observed & (user_mask - non_dominated));
        // Update the non-dominated mask with what we
        // we're actually not-dominated by
        non_dominated = user_mask - dominated;
        const bool skip_analysis = !non_dominated;
        for (LegionMap<Event,EventUsers>::aligned::const_iterator pit = 
              previous_epoch_users.begin(); pit != 
              previous_epoch_users.end(); pit++)
        {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY) && \
      !defined(EVENT_GRAPH_TRACE)
          // We're about to do a bunch of expensive tests, 
          // so first do something cheap to see if we can 
          // skip all the tests.
          if (pit->first.has_triggered())
          {
            dead_events.insert(pit->first);
            continue;
          }
#endif
          // No need to check for dependences on ourselves
          if (pit->first == term_event)
            continue;
          // If we arleady recorded a dependence, then we are done
          if (preconditions.find(pit->first) != preconditions.end())
            continue;
          const EventUsers &event_users = pit->second;
          if (!!dominated)
          {
            FieldMask dom_overlap = event_users.user_mask & dominated;
            if (!!dom_overlap)
              filter_previous[pit->first] = dom_overlap;
          }
          // If we don't have any non-dominated fields we can skip the
          // rest of the analysis because we dominated everything
          if (skip_analysis)
            continue;
          if (event_users.single)
          {
            find_previous_preconditions(pit->first,
                                        event_users.users.single_user,
                                        event_users.user_mask,
                                        usage, non_dominated,
                                        child_color, preconditions);
          }
          else
          {
            if (!(non_dominated * event_users.user_mask))
            {
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                    it = event_users.users.multi_users->begin(); it !=
                    event_users.users.multi_users->end(); it++)
              {
                // Once we find a dependence we are can skip the rest
                if (find_previous_preconditions(pit->first,
                                                it->first, it->second,
                                                usage, non_dominated,
                                                child_color, preconditions))
                  break;
              }
            }
          }
        }
      }
      PhysicalUser *new_user = NULL;
      if (term_event.exists())
      {
        // Only need to record version info if we are read-only
        // because we only use the version info for avoiding WAR dependences
        if (IS_READ_ONLY(usage))
          new_user = legion_new<PhysicalUser>(usage, child_color,
                                      version_info.get_versions(logical_node));
        else
          new_user = legion_new<PhysicalUser>(usage, child_color);
        new_user->add_reference();
      }
      // No matter what, we retake the lock in exclusive mode so we
      // can handle any clean-up and add our user
      AutoLock v_lock(view_lock);
      if (!dead_events.empty())
      {
        for (std::set<Event>::const_iterator it = dead_events.begin();
              it != dead_events.end(); it++)
        {
          filter_local_users(*it);
        }
      }
      if (!filter_previous.empty())
        filter_previous_users(filter_previous);
      if (!!dominated)
        filter_current_users(dominated);
      // Finally add our user and return if we need to issue a GC meta-task
      if (term_event.exists())
      {
        add_current_user(new_user, term_event, user_mask);
        if (outstanding_gc_events.find(term_event) == 
            outstanding_gc_events.end())
        {
          outstanding_gc_events.insert(term_event);
          return base_user;
        }
      }
      return false;
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::find_current_preconditions(Event test_event,
                                                 const PhysicalUser *prev_user,
                                                 const FieldMask &prev_mask,
                                                 const RegionUsage &next_user,
                                                 const FieldMask &next_mask,
                                                 const ColorPoint &child_color,
                                                 std::set<Event> &preconditions,
                                                 FieldMask &observed,
                                                 FieldMask &non_dominated)
    //--------------------------------------------------------------------------
    {
      FieldMask overlap = prev_mask & next_mask;
      if (!overlap)
        return false;
      else
        observed |= overlap;
      if (child_color.is_valid())
      {
        // Same child, already done the analysis
        if (child_color == prev_user->child)
        {
          non_dominated |= overlap;
          return false;
        }
        // Disjoint children, keep going
        if (prev_user->child.is_valid() &&
            logical_node->are_children_disjoint(child_color,
                                                prev_user->child))
        {
          non_dominated |= overlap;
          return false;
        }
      }
      // Now do a dependence analysis
      DependenceType dt = check_dependence_type(prev_user->usage, next_user);
      switch (dt)
      {
        case NO_DEPENDENCE:
        case ATOMIC_DEPENDENCE:
        case SIMULTANEOUS_DEPENDENCE:
          {
            // No actual dependence
            non_dominated |= overlap;
            return false;
          }
        case TRUE_DEPENDENCE:
        case ANTI_DEPENDENCE:
          {
            // Actual dependence
            preconditions.insert(test_event);
            break;
          }
        default:
          assert(false); // should never get here
      }
      // If we made it to the end we recorded a dependence so return true
      return true;
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::find_previous_preconditions(Event test_event,
                                                 const PhysicalUser *prev_user,
                                                 const FieldMask &prev_mask,
                                                 const RegionUsage &next_user,
                                                 const FieldMask &next_mask,
                                                 const ColorPoint &child_color,
                                                 std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      if (child_color.is_valid())
      {
        // Same child: did analysis below
        if (child_color == prev_user->child)
          return false;
        if (prev_user->child.is_valid() &&
            logical_node->are_children_disjoint(child_color,
                                          prev_user->child))
          return false;
      }
      FieldMask overlap = prev_mask & next_mask;
      if (!overlap)
        return false;
      // Now do a dependence analysis
      DependenceType dt = check_dependence_type(prev_user->usage, next_user);
      switch (dt)
      {
        case NO_DEPENDENCE:
        case ATOMIC_DEPENDENCE:
        case SIMULTANEOUS_DEPENDENCE:
          {
            // No actual dependence
            return false;
          }
        case TRUE_DEPENDENCE:
        case ANTI_DEPENDENCE:
          {
            // Actual dependence
            preconditions.insert(test_event);
            break;
          }
        default:
          assert(false); // should never get here
      }
      // If we make it here, we recorded a dependence
      return true;
    }
 
    //--------------------------------------------------------------------------
    void MaterializedView::find_copy_preconditions(ReductionOpID redop, 
                                                   bool reading, 
                                                   const FieldMask &copy_mask,
                                                const VersionInfo &version_info,
                             LegionMap<Event,FieldMask>::aligned &preconditions)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FIND_COPY_PRECONDITIONS_CALL);
#endif
      Event start_use_event = manager->get_use_event();
      if (start_use_event.exists())
      {
        LegionMap<Event,FieldMask>::aligned::iterator finder = 
          preconditions.find(start_use_event);
        if (finder == preconditions.end())
          preconditions[start_use_event] = copy_mask;
        else
          finder->second |= copy_mask;
      }
      if ((parent != NULL) && !version_info.is_upper_bound_node(logical_node))
      {
        const ColorPoint &local_point = logical_node->get_color();
        parent->find_copy_preconditions_above(redop, reading, copy_mask,
                                      local_point, version_info, preconditions);
      }
      find_local_copy_preconditions(redop, reading, copy_mask, 
                                    ColorPoint(), version_info, preconditions);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::find_copy_preconditions_above(ReductionOpID redop,
                                                         bool reading,
                                                     const FieldMask &copy_mask,
                                                  const ColorPoint &child_color,
                                                const VersionInfo &version_info,
                             LegionMap<Event,FieldMask>::aligned &preconditions)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FIND_COPY_PRECONDITIONS_ABOVE_CALL);
#endif
      if ((parent != NULL) && !version_info.is_upper_bound_node(logical_node))
      {
        const ColorPoint &local_point = logical_node->get_color();
        parent->find_copy_preconditions_above(redop, reading, copy_mask,
                                      local_point, version_info, preconditions);
      }
      find_local_copy_preconditions(redop, reading, copy_mask, 
                                    child_color, version_info, preconditions);
    }
    
    //--------------------------------------------------------------------------
    void MaterializedView::find_local_copy_preconditions(ReductionOpID redop,
                                                         bool reading,
                                                     const FieldMask &copy_mask,
                                                  const ColorPoint &child_color,
                                                const VersionInfo &version_info,
                             LegionMap<Event,FieldMask>::aligned &preconditions)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, FIND_LOCAL_COPY_PRECONDITIONS_CALL);
#endif
      // First get our set of version data in case we need it 
      const FieldVersions *versions = version_info.get_versions(logical_node);
      std::set<Event> dead_events;
      LegionMap<Event,FieldMask>::aligned filter_previous;
      FieldMask dominated;
      {
        // Hold the lock in read-only mode when doing this analysis
        AutoLock v_lock(view_lock,1,false/*exclusive*/);
        FieldMask observed, non_dominated;
        for (LegionMap<Event,EventUsers>::aligned::const_iterator cit = 
              current_epoch_users.begin(); cit != 
              current_epoch_users.end(); cit++)
        {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY) && \
      !defined(EVENT_GRAPH_TRACE)
          // We're about to do a bunch of expensive tests, 
          // so first do something cheap to see if we can 
          // skip all the tests.
          if (cit->first.has_triggered())
          {
            dead_events.insert(cit->first);
            continue;
          }
#endif
          const EventUsers &event_users = cit->second;
          if (event_users.single)
          {
            find_current_copy_preconditions(cit->first,
                                            event_users.users.single_user,
                                            event_users.user_mask,
                                            redop, reading, copy_mask,
                                            child_color, versions,
                                            preconditions, observed, 
                                            non_dominated);
          }
          else
          {
            // Otherwise do a quick test for non-interference on the
            // summary mask and iterate the users if needed
            if (!(copy_mask * event_users.user_mask))
            {
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                    it = event_users.users.multi_users->begin(); it !=
                    event_users.users.multi_users->end(); it++)
              {
                // You might think after we record one event dependence that
                // would be enough to skip the other users for the same event,
                // but we actually do need precise field information for each
                // event to properly issue dependent copies
                find_current_copy_preconditions(cit->first,it->first,it->second,
                                                redop, reading, copy_mask,
                                                child_color, versions,
                                                preconditions, observed,
                                                non_dominated);
              }
            }
          }
        }
        // See if we have any fields for which we need to do an analysis
        // on the previous fields
        // It's only safe to dominate fields that we observed
        dominated = (observed & (copy_mask - non_dominated));
        // Update the non-dominated mask with what we
        // we're actually not-dominated by
        non_dominated = copy_mask - dominated;
        const bool skip_analysis = !non_dominated;
        for (LegionMap<Event,EventUsers>::aligned::const_iterator pit = 
              previous_epoch_users.begin(); pit != 
              previous_epoch_users.end(); pit++)
        {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY) && \
      !defined(EVENT_GRAPH_TRACE)
          // We're about to do a bunch of expensive tests, 
          // so first do something cheap to see if we can 
          // skip all the tests.
          if (pit->first.has_triggered())
          {
            dead_events.insert(pit->first);
            continue;
          }
#endif
          const EventUsers &event_users = pit->second;
          if (!!dominated)
          {
            FieldMask dom_overlap = event_users.user_mask & dominated;
            if (!!dom_overlap)
              filter_previous[pit->first] = dom_overlap;
          }
          // If we don't have any non-dominated fields we can skip the
          // rest of the analysis because we dominated everything
          if (skip_analysis)
            continue;
          if (event_users.single)
          {
            find_previous_copy_preconditions(pit->first,
                                             event_users.users.single_user,
                                             event_users.user_mask,
                                             redop, reading, non_dominated,
                                             child_color, versions,
                                             preconditions);
          }
          else
          {
            if (!(non_dominated * event_users.user_mask))
            {
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                    it = event_users.users.multi_users->begin(); it !=
                    event_users.users.multi_users->end(); it++)
              {
                find_previous_copy_preconditions(pit->first,
                                                 it->first, it->second,
                                                 redop, reading, 
                                                 non_dominated, child_color,
                                                 versions, preconditions);
              }
            }
          }
        }
      }
      // Release the lock, if we have any modifications to make, then
      // retake the lock in exclusive mode
      if (!dead_events.empty() || !filter_previous.empty() || !!dominated)
      {
        AutoLock v_lock(view_lock);
        if (!dead_events.empty())
        {
          for (std::set<Event>::const_iterator it = dead_events.begin();
                it != dead_events.end(); it++)
          {
            filter_local_users(*it);
          }
        }
        if (!filter_previous.empty())
          filter_previous_users(filter_previous);
        if (!!dominated)
          filter_current_users(dominated);
      }
    }

    //--------------------------------------------------------------------------
    void MaterializedView::find_current_copy_preconditions(Event test_event,
                                              const PhysicalUser *user,
                                              const FieldMask &user_mask,
                                              ReductionOpID redop, bool reading,
                                              const FieldMask &copy_mask,
                                              const ColorPoint &child_color,
                                              const FieldVersions *versions,
                             LegionMap<Event,FieldMask>::aligned &preconditions,
                                              FieldMask &observed,
                                              FieldMask &non_dominated)
    //--------------------------------------------------------------------------
    {
      FieldMask overlap = copy_mask & user_mask;
      if (!overlap)
        return;
      else
        observed |= overlap;
      if (child_color.is_valid())
      {
        // Same child, already done the analysis
        if (child_color == user->child)
        {
          non_dominated |= overlap;
          return;
        }
        // Disjoint children, keep going
        if (user->child.is_valid() &&
            logical_node->are_children_disjoint(child_color,
                                                user->child))
        {
          non_dominated |= overlap;
          return;
        }
      }
      // Now do a dependence analysis
      if (reading && IS_READ_ONLY(user->usage))
      {
        non_dominated |= overlap;
        return;
      }
      if ((redop > 0) && (user->usage.redop == redop))
      {
        non_dominated |= overlap;
        return;
      }
      // Check for WAR and WAW dependences, if we have one we
      // can see if we are writing the same version number
      // in which case there is no need for a dependence, thank
      // you wonchan and mini-aero for raising this case
      if (!reading && (redop == 0) && !IS_REDUCE(user->usage) &&
          user->same_versions(overlap, versions))
      {
        non_dominated |= overlap;
        return;
      }
      // If we make it here, then we have a dependence, so record it 
      LegionMap<Event,FieldMask>::aligned::iterator finder = 
        preconditions.find(test_event);
      if (finder == preconditions.end())
        preconditions[test_event] = overlap;
      else
        finder->second |= overlap;
    }

    //--------------------------------------------------------------------------
    void MaterializedView::find_previous_copy_preconditions(Event test_event,
                                              const PhysicalUser *user,
                                              const FieldMask &user_mask,
                                              ReductionOpID redop, bool reading,
                                              const FieldMask &copy_mask,
                                              const ColorPoint &child_color,
                                              const FieldVersions *versions,
                        LegionMap<Event,FieldMask>::aligned &preconditions)
    //--------------------------------------------------------------------------
    { 
      if (child_color.is_valid())
      {
        // Same child: did analysis below
        if (child_color == user->child)
          return;
        if (user->child.is_valid() &&
            logical_node->are_children_disjoint(child_color,
                                                user->child))
          return;
      }
      FieldMask overlap = user_mask & copy_mask;
      if (!overlap)
        return;
      if (reading && IS_READ_ONLY(user->usage))
        return;
      if ((redop > 0) && (user->usage.redop == redop))
        return;
      if (!reading && (redop == 0) && !IS_REDUCE(user->usage) &&
          user->same_versions(overlap, versions))
        return;
      // Otherwise record the dependence
      LegionMap<Event,FieldMask>::aligned::iterator finder = 
        preconditions.find(test_event);
      if (finder == preconditions.end())
        preconditions[test_event] = overlap;
      else
        finder->second |= overlap;
    }

    //--------------------------------------------------------------------------
    void MaterializedView::filter_previous_users(
                     const LegionMap<Event,FieldMask>::aligned &filter_previous)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<Event,FieldMask>::aligned::const_iterator fit = 
            filter_previous.begin(); fit != filter_previous.end(); fit++)
      {
        LegionMap<Event,EventUsers>::aligned::iterator finder = 
          previous_epoch_users.find(fit->first);
        // Someone might have already removed it
        if (finder == previous_epoch_users.end())
          continue;
        finder->second.user_mask -= fit->second;
        if (!finder->second.user_mask)
        {
          // We can delete the whole entry
          if (finder->second.single)
          {
            PhysicalUser *user = finder->second.users.single_user;
            if (user->remove_reference())
              legion_delete(user);
          }
          else
          {
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                  it = finder->second.users.multi_users->begin(); it !=
                  finder->second.users.multi_users->end(); it++)
            {
              if (it->first->remove_reference())
                legion_delete(it->first);
            }
            // Delete the map too
            delete finder->second.users.multi_users;
          }
          previous_epoch_users.erase(finder);
        }
        else if (!finder->second.single) // only need to filter for non-single
        {
          // Filter out the users for the dominated fields
          std::vector<PhysicalUser*> to_delete;
          for (LegionMap<PhysicalUser*,FieldMask>::aligned::iterator it = 
                finder->second.users.multi_users->begin(); it !=
                finder->second.users.multi_users->end(); it++)
          {
            it->second -= fit->second; 
            if (!it->second)
              to_delete.push_back(it->first);
          }
          if (!to_delete.empty())
          {
            for (std::vector<PhysicalUser*>::const_iterator it = 
                  to_delete.begin(); it != to_delete.end(); it++)
            {
              finder->second.users.multi_users->erase(*it);
              if ((*it)->remove_reference())
                legion_delete(*it);
            }
            // See if we can shrink this back down
            if (finder->second.users.multi_users->size() == 1)
            {
              LegionMap<PhysicalUser*,FieldMask>::aligned::iterator first_it =
                            finder->second.users.multi_users->begin();     
#ifdef DEBUG_HIGH_LEVEL
              // This summary mask should dominate
              assert(!(first_it->second - finder->second.user_mask));
#endif
              PhysicalUser *user = first_it->first;
              finder->second.user_mask = first_it->second;
              delete finder->second.users.multi_users;
              finder->second.users.single_user = user;
              finder->second.single = true;
            }
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void MaterializedView::filter_current_users(const FieldMask &dominated)
    //--------------------------------------------------------------------------
    {
      std::vector<Event> events_to_delete;
      for (LegionMap<Event,EventUsers>::aligned::iterator cit = 
            current_epoch_users.begin(); cit !=
            current_epoch_users.end(); cit++)
      {
#if !defined(LEGION_LOGGING) && !defined(LEGION_SPY) && \
      !defined(EVENT_GRAPH_TRACE)
        if (cit->first.has_triggered())
        {
          EventUsers &current_users = cit->second;
          if (current_users.single)
          {
            if (current_users.users.single_user->remove_reference())
              legion_delete(current_users.users.single_user);
          }
          else
          {
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::iterator it = 
                  current_users.users.multi_users->begin(); it !=
                  current_users.users.multi_users->end(); it++)
            {
              if (it->first->remove_reference())
                legion_delete(it->first);
            }
            delete current_users.users.multi_users;
          }
          events_to_delete.push_back(cit->first);
          continue;
        }
#endif
        EventUsers &current_users = cit->second;
        FieldMask summary_overlap = current_users.user_mask & dominated;
        if (!summary_overlap)
          continue;
        current_users.user_mask -= summary_overlap;
        EventUsers &prev_users = previous_epoch_users[cit->first];
        if (current_users.single)
        {
          PhysicalUser *user = current_users.users.single_user;
          if (prev_users.single)
          {
            // Single, see if something exists there yet
            if (prev_users.users.single_user == NULL)
            {
              prev_users.users.single_user = user; 
              prev_users.user_mask = summary_overlap;
              if (!current_users.user_mask) // reference flows back
                events_to_delete.push_back(cit->first); 
              else
                user->add_reference(); // add a reference
            }
            else if (prev_users.users.single_user == user)
            {
              // Same user, update the fields 
              prev_users.user_mask |= summary_overlap;
              if (!current_users.user_mask)
              {
                events_to_delete.push_back(cit->first);
                user->remove_reference(); // remove unnecessary reference
              }
            }
            else
            {
              // Go to multi mode
              LegionMap<PhysicalUser*,FieldMask>::aligned *new_map = 
                            new LegionMap<PhysicalUser*,FieldMask>::aligned();
              (*new_map)[prev_users.users.single_user] = prev_users.user_mask;
              (*new_map)[user] = summary_overlap;
              if (!current_users.user_mask) // reference flows back
                events_to_delete.push_back(cit->first); 
              else
                user->add_reference();
              prev_users.user_mask |= summary_overlap;
              prev_users.users.multi_users = new_map;
              prev_users.single = false;
            }
          }
          else
          {
            // Already multi
            prev_users.user_mask |= summary_overlap;
            // See if we can find it in the multi-set
            LegionMap<PhysicalUser*,FieldMask>::aligned::iterator finder = 
              prev_users.users.multi_users->find(user);
            if (finder == prev_users.users.multi_users->end())
            {
              // Couldn't find it
              (*prev_users.users.multi_users)[user] = summary_overlap;
              if (!current_users.user_mask) // reference flows back
                events_to_delete.push_back(cit->first); 
              else
                user->add_reference();
            }
            else
            {
              // Found it, update it 
              finder->second |= summary_overlap;
              if (!current_users.user_mask)
              {
                events_to_delete.push_back(cit->first);
                user->remove_reference(); // remove redundant reference
              }
            }
          }
        }
        else
        {
          // Many things, filter them and move them back
          if (!current_users.user_mask)
          {
            // Moving the whole set back, see what the previous looks like
            if (prev_users.single)
            {
              if (prev_users.users.single_user != NULL)
              {
                // Merge the one user into this map so we can move 
                // the whole map back
                PhysicalUser *user = prev_users.users.single_user;  
                LegionMap<PhysicalUser*,FieldMask>::aligned::iterator finder =
                  current_users.users.multi_users->find(user);
                if (finder == current_users.users.multi_users->end())
                {
                  // Add it reference is already there
                  (*current_users.users.multi_users)[user] = 
                    prev_users.user_mask;
                }
                else
                {
                  // Already there, update it and remove duplicate reference
                  finder->second |= prev_users.user_mask;
                  user->remove_reference();
                }
              }
              // Now just move the map back
              prev_users.user_mask |= summary_overlap;
              prev_users.users.multi_users = current_users.users.multi_users;
              prev_users.single = false;
            }
            else
            {
              // merge the two sets
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator
                    it = current_users.users.multi_users->begin();
                    it != current_users.users.multi_users->end(); it++)
              {
                // See if we can find it
                LegionMap<PhysicalUser*,FieldMask>::aligned::iterator finder = 
                  prev_users.users.multi_users->find(it->first);
                if (finder == prev_users.users.multi_users->end())
                {
                  // Didn't find it, just move it back, reference moves back
                  prev_users.users.multi_users->insert(*it);
                }
                else
                {
                  finder->second |= it->second; 
                  // Remove the duplicate reference
                  it->first->remove_reference();
                }
              }
              prev_users.user_mask |= summary_overlap;
              // Now delete the set
              delete current_users.users.multi_users;
            }
            events_to_delete.push_back(cit->first);
          }
          else
          {
            // Only send back filtered users
            std::vector<PhysicalUser*> to_delete;
            if (prev_users.single)
            {
              // Make a new map to send back  
              LegionMap<PhysicalUser*,FieldMask>::aligned *new_map = 
                            new LegionMap<PhysicalUser*,FieldMask>::aligned();
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::iterator it = 
                    current_users.users.multi_users->begin(); it !=
                    current_users.users.multi_users->end(); it++)
              {
                FieldMask overlap = summary_overlap & it->second;
                if (!overlap)
                  continue;
                // Can move without checking
                (*new_map)[it->first] = overlap;
                it->second -= overlap;
                if (!it->second)
                  to_delete.push_back(it->first); // reference flows back
                else
                  it->first->add_reference(); // need new reference
              }
              // Also capture the existing previous user if there is one
              if (prev_users.users.single_user != NULL)
              {
                LegionMap<PhysicalUser*,FieldMask>::aligned::iterator finder = 
                  new_map->find(prev_users.users.single_user);
                if (finder == new_map->end())
                {
                  (*new_map)[prev_users.users.single_user] = 
                    prev_users.user_mask;
                }
                else
                {
                  finder->second |= prev_users.user_mask;
                  // Remove redundant reference
                  finder->first->remove_reference();
                }
              }
              // Make the new map the previous set
              prev_users.user_mask |= summary_overlap;
              prev_users.users.multi_users = new_map;
              prev_users.single = false;
            }
            else
            {
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::iterator it =
                    current_users.users.multi_users->begin(); it !=
                    current_users.users.multi_users->end(); it++)
              {
                FieldMask overlap = summary_overlap & it->second; 
                if (!overlap)
                  continue;
                it->second -= overlap;
                LegionMap<PhysicalUser*,FieldMask>::aligned::iterator finder = 
                  prev_users.users.multi_users->find(it->first);
                // See if it already exists
                if (finder == prev_users.users.multi_users->end())
                {
                  // Doesn't exist yet, so add it 
                  (*prev_users.users.multi_users)[it->first] = overlap;
                  if (!it->second) // reference flows back
                    to_delete.push_back(it->first);
                  else
                    it->first->add_reference();
                }
                else
                {
                  // Already exists so update it
                  finder->second |= overlap;
                  if (!it->second)
                  {
                    to_delete.push_back(it->first);
                    // Remove redundant reference
                    it->first->remove_reference();
                  }
                }
              }
              prev_users.user_mask |= summary_overlap;
            }
            // See if we can collapse this map back down
            if (!to_delete.empty())
            {
              for (std::vector<PhysicalUser*>::const_iterator it = 
                    to_delete.begin(); it != to_delete.end(); it++)
              {
                current_users.users.multi_users->erase(*it);
              }
              if (current_users.users.multi_users->size() == 1)
              {
                LegionMap<PhysicalUser*,FieldMask>::aligned::iterator 
                  first_it = current_users.users.multi_users->begin();
#ifdef DEBUG_HIGH_LEVEL
                // Should dominate as an upper bound
                assert(!(first_it->second - current_users.user_mask));
#endif
                PhysicalUser *user = first_it->first;
                current_users.user_mask = first_it->second;
                delete current_users.users.multi_users;
                current_users.users.single_user = user;   
                current_users.single = true;
              }
            }
          }
        }
      }
      // Delete any events
      if (!events_to_delete.empty())
      {
        for (std::vector<Event>::const_iterator it = events_to_delete.begin();
              it != events_to_delete.end(); it++)
        {
          current_epoch_users.erase(*it); 
        }
      }
    }

    //--------------------------------------------------------------------------
    void MaterializedView::add_current_user(PhysicalUser *user, 
                                            Event term_event,
                                            const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      // Reference should already have been added
      EventUsers &event_users = current_epoch_users[term_event];
      if (event_users.single)
      {
        if (event_users.users.single_user == NULL)
        {
          // make it the entry
          event_users.users.single_user = user;
          event_users.user_mask = user_mask;
        }
        else
        {
          // convert to multi
          LegionMap<PhysicalUser*,FieldMask>::aligned *new_map = 
                           new LegionMap<PhysicalUser*,FieldMask>::aligned();
          (*new_map)[event_users.users.single_user] = event_users.user_mask;
          (*new_map)[user] = user_mask;
          event_users.user_mask |= user_mask;
          event_users.users.multi_users = new_map;
          event_users.single = false;
        }
      }
      else
      {
        // Add it to the set 
        (*event_users.users.multi_users)[user] = user_mask;
        event_users.user_mask |= user_mask;
      }
    }

    //--------------------------------------------------------------------------
    void MaterializedView::add_previous_user(PhysicalUser *user, 
                                             Event term_event,
                                             const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      // Reference should already have been added
      EventUsers &event_users = previous_epoch_users[term_event];
      if (event_users.single)
      {
        if (event_users.users.single_user == NULL)
        {
          // make it the entry
          event_users.users.single_user = user;
          event_users.user_mask = user_mask;
        }
        else
        {
          // convert to multi
          LegionMap<PhysicalUser*,FieldMask>::aligned *new_map = 
                           new LegionMap<PhysicalUser*,FieldMask>::aligned();
          (*new_map)[event_users.users.single_user] = event_users.user_mask;
          (*new_map)[user] = user_mask;
          event_users.user_mask |= user_mask;
          event_users.users.multi_users = new_map;
          event_users.single = false;
        }
      }
      else
      {
        // Add it to the set 
        (*event_users.users.multi_users)[user] = user_mask;
        event_users.user_mask |= user_mask;
      }
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::has_war_dependence_above(const RegionUsage &usage,
                                                    const FieldMask &user_mask,
                                                  const ColorPoint &child_color)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, HAS_WAR_DEPENDENCE_ABOVE_CALL);
#endif
      const ColorPoint &local_color = logical_node->get_color();
      if (has_local_war_dependence(usage, user_mask, child_color, local_color))
        return true;
      if (parent != NULL)
        return parent->has_war_dependence_above(usage, user_mask, local_color);
      return false;
    }

    //--------------------------------------------------------------------------
    bool MaterializedView::has_local_war_dependence(const RegionUsage &usage,
                                                    const FieldMask &user_mask,
                                                  const ColorPoint &child_color,
                                                  const ColorPoint &local_color)
    //--------------------------------------------------------------------------
    {
      // Do the local analysis
      FieldMask observed;
      AutoLock v_lock(view_lock,1,false/*exclusive*/);
      for (LegionMap<Event,EventUsers>::aligned::const_iterator cit = 
            current_epoch_users.begin(); cit != 
            current_epoch_users.end(); cit++)
      {
        const EventUsers &event_users = cit->second;
        FieldMask overlap = user_mask & event_users.user_mask;
        if (!overlap)
          continue;
        else
          observed |= overlap;
        if (event_users.single)
        {
          if (IS_READ_ONLY(event_users.users.single_user->usage))
            return true;
        }
        else
        {
          for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator it =
                event_users.users.multi_users->begin(); it !=
                event_users.users.multi_users->end(); it++)
          {
            FieldMask overlap2 = user_mask & it->second;
            if (!overlap2)
              continue;
            if (IS_READ_ONLY(it->first->usage))
              return true;
          }
        }
      }
      FieldMask not_observed = user_mask - observed;
      // If we had fields that were not observed, check the previous list
      if (!!not_observed)
      {
        for (LegionMap<Event,EventUsers>::aligned::const_iterator pit = 
              previous_epoch_users.begin(); pit != 
              previous_epoch_users.end(); pit++)
        {
          const EventUsers &event_users = pit->second;
          if (event_users.single)
          {
            FieldMask overlap = user_mask & event_users.user_mask;
            if (!overlap)
              continue;
            if (IS_READ_ONLY(event_users.users.single_user->usage))
              return true;
          }
          else
          {
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                  it = event_users.users.multi_users->begin(); it !=
                  event_users.users.multi_users->end(); it++)
            {
              FieldMask overlap = user_mask & it->second;
              if (!overlap)
                continue;
              if (IS_READ_ONLY(it->first->usage))
                return true;
            }
          }
        }
      }
      return false;
    }
    
#if 0
    //--------------------------------------------------------------------------
    void MaterializedView::update_versions(const FieldMask &update_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, UPDATE_VERSIONS_CALL);
#endif
      std::vector<VersionID> to_delete;
      LegionMap<VersionID,FieldMask>::aligned new_versions;
      for (LegionMap<VersionID,FieldMask>::aligned::iterator it = 
            current_versions.begin(); it != current_versions.end(); it++)
      {
        FieldMask overlap = it->second & update_mask;
        if (!!overlap)
        {
          new_versions[(it->first+1)] = overlap; 
          it->second -= update_mask;
          if (!it->second)
            to_delete.push_back(it->first);
        }
      }
      for (std::vector<VersionID>::const_iterator it = to_delete.begin();
            it != to_delete.end(); it++)
      {
        current_versions.erase(*it);
      }
      for (LegionMap<VersionID,FieldMask>::aligned::const_iterator it = 
            new_versions.begin(); it != new_versions.end(); it++)
      {
        LegionMap<VersionID,FieldMask>::aligned::iterator finder = 
          current_versions.find(it->first);
        if (finder == current_versions.end())
          current_versions.insert(*it);
        else
          finder->second |= it->second;
      }
    }
#endif

    //--------------------------------------------------------------------------
    void MaterializedView::filter_local_users(Event term_event) 
    //--------------------------------------------------------------------------
    {
      // Don't do this if we are in Legion Spy since we want to see
      // all of the dependences on an instance
#if !defined(LEGION_SPY) && !defined(LEGION_LOGGING) && \
      !defined(EVENT_GRAPH_TRACE)
      std::set<Event>::iterator event_finder = 
        outstanding_gc_events.find(term_event); 
      if (event_finder != outstanding_gc_events.end())
      {
        LegionMap<Event,EventUsers>::aligned::iterator current_finder = 
          current_epoch_users.find(term_event);
        if (current_finder != current_epoch_users.end())
        {
          EventUsers &event_users = current_finder->second;
          if (event_users.single)
          {
            if (event_users.users.single_user->remove_reference())
              legion_delete(event_users.users.single_user);
          }
          else
          {
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::iterator
                  it = event_users.users.multi_users->begin(); it !=
                  event_users.users.multi_users->end(); it++)
            {
              if (it->first->remove_reference())
                legion_delete(it->first);
            }
            delete event_users.users.multi_users;
          }
          current_epoch_users.erase(current_finder);
        }
        LegionMap<Event,EventUsers>::aligned::iterator previous_finder = 
          previous_epoch_users.find(term_event);
        if (previous_finder != previous_epoch_users.end())
        {
          EventUsers &event_users = previous_finder->second; 
          if (event_users.single)
          {
            if (event_users.users.single_user->remove_reference())
              legion_delete(event_users.users.single_user);
          }
          else
          {
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::iterator
                  it = event_users.users.multi_users->begin(); it !=
                  event_users.users.multi_users->end(); it++)
            {
              if (it->first->remove_reference())
                legion_delete(it->first);
            }
            delete event_users.users.multi_users;
          }
          previous_epoch_users.erase(previous_finder);
        }
        outstanding_gc_events.erase(event_finder);
      }
#endif
    }

    //--------------------------------------------------------------------------
    void MaterializedView::find_atomic_reservations(InstanceRef &target,
                                                    const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      // Keep going up the tree until we get to the root
      if (parent == NULL)
      {
        // Compute the field set
        std::set<FieldID> atomic_fields;
        logical_node->column_source->get_field_set(mask, atomic_fields);
        std::vector<std::pair<FieldID,Reservation> > to_send_back;
        // Take our lock and lookup the needed fields in order
        {
          AutoLock v_lock(view_lock);
          for (std::set<FieldID>::const_iterator it = atomic_fields.begin();
                it != atomic_fields.end(); it++)
          {
            std::map<FieldID,Reservation>::const_iterator finder = 
              atomic_reservations.find(*it);
            if (finder == atomic_reservations.end())
            {
              // Make a new reservation and add it to the set
              Reservation handle = Reservation::create_reservation();
              atomic_reservations[*it] = handle;
              target.add_reservation(handle);
              if (!is_owner())
                to_send_back.push_back(
                        std::pair<FieldID,Reservation>(*it, handle));
            }
            else
              target.add_reservation(finder->second);
          }
        }
        if (!to_send_back.empty())
          send_back_atomic_reservations(to_send_back);
      }
      else
        parent->find_atomic_reservations(target, mask);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::set_descriptor(FieldDataDescriptor &desc,
                                          unsigned fid_idx) const
    //--------------------------------------------------------------------------
    {
      // Get the low-level index space
      const Domain &dom = logical_node->get_domain_no_wait();
      desc.index_space = dom.get_index_space();
      // Then ask the manager to fill in the rest of the information
      manager->set_descriptor(desc, fid_idx);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::send_back_atomic_reservations(
                  const std::vector<std::pair<FieldID,Reservation> > &send_back)
    //--------------------------------------------------------------------------
    {
      Serializer rez;
      {
        RezCheck z(rez);
        rez.serialize(did);
        bool is_region = logical_node->is_region();
        rez.serialize<bool>(is_region);
        if (is_region)
          rez.serialize(logical_node->as_region_node()->handle);
        else
          rez.serialize(logical_node->as_partition_node()->handle);
        rez.serialize<size_t>(send_back.size());
        for (std::vector<std::pair<FieldID,Reservation> >::const_iterator it =
              send_back.begin(); it != send_back.end(); it++)
        {
          rez.serialize(it->first);
          rez.serialize(it->second);
        }
      }
      context->runtime->send_back_atomic(owner_space, rez);
    }

    //--------------------------------------------------------------------------
    void MaterializedView::process_atomic_reservations(Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      size_t num_handles;
      derez.deserialize(num_handles);
      AutoLock v_lock(view_lock);
      for (unsigned idx = 0; idx < num_handles; idx++)
      {
        FieldID fid;
        derez.deserialize(fid);
        Reservation handle;
        derez.deserialize(handle);
        // TODO: If we ever hit this assertion then we need to serialize
        // atomic mappings occurring on different nodes at the same time.
        // This might occur because two tasks with atomic read-only
        // privileges mapped on different nodes and generated different
        // reservations for the same instance. We can either serialize
        // the mapping process or deal with sets of reservations for some
        // fields. Either way we defer this for later work.
        assert(atomic_reservations.find(fid) == atomic_reservations.end());
        atomic_reservations[fid] = handle;
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void MaterializedView::handle_send_back_atomic(
                                                          RegionTreeForest *ctx,
                                                          Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      bool is_region;
      derez.deserialize(is_region);
      RegionTreeNode *node;
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        node = ctx->get_node(handle);
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        node = ctx->get_node(handle);
      }
      LogicalView *target_view = node->find_view(did);
#ifdef DEBUG_HIGH_LEVEL 
      assert(target_view->is_instance_view());
#endif
      InstanceView *inst_view = target_view->as_instance_view();
#ifdef DEBUG_HIGH_LEVEL
      assert(inst_view->is_materialized_view()); 
#endif
      inst_view->as_materialized_view()->process_atomic_reservations(derez);
    }

    //--------------------------------------------------------------------------
    /*static*/ void MaterializedView::handle_send_materialized_view(
                   Runtime *runtime, Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez); 
      DistributedID did;
      derez.deserialize(did);
      DistributedID manager_did;
      derez.deserialize(manager_did);
      LogicalRegion handle;
      derez.deserialize(handle);
      AddressSpaceID owner_space;
      derez.deserialize(owner_space);
      unsigned depth;
      derez.deserialize(depth);
      bool is_persistent;
      derez.deserialize(is_persistent);

      RegionNode *target_node = runtime->forest->get_node(handle); 
      PhysicalManager *phy_man = target_node->find_manager(manager_did);
#ifdef DEBUG_HIGH_LEVEL
      assert(!phy_man->is_reduction_manager());
#endif
      InstanceManager *inst_manager = phy_man->as_instance_manager();

      MaterializedView *new_view = legion_new<MaterializedView>(runtime->forest,
                                      did, owner_space, runtime->address_space,
                                      target_node, inst_manager, 
                                      (MaterializedView*)NULL/*parent*/,
                                      depth, false/*don't register yet*/,
                                      is_persistent);
      if (!target_node->register_logical_view(new_view))
        legion_delete(new_view);
      else
        new_view->update_remote_instances(source);
    }

    //--------------------------------------------------------------------------
    /*static*/ void MaterializedView::handle_send_update(Runtime *runtime,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez); 
      bool is_region;
      derez.deserialize(is_region);
      RegionTreeNode *target_node;
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        target_node = runtime->forest->get_node(handle);
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        target_node = runtime->forest->get_node(handle);
      }
      DistributedID did;
      derez.deserialize(did);
      LogicalView *view = target_node->find_view(did);
#ifdef DEBUG_HIGH_LEVEL
      assert(view->is_instance_view());
      assert(view->as_instance_view()->is_materialized_view());
#endif
      MaterializedView *mat_view = 
        view->as_instance_view()->as_materialized_view();
      mat_view->process_update(derez, source);
    }

    //--------------------------------------------------------------------------
    /*static*/ void MaterializedView::handle_make_persistent(Runtime *runtime,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      UniqueID remote_owner_uid;
      derez.deserialize(remote_owner_uid);
      SingleTask *remote_ctx;
      derez.deserialize(remote_ctx);
      LogicalRegion handle;
      derez.deserialize(handle);
      DistributedID did;
      derez.deserialize(did);
      unsigned parent_idx;
      derez.deserialize(parent_idx);
      UserEvent to_trigger;
      derez.deserialize(to_trigger);

      SingleTask *parent_ctx = runtime->find_remote_context(remote_owner_uid,
                                                            remote_ctx);
      RegionTreeNode *node = runtime->forest->get_node(handle);
      LogicalView *view = node->find_view(did);
#ifdef DEBUG_HIGH_LEVEL
      assert(view->is_instance_view());
      assert(view->as_instance_view()->is_materialized_view());
#endif
      MaterializedView *mat_view = 
        view->as_instance_view()->as_materialized_view();
      mat_view->make_persistent(parent_ctx, parent_idx, source, to_trigger);
    }

    /////////////////////////////////////////////////////////////
    // DeferredView 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    DeferredView::DeferredView(RegionTreeForest *ctx, DistributedID did,
                               AddressSpaceID owner_sp, AddressSpaceID local_sp,
                               RegionTreeNode *node, bool register_now)
      : LogicalView(ctx, did, owner_sp, local_sp, node, register_now)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    DeferredView::~DeferredView(void)
    //--------------------------------------------------------------------------
    {
      // Remove resource references on our valid reductions
      for (std::deque<ReductionEpoch>::iterator rit = reduction_epochs.begin();
            rit != reduction_epochs.end(); rit++)
      {
        ReductionEpoch &epoch = *rit;
        for (std::set<ReductionView*>::const_iterator it = 
              epoch.views.begin(); it != epoch.views.end(); it++)
        {
          if ((*it)->remove_nested_resource_ref(did))
            legion_delete(*it);
        }
      }
      reduction_epochs.clear();
    }

    //--------------------------------------------------------------------------
    void DeferredView::update_reduction_views(ReductionView *view,
                                              const FieldMask &valid,
                                              bool update_parent /*= true*/)
    //--------------------------------------------------------------------------
    {
      // First if we have a parent, we have to update its valid reduciton views
      if (update_parent && has_parent())
      {
        DeferredView *parent_view = get_parent()->as_deferred_view();
        parent_view->update_reduction_views_above(view, valid, this);
      }
      if ((logical_node == view->logical_node) ||  
          logical_node->intersects_with(view->logical_node))
      {
        // If it intersects, then we need to update our local reductions
        // and then also update any child reductions
        update_local_reduction_views(view, valid);
        update_child_reduction_views(view, valid);
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::update_reduction_epochs(const ReductionEpoch &epoch)
    //--------------------------------------------------------------------------
    {
      // This should be the parent and have no children
#ifdef DEBUG_HIGH_LEVEL
      assert(get_parent() == NULL);
#endif
      // No need to hold the lock since this is only called when 
      // the deferred view is being constructed
      reduction_epochs.push_back(epoch);
      // Don't forget to update the reduction mask
      reduction_mask |= epoch.valid_fields;
    }

    //--------------------------------------------------------------------------
    void DeferredView::update_reduction_views_above(ReductionView *view,
                                                    const FieldMask &valid,
                                                    DeferredView *from_child)
    //--------------------------------------------------------------------------
    {
      // Keep going up if necessary
      if (has_parent())
      {
        DeferredView *parent_view = get_parent()->as_deferred_view();
        parent_view->update_reduction_views_above(view, valid, this);
      }
      if ((logical_node == view->logical_node) ||
          logical_node->intersects_with(view->logical_node))
      {
        update_local_reduction_views(view, valid);
        update_child_reduction_views(view, valid, from_child);
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::update_local_reduction_views(ReductionView *view,
                                                    const FieldMask &valid_mask)
    //--------------------------------------------------------------------------
    {
      // We can do this before taking the lock
      ReductionOpID redop = view->get_redop();
      bool added = false;
      AutoLock v_lock(view_lock);
      reduction_mask |= valid_mask;
      // Iterate backwards and add to the first epoch that matches
      for (std::deque<ReductionEpoch>::reverse_iterator it = 
            reduction_epochs.rbegin(); it != reduction_epochs.rend(); it++)
      {
        if (redop != it->redop)
          continue;
        if (valid_mask * it->valid_fields)
          continue;
#ifdef DEBUG_HIGH_LEVEL
        assert(valid_mask == it->valid_fields);
#endif
        if (it->views.find(view) == it->views.end())
        {
          // Look at our state to see how to add the reduction view
#ifdef DEBUG_HIGH_LEVEL
          assert((current_state == INACTIVE_STATE) ||
                 (current_state == ACTIVE_INVALID_STATE) || 
                 (current_state == VALID_STATE));
#endif
          view->add_nested_resource_ref(did);
          if (current_state != INACTIVE_STATE)
            view->add_nested_gc_ref(did);
          if (current_state == VALID_STATE)
            view->add_nested_valid_ref(did);
          it->views.insert(view);
        }
        added = true;
        break;
      }
      if (!added)
      {
#ifdef DEBUG_HIGH_LEVEL
        assert((current_state == INACTIVE_STATE) ||
               (current_state == ACTIVE_INVALID_STATE) || 
               (current_state == VALID_STATE));
#endif
        view->add_nested_resource_ref(did);
        if (current_state != INACTIVE_STATE)
          view->add_nested_gc_ref(did);
        if (current_state == VALID_STATE)
          view->add_nested_valid_ref(did);
        reduction_epochs.push_back(ReductionEpoch(view, redop, valid_mask));
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::flush_reductions(const MappableInfo &info,
                                        MaterializedView *dst,
                                        const FieldMask &reduce_mask,
                                LegionMap<Event,FieldMask>::aligned &conditions)
    //--------------------------------------------------------------------------
    {
      // Iterate over all the reduction epochs and issue reductions
      LegionDeque<ReductionEpoch>::aligned to_issue;  
      {
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
        for (LegionDeque<ReductionEpoch>::aligned::const_iterator it = 
              reduction_epochs.begin(); it != reduction_epochs.end(); it++)
        {
          if (reduce_mask * it->valid_fields)
            continue;
          to_issue.push_back(*it);
        }
      }
      for (LegionDeque<ReductionEpoch>::aligned::const_iterator rit = 
            to_issue.begin(); rit != to_issue.end(); rit++)
      {
        const ReductionEpoch &epoch = *rit; 
        // Compute the per-field preconditions
        std::set<Event> preconditions;
        for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
              conditions.begin(); it != conditions.end(); it++)
        {
          if (it->second * epoch.valid_fields)
            continue;
          preconditions.insert(it->first);
        }
        // Now issue the reductions from all the views
        std::set<Event> postconditions;
        for (std::set<ReductionView*>::const_iterator it = 
              epoch.views.begin(); it != epoch.views.end(); it++)
        {
          std::set<Domain> component_domains;
          Event dom_pre = find_component_domains(*it, dst, component_domains);
          if (!component_domains.empty())
          {
            Event result = (*it)->perform_deferred_reduction(dst,
                                    epoch.valid_fields, info.version_info, 
                                    preconditions, component_domains, 
                                    dom_pre, info.op);
            if (result.exists())
              postconditions.insert(result);
          }
        }
        // Merge the post-conditions together and add them to results
        Event result = Event::merge_events(postconditions);
        if (result.exists())
          conditions[result] = epoch.valid_fields;
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::flush_reductions_across(const MappableInfo &info,
                                               MaterializedView *dst,
                                               FieldID src_field, 
                                               FieldID dst_field,
                                               Event dst_precondition,
                                               std::set<Event> &conditions)
    //--------------------------------------------------------------------------
    {
      // Find the reductions to perform
      unsigned src_index = 
        logical_node->column_source->get_field_index(src_field);
      std::deque<ReductionEpoch> to_issue;
      {
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
        for (std::deque<ReductionEpoch>::const_iterator it = 
              reduction_epochs.begin(); it != reduction_epochs.end(); it++)
        {
          if (it->valid_fields.is_set(src_index))
            to_issue.push_back(*it);
        }
      }
      if (!to_issue.empty())
      {
        std::set<Event> preconditions = conditions;
        preconditions.insert(dst_precondition);
        for (std::deque<ReductionEpoch>::const_iterator rit = 
              to_issue.begin(); rit != to_issue.end(); rit++)
        {
          const ReductionEpoch &epoch = *rit;
          std::set<Event> postconditions;
          for (std::set<ReductionView*>::const_iterator it = 
                epoch.views.begin(); it != epoch.views.end(); it++)
          {
            // Get the domains for this reduction view
            std::set<Domain> component_domains;
            Event dom_pre = find_component_domains(*it, dst, component_domains);
            if (!component_domains.empty())
            {
              Event result = (*it)->perform_deferred_across_reduction(dst,
                                              dst_field, src_field, src_index,
                                              info.version_info,
                                              preconditions, component_domains,
                                              dom_pre, info.op);
              if (result.exists())
                postconditions.insert(result);
            }
          }
          // Merge the postconditions
          Event result = Event::merge_events(postconditions);
          if (result.exists())
          {
            conditions.insert(result);
            preconditions.insert(result);
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    Event DeferredView::find_component_domains(ReductionView *reduction_view,
                                               MaterializedView *dst_view,
                                            std::set<Domain> &component_domains)
    //--------------------------------------------------------------------------
    {
      Event result = Event::NO_EVENT;
      if (dst_view->logical_node == reduction_view->logical_node)
      {
        if (dst_view->logical_node->has_component_domains())
          component_domains = 
            dst_view->logical_node->get_component_domains(result);
        else
          component_domains.insert(dst_view->logical_node->get_domain(result));
      }
      else
        component_domains = dst_view->logical_node->get_intersection_domains(
                                                reduction_view->logical_node);
      return result;
    }

    //--------------------------------------------------------------------------
    void DeferredView::activate_deferred(void)
    //--------------------------------------------------------------------------
    {
      // Add gc references to all our reduction views
      // Have to hold the lock when accessing this data structure 
      AutoLock v_lock(view_lock, 1, false/*exclusive*/);
      for (LegionDeque<ReductionEpoch>::aligned::const_iterator rit = 
            reduction_epochs.begin(); rit != reduction_epochs.end(); rit++)
      {
        const ReductionEpoch &epoch = *rit;
        for (std::set<ReductionView*>::const_iterator it = 
              epoch.views.begin(); it != epoch.views.end(); it++)
        {
          (*it)->add_nested_gc_ref(did);
        }
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::deactivate_deferred(void)
    //--------------------------------------------------------------------------
    {
      // Hold the lock when accessing the reduction views
      AutoLock v_lock(view_lock, 1, false/*exclusive*/);
      for (LegionDeque<ReductionEpoch>::aligned::const_iterator rit = 
            reduction_epochs.begin(); rit != reduction_epochs.end(); rit++)
      {
        const ReductionEpoch &epoch = *rit;
        for (std::set<ReductionView*>::const_iterator it = 
              epoch.views.begin(); it != epoch.views.end(); it++)
        {
          // No need to check for deletion condition since we hold resource refs
          (*it)->remove_nested_gc_ref(did);
        }
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::validate_deferred(void)
    //--------------------------------------------------------------------------
    {
      AutoLock v_lock(view_lock, 1, false/*exclusive*/);
      for (LegionDeque<ReductionEpoch>::aligned::const_iterator rit = 
            reduction_epochs.begin(); rit != reduction_epochs.end(); rit++)
      {
        const ReductionEpoch &epoch = *rit;
        for (std::set<ReductionView*>::const_iterator it = 
              epoch.views.begin(); it != epoch.views.end(); it++)
        {
          (*it)->add_nested_valid_ref(did);
        }
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::invalidate_deferred(void)
    //--------------------------------------------------------------------------
    {
      AutoLock v_lock(view_lock, 1, false/*exclusive*/);
      for (LegionDeque<ReductionEpoch>::aligned::const_iterator rit = 
            reduction_epochs.begin(); rit != reduction_epochs.end(); rit++)
      {
        const ReductionEpoch &epoch = *rit;
        for (std::set<ReductionView*>::const_iterator it = 
              epoch.views.begin(); it != epoch.views.end(); it++)
        {
          // No need to check for deletion condition since we hold resource refs
          (*it)->remove_nested_valid_ref(did);
        }
      }
    }

    //--------------------------------------------------------------------------
    void DeferredView::send_deferred_view_updates(AddressSpaceID target,
                                                  const FieldMask &update_mask)
    //--------------------------------------------------------------------------
    {
      LegionMap<unsigned/*idx*/,ReductionEpoch>::aligned to_send;
      {
        AutoLock v_lock(view_lock,1,false/*exclusive*/);
        // Quick check for being done
        if (update_mask * reduction_mask)
          return;
        unsigned idx = 0;
        for (LegionDeque<ReductionEpoch>::aligned::const_iterator it = 
             reduction_epochs.begin(); it != reduction_epochs.end(); it++,idx++)
        {
          if (update_mask * it->valid_fields)
            continue;
          to_send[idx] = *it;
        }
      }
      if (to_send.empty())
        return;
      // Now pack up the results and send the views in the process
      Serializer rez;
      {
        RezCheck z(rez);  
        bool is_region = logical_node->is_region();
        rez.serialize(is_region);
        if (is_region)
        {
          LogicalRegion handle = logical_node->as_region_node()->handle;
          rez.serialize(handle);
        }
        else
        {
          LogicalPartition handle = logical_node->as_partition_node()->handle;
          rez.serialize(handle);
        }
        rez.serialize(did);
        rez.serialize<size_t>(to_send.size());
        for (LegionMap<unsigned,ReductionEpoch>::aligned::const_iterator sit =
              to_send.begin(); sit != to_send.end(); sit++)
        {
          rez.serialize(sit->first);
          rez.serialize(sit->second.redop);
          rez.serialize(sit->second.valid_fields);
          rez.serialize<size_t>(sit->second.views.size());
          for (std::set<ReductionView*>::const_iterator it = 
                sit->second.views.begin(); it != sit->second.views.end(); it++)
          {
#ifdef DEBUG_HIGH_LEVEL
            assert((*it)->logical_node->is_region());
#endif
            rez.serialize((*it)->logical_node->as_region_node()->handle);
            DistributedID red_did = (*it)->send_view(target, update_mask);
            rez.serialize(red_did);
          }
        }
      }
      runtime->send_deferred_update(target, rez);
    }

    //--------------------------------------------------------------------------
    void DeferredView::process_deferred_view_update(Deserializer &derez, 
                                                    AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      size_t num_epochs;
      derez.deserialize(num_epochs);
      FieldSpaceNode *field_node = logical_node->column_source;
      AutoLock v_lock(view_lock);
      for (unsigned idx = 0; idx < num_epochs; idx++)
      {
        unsigned index;
        derez.deserialize(index);
        if (index >= reduction_epochs.size())
          reduction_epochs.resize(index+1);
        ReductionEpoch &epoch = reduction_epochs[index]; 
        derez.deserialize(epoch.redop);
        FieldMask valid_fields;
        derez.deserialize(valid_fields);
        field_node->transform_field_mask(valid_fields, source);
        epoch.valid_fields |= valid_fields;
        reduction_mask |= valid_fields;
        size_t num_reductions;
        derez.deserialize(num_reductions);
        for (unsigned idx2 = 0; idx2 < num_reductions; idx2++)
        {
          LogicalRegion handle;
          derez.deserialize(handle);
          RegionTreeNode *node = context->get_node(handle);
          DistributedID red_did;
          derez.deserialize(red_did);
          LogicalView *red_view = node->find_view(red_did);
#ifdef DEBUG_HIGH_LEVEL
          assert(red_view->is_instance_view());
          assert(red_view->as_instance_view()->is_reduction_view());
#endif
          epoch.views.insert(red_view->as_instance_view()->as_reduction_view());
        }
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void DeferredView::handle_deferred_update(Runtime *runtime,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez); 
      bool is_region;
      derez.deserialize(is_region);
      RegionTreeNode *target_node;
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        target_node = runtime->forest->get_node(handle);
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        target_node = runtime->forest->get_node(handle);
      }
      DistributedID did;
      derez.deserialize(did);
      LogicalView *view = target_node->find_view(did);
#ifdef DEBUG_HIGH_LEVEL
      assert(view->is_deferred_view());
#endif
      view->as_deferred_view()->process_deferred_view_update(derez, source);
    }

    /////////////////////////////////////////////////////////////
    // CompositeView
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CompositeView::CompositeView(RegionTreeForest *ctx, DistributedID did,
                              AddressSpaceID owner_proc, RegionTreeNode *node,
                              AddressSpaceID local_proc, const FieldMask &mask,
                              bool register_now, CompositeView *par/*= NULL*/)
      : DeferredView(ctx, did, owner_proc, local_proc, node, register_now), 
        parent(par), valid_mask(mask)
    //--------------------------------------------------------------------------
    {
      // If we are either not a parent or we are a remote parent add 
      // a resource reference to avoid being collected
      if (parent != NULL)
        add_nested_resource_ref(did);
      else 
      {
        // Do remote registration for the top of each remote tree
        if (!is_owner())
        {
          add_base_resource_ref(REMOTE_DID_REF);
          send_remote_registration();
        }
      }
#ifdef LEGION_GC
      log_garbage.info("GC Composite View %ld", did);
#endif
    }
    
    //--------------------------------------------------------------------------
    CompositeView::CompositeView(const CompositeView &rhs)
      : DeferredView(NULL, 0, 0, 0, NULL, false), parent(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    CompositeView::~CompositeView(void)
    //--------------------------------------------------------------------------
    {
      // Remove any resource references that we hold on child views
      // Capture our child destruction events in the process
      for (std::map<ColorPoint,CompositeView*>::const_iterator it = 
            children.begin(); it != children.end(); it++)
      {
        recycle_events.insert(it->second->get_destruction_event());
        if (it->second->remove_nested_resource_ref(did))
          legion_delete(it->second);
      }
      children.clear();
      if ((parent == NULL) && is_owner())
      {
        UpdateReferenceFunctor<RESOURCE_REF_KIND,false/*add*/> functor(this);
        map_over_remote_instances(functor);
        // If we are the top the tree on the owner node we can recycle 
        // the distributed ID once our destruction event triggers
        runtime->recycle_distributed_id(did, destruction_event);
      }
      // Remove any references we have to our roots
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        if (it->first->remove_reference())
          legion_delete(it->first);
      }
      roots.clear(); 
    }

    //--------------------------------------------------------------------------
    CompositeView& CompositeView::operator=(const CompositeView &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* CompositeView::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<CompositeView,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void CompositeView::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void CompositeView::notify_active(void)
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        parent->add_nested_gc_ref(did);

      activate_deferred();

      // If we are the top level view, add gc references to all our instances
      if (parent == NULL)
      {
        for (LegionMap<CompositeNode*,FieldMask>::aligned::iterator it = 
              roots.begin(); it != roots.end(); it++)
        {
          it->first->add_gc_references();
        }
      } 
    }

    //--------------------------------------------------------------------------
    void CompositeView::notify_inactive(void)
    //--------------------------------------------------------------------------
    {
      if (parent == NULL)
      {
        for (LegionMap<CompositeNode*,FieldMask>::aligned::iterator it = 
              roots.begin(); it != roots.end(); it++)
        {
          it->first->remove_gc_references();
        }
      }

      deactivate_deferred();

      if ((parent != NULL) && parent->remove_nested_gc_ref(did))
        legion_delete(parent); 
    }

    //--------------------------------------------------------------------------
    void CompositeView::notify_valid(void)
    //--------------------------------------------------------------------------
    {
      if (parent == NULL)
      {
        if (!is_owner())
          send_remote_valid_update(owner_space, 1/*count*/, true/*add*/);
      }
      else
        parent->add_nested_valid_ref(did);

      for (LegionMap<CompositeNode*,FieldMask>::aligned::iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        it->first->add_valid_references();
      }

      validate_deferred(); 
    }

    //--------------------------------------------------------------------------
    void CompositeView::notify_invalid(void)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<CompositeNode*,FieldMask>::aligned::iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        it->first->remove_valid_references();
      }

      invalidate_deferred(); 

      if (parent == NULL)
      {
        if (!is_owner())
          send_remote_valid_update(owner_space, 1/*count*/, false/*add*/);
      }
      else if (parent->remove_nested_valid_ref(did))
        legion_delete(parent);
    }

    //--------------------------------------------------------------------------
    DistributedID CompositeView::send_view_base(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      if (!has_remote_instance(target))
      {
        if (parent == NULL)
        {
          // Don't take the lock, it's alright to have duplicate sends
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(owner_space);
            rez.serialize(valid_mask);
            bool is_region = logical_node->is_region();
            rez.serialize(is_region);
            if (is_region)
              rez.serialize(logical_node->as_region_node()->handle);
            else
              rez.serialize(logical_node->as_partition_node()->handle);
            rez.serialize<size_t>(roots.size());
            for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator 
                  it = roots.begin(); it != roots.end(); it++)
            {
#ifdef DEBUG_HIGH_LEVEL
              assert(it->first->logical_node == this->logical_node);
#endif
              // Pack the version info and then pack the composite tree
              // We know that the physical states have already been 
              // created for this version info in order to do the capture
              // of the tree, so we can pass in dummy context and 
              // local space parameters.
              VersionInfo &info = it->first->version_info->get_version_info();
              info.pack_version_info(rez, 0, 0);
              it->first->pack_composite_tree(rez, target);
              rez.serialize(it->second);
            }
          }
          runtime->send_composite_view(target, rez);
        }
        else // Ask our parent to do the send
          parent->send_view_base(target);
        // We've done the send so record it
        update_remote_instances(target);
      }
      return did;
    }

    //--------------------------------------------------------------------------
    void CompositeView::unpack_composite_view(Deserializer &derez, 
                                              AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      size_t num_roots;
      derez.deserialize(num_roots);
      for (unsigned idx = 0; idx < num_roots; idx++)
      {
        CompositeVersionInfo *version_info = new CompositeVersionInfo();
        VersionInfo &info = version_info->get_version_info();
        info.unpack_version_info(derez);
        CompositeNode *new_node = legion_new<CompositeNode>(logical_node,
                            (CompositeNode*)NULL/*parent*/, version_info);
        new_node->unpack_composite_tree(derez, source);
        FieldMask &mask = roots[new_node];
        derez.deserialize(mask);
        logical_node->column_source->transform_field_mask(mask, source);
        new_node->set_owner_did(did);
      }
    }

    //--------------------------------------------------------------------------
    void CompositeView::make_local(std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      // This might not be the top of the view tree, but that is alright
      // because all the composite nodes share the same VersionInfo
      // data structure so we'll end up waiting for the right set of
      // version states to be local. It might be a little bit of an 
      // over approximation for sub-views, but that is ok for now.
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        VersionInfo &info = it->first->version_info->get_version_info();
        // If we are getting this call, we know we are on a remote node
        // so we know the physical states are already unpacked and therefore
        // we can pass in a dummy context ID
        info.make_local(preconditions, context, 0/*dummy ctx*/);
        // Now check the sub-tree for recursive composite views
        std::set<DistributedID> checked_views;
        it->first->make_local(preconditions, checked_views);
      }
    }

    //--------------------------------------------------------------------------
    /*static*/ void CompositeView::handle_send_composite_view(Runtime *runtime,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez); 
      DistributedID did;
      derez.deserialize(did);
      AddressSpaceID owner;
      derez.deserialize(owner);
      FieldMask valid_mask;
      derez.deserialize(valid_mask);
      bool is_region;
      derez.deserialize(is_region);
      RegionTreeNode *target_node;
      if (is_region)
      {
        LogicalRegion handle;
        derez.deserialize(handle);
        target_node = runtime->forest->get_node(handle);
      }
      else
      {
        LogicalPartition handle;
        derez.deserialize(handle);
        target_node = runtime->forest->get_node(handle);
      }
      // Transform the fields mask 
      target_node->column_source->transform_field_mask(valid_mask, source);
      CompositeView *new_view = legion_new<CompositeView>(runtime->forest,
                            did, owner, target_node, runtime->address_space,
                            valid_mask, false/*register now*/);
      new_view->unpack_composite_view(derez, source);
      if (!target_node->register_logical_view(new_view))
        legion_delete(new_view);
      else
        new_view->update_remote_instances(source);
    }

    //--------------------------------------------------------------------------
    void CompositeView::send_view_updates(AddressSpaceID target,
                                          const FieldMask &update_mask)
    //--------------------------------------------------------------------------
    {
      // For composite views, we only need to send the view structures
      // of our actual instances, the enclosing version state will check
      // to see if our version infos are up to date. We do still need to
      // send updates for our constituent reduction views.
      send_deferred_view_updates(target, update_mask); 
    }

    //--------------------------------------------------------------------------
    void CompositeView::add_root(CompositeNode *root, 
                                 const FieldMask &valid, bool top)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!(valid - valid_mask));
      // There should always be at most one root for each field
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        assert(it->second * valid);
      }
#endif
      LegionMap<CompositeNode*,FieldMask>::aligned::iterator finder = 
                                                            roots.find(root);
      if (finder == roots.end())
      {
        roots[root] = valid;
        // Add a reference for when we go to delete it
        root->add_reference();
      }
      else
        finder->second |= valid;
      if (top)
        root->set_owner_did(did);
    }

    //--------------------------------------------------------------------------
    void CompositeView::update_child_reduction_views(ReductionView *view,
                                                    const FieldMask &valid_mask,
                                                    DeferredView *to_skip)
    //--------------------------------------------------------------------------
    {
      // Make a copy of the child views and update them
      std::map<ColorPoint,CompositeView*> to_handle;
      {
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
        to_handle = children;
      }
      std::set<ColorPoint> handled;
      // Keep iterating until we've handled all the children
      while (!to_handle.empty())
      {
        for (std::map<ColorPoint,CompositeView*>::const_iterator it = 
              to_handle.begin(); it != to_handle.end(); it++)
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(handled.find(it->first) == handled.end());
#endif
          handled.insert(it->first);
          if (it->second == to_skip)
            continue;
          it->second->update_reduction_views(view, valid_mask, false/*parent*/);
        }
        to_handle.clear();
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
#ifdef DEBUG_HIGH_LEVEL
        assert(handled.size() <= children.size());
#endif
        if (handled.size() == children.size())
          break;
        // Otherwise figure out what additional children to handle
        for (std::map<ColorPoint,CompositeView*>::const_iterator it = 
              children.begin(); it != children.end(); it++)
        {
          if (handled.find(it->first) == handled.end())
            to_handle.insert(*it);
        }
      }
    }

    //--------------------------------------------------------------------------
    LogicalView* CompositeView::get_subview(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // This is the common case
      {
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
        std::map<ColorPoint,CompositeView*>::const_iterator finder = 
                                                          children.find(c);
        if (finder != children.end())
          return finder->second;
      }
      RegionTreeNode *child_node = logical_node->get_tree_child(c); 
      CompositeView *child_view = legion_new<CompositeView>(context, did, 
                                                    owner_space, child_node,
                                                    local_space, valid_mask,
                                                    false/*register*/, this);
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        it->first->find_bounding_roots(child_view, it->second); 
      }
      
      // Retake the lock and try and add the child, see if
      // someone else added the child in the meantime
      bool free_child_view = false;
      CompositeView *result = child_view;
      {
        AutoLock v_lock(view_lock);
        std::map<ColorPoint,CompositeView*>::const_iterator finder = 
                                                          children.find(c);
        if (finder != children.end())
        {
          // Guaranteed to succeed
          if (child_view->remove_nested_resource_ref(did))
            free_child_view = true;
          result = finder->second;
        }
        else
        {
          children[c] = child_view;
          // Update the subviews while holding the lock
          for (std::deque<ReductionEpoch>::const_iterator rit = 
                reduction_epochs.begin(); rit != reduction_epochs.end(); rit++)
          {
            const ReductionEpoch &epoch = *rit;
            for (std::set<ReductionView*>::const_iterator it = 
                  epoch.views.begin(); it != epoch.views.end(); it++)
            {
              child_view->update_reduction_views(*it, epoch.valid_fields,
                                                 false/*update parent*/);
            }
          }
          child_node->register_logical_view(child_view);
        }
      }
      if (free_child_view)
        legion_delete(child_view);
      return result;
    }

    //--------------------------------------------------------------------------
    void CompositeView::update_valid_mask(const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      AutoLock v_lock(view_lock);
      valid_mask |= mask;
    }

    //--------------------------------------------------------------------------
    void CompositeView::flatten_composite_view(FieldMask &global_dirt,
                                               const FieldMask &flatten_mask, 
                                               CompositeCloser &closer, 
                                               CompositeNode *target)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!(valid_mask * flatten_mask)); // A little sanity check
#endif
      // Try to flatten this composite view, first make sure there are no
      // reductions which cannot be flattened
      LegionDeque<ReductionEpoch>::aligned flat_reductions;
      {
        // Hold the lock when 
        AutoLock v_lock(view_lock,1,false/*exclusive*/);
        for (LegionDeque<ReductionEpoch>::aligned::const_iterator it1 = 
              reduction_epochs.begin(); it1 != reduction_epochs.end(); it1++)
        {
          FieldMask overlap = it1->valid_fields & flatten_mask; 
          if (!overlap)
            continue;
          const ReductionEpoch &epoch = *it1; 
          flat_reductions.push_back(ReductionEpoch());
          ReductionEpoch &next = flat_reductions.back();
          for (std::set<ReductionView*>::const_iterator it = 
                epoch.views.begin(); it != epoch.views.end(); it++)
          {
            FieldMask temp = overlap;
            closer.filter_capture_mask((*it)->logical_node, temp);
            if (!!temp)
              next.views.insert(*it);
          }
          if (!next.views.empty())
          {
            next.valid_fields = overlap;
            next.redop = epoch.redop;
          }
          else // Actually empty so we can ignore it
            flat_reductions.pop_back();
        }
      }
      // Now see if we can flatten any roots
      LegionMap<CompositeNode*,FieldMask>::aligned new_roots;
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it =
            roots.begin(); it != roots.end(); it++)
      {
        FieldMask overlap = flatten_mask & it->second;
        if (!overlap)
          continue;
        // Check to see if we already captured this root
        closer.filter_capture_mask(it->first->logical_node, overlap);
        if (!overlap)
          continue;
        // If we can't flatten the reductions, then we also 
        // can't flatten the roots
        CompositeNode *new_root = it->first->flatten(overlap, closer, 
                                         NULL/*parent*/, global_dirt, 
                                     (flat_reductions.empty() ? target : NULL));
        if (new_root != NULL)
          new_roots[new_root] = overlap;
      }
      // If we have no new roots and we have no reductions then 
      // we successfully flattened into the target
      if (!flat_reductions.empty() || !new_roots.empty())
      {
#ifdef DEBUG_HIGH_LEVEL
        // Sanity check that we always have something to apply reductions to
        if (!flat_reductions.empty())
          assert(!new_roots.empty()); 
#endif
        // Make a new composite view and then iterate over the roots
        DistributedID flat_did = 
          context->runtime->get_available_distributed_id(false);
        CompositeView *result = legion_new<CompositeView>(context, flat_did,
                                              context->runtime->address_space,
                                              logical_node,
                                              context->runtime->address_space,
                                              flatten_mask, true/*reg now*/);
        for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
              new_roots.begin(); it != new_roots.end(); it++)
        {
          result->add_root(it->first, it->second, true/*top*/);
        }
        for (std::deque<ReductionEpoch>::const_iterator it = 
              flat_reductions.begin(); it != flat_reductions.end(); it++)
        {
          result->update_reduction_epochs(*it);
        }
        // Add the new view to the target
        target->update_instance_views(result, flatten_mask); 
#ifdef UNIMPLEMENTED_VERSIONING
        // TODO: As an optimization, send the new composite view to all the 
        // known locations of this view so we can avoid duplicate sends of the 
        // meta-data for all the constituent views
#endif
      }
    }

    //--------------------------------------------------------------------------
    void CompositeView::find_field_descriptors(Event term_event, 
                                               const RegionUsage &usage, 
                                               const FieldMask &user_mask,
                                               unsigned fid_idx,
                                               Processor local_proc,
                                   std::vector<FieldDataDescriptor> &field_data,
                                   std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      // Iterate over all the roots and find the one for our event
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it =
            roots.begin(); it != roots.end(); it++)
      {
        if (it->second.is_set(fid_idx))
        {
          Event target_pre;
          const Domain &target = 
            it->first->logical_node->get_domain(target_pre);
          std::vector<LowLevel::IndexSpace> already_handled;
          std::set<Event> already_preconditions;
          it->first->find_field_descriptors(term_event, usage,
                                            user_mask, fid_idx, local_proc, 
                                            target.get_index_space(),
                                            target_pre, field_data, 
                                            preconditions, already_handled,
                                            already_preconditions);
          return;
        }
      }
      // We should never get here
      assert(false);
    }

    //--------------------------------------------------------------------------
    bool CompositeView::find_field_descriptors(Event term_event,
                                               const RegionUsage &usage,
                                               const FieldMask &user_mask,
                                               unsigned fid_idx,
                                               Processor local_proc,
                                               LowLevel::IndexSpace target,
                                               Event target_precondition,
                                   std::vector<FieldDataDescriptor> &field_data,
                                   std::set<Event> &preconditions,
                             std::vector<LowLevel::IndexSpace> &already_handled,
                             std::set<Event> &already_preconditions)
    //--------------------------------------------------------------------------
    {
      // Iterate over all the roots and find the one for our event
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it =
            roots.begin(); it != roots.end(); it++)
      {
        if (it->second.is_set(fid_idx))
        {
          return it->first->find_field_descriptors(term_event, usage, user_mask,
                                                   fid_idx, local_proc,
                                                   target, target_precondition,
                                                   field_data, preconditions,
                                                   already_handled, 
                                                   already_preconditions);
        }
      }
      // We should never get here
      assert(false);
      return false;
    }
    
    //--------------------------------------------------------------------------
    void CompositeView::issue_deferred_copies(const MappableInfo &info,
                                              MaterializedView *dst,
                                              const FieldMask &copy_mask,
                                              CopyTracker *tracker /* = NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!(copy_mask - valid_mask));
#endif
      LegionMap<Event,FieldMask>::aligned preconditions;
      dst->find_copy_preconditions(0/*redop*/, false/*reading*/,
                                   copy_mask, info.version_info, preconditions);
      // Iterate over all the roots and issue copies to update the 
      // target instance from this particular view
      LegionMap<Event,FieldMask>::aligned postconditions;
#ifdef DEBUG_HIGH_LEVEL
      FieldMask accumulate_mask;
#endif
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it =
            roots.begin(); it != roots.end(); it++)
      {
        FieldMask overlap = it->second & copy_mask;
        if (!overlap)
          continue;
        it->first->issue_update_copies(info, dst, overlap, overlap,
                                       preconditions, postconditions, tracker);
#ifdef DEBUG_HIGH_LEVEL
        assert(overlap * accumulate_mask);
        accumulate_mask |= overlap;
#endif
      }
      // Now that we've issued all our copies, flush any reductions
      FieldMask reduce_overlap = reduction_mask & copy_mask;
      if (!!reduce_overlap)
        flush_reductions(info, dst, reduce_overlap, postconditions); 
      // Fun trick here, use the precondition set routine to get the
      // sets of fields which all have the same precondition events
      LegionList<EventSet>::aligned postcondition_sets;
      RegionTreeNode::compute_event_sets(copy_mask, postconditions,
                                         postcondition_sets);
      // Now add all the post conditions for each of the
      // writes for fields with the same set of post condition events
      for (LegionList<EventSet>::aligned::iterator pit = 
            postcondition_sets.begin(); pit !=
            postcondition_sets.end(); pit++)
      {
        EventSet &post_set = *pit;
        // Don't need to record anything if empty
        if (post_set.preconditions.empty())
          continue;
        // Compute the merge event
        Event postcondition = Event::merge_events(post_set.preconditions);
        if (postcondition.exists())
        {
          dst->add_copy_user(0/*redop*/, postcondition, info.version_info,
                             post_set.set_mask, false/*reading*/);
          if (tracker != NULL)
            tracker->add_copy_event(postcondition);
        }
      }
    }

    //--------------------------------------------------------------------------
    void CompositeView::issue_deferred_copies(const MappableInfo &info,
                                              MaterializedView *dst,
                                              const FieldMask &copy_mask,
                     const LegionMap<Event,FieldMask>::aligned &preconditions,
                           LegionMap<Event,FieldMask>::aligned &postconditions,
                                              CopyTracker *tracker /* = NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(!(copy_mask - valid_mask));
#endif
      LegionMap<Event,FieldMask>::aligned local_postconditions;
#ifdef DEBUG_HIGH_LEVEL
      FieldMask accumulate_mask;
#endif
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        FieldMask overlap = it->second & copy_mask;
        if (!overlap)
          continue;
        it->first->issue_update_copies(info, dst, overlap, overlap,
                                 preconditions, local_postconditions, tracker);
#ifdef DEBUG_HIGH_LEVEL
        assert(overlap * accumulate_mask);
        accumulate_mask |= overlap;
#endif
      }
      FieldMask reduce_overlap = reduction_mask & copy_mask;
      // Finally see if we have any reductions to flush
      if (!!reduce_overlap)
        flush_reductions(info, dst, reduce_overlap, postconditions);
    }

    //--------------------------------------------------------------------------
    void CompositeView::issue_deferred_copies_across(const MappableInfo &info,
                                                     MaterializedView *dst,
                                                     FieldID src_field,
                                                     FieldID dst_field,
                                                     Event precondition,
                                                std::set<Event> &postconditions)
    //--------------------------------------------------------------------------
    {
      unsigned src_index = 
        logical_node->column_source->get_field_index(src_field);
      std::set<Event> preconditions;
      // This includes the destination precondition
      preconditions.insert(precondition);
      // Keep track of the local postconditions
      std::set<Event> local_postconditions;
      for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
            roots.begin(); it != roots.end(); it++)
      {
        if (it->second.is_set(src_index))
        {
          it->first->issue_across_copies(info, dst, src_index, src_field, 
                                         dst_field, true/*need field*/,
                                         preconditions, local_postconditions);
          // We know there is at most one root here so
          // once we find it then we are done
          break;
        }
      }
      if (!reduction_epochs.empty() && reduction_mask.is_set(src_index))
      {
        // Merge our local postconditions to generate a new precondition
        Event local_postcondition = Event::merge_events(local_postconditions);
        flush_reductions_across(info, dst, src_field, dst_field,
                                local_postcondition, postconditions);
      }
      else // Otherwise we can just add locally
        postconditions.insert(local_postconditions.begin(),
                              local_postconditions.end());
    }

    /////////////////////////////////////////////////////////////
    // CompositeNode 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CompositeVersionInfo::CompositeVersionInfo(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CompositeVersionInfo::CompositeVersionInfo(const CompositeVersionInfo &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    CompositeVersionInfo::~CompositeVersionInfo(void)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    CompositeVersionInfo& CompositeVersionInfo::operator=(
                                                const CompositeVersionInfo &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    /////////////////////////////////////////////////////////////
    // CompositeNode 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    CompositeNode::CompositeNode(RegionTreeNode *logical, CompositeNode *par,
                                 CompositeVersionInfo *ver_info)
      : Collectable(), context(logical->context), logical_node(logical), 
        parent(par), version_info(ver_info), owner_did(0)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(version_info != NULL);
#endif
      version_info->add_reference();
    }

    //--------------------------------------------------------------------------
    CompositeNode::CompositeNode(const CompositeNode &rhs)
      : Collectable(), context(NULL), logical_node(NULL), 
        parent(NULL), version_info(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    CompositeNode::~CompositeNode(void)
    //--------------------------------------------------------------------------
    {
      // Free up all our children
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        if (it->first->remove_reference())
          legion_delete(it->first);
      }
      open_children.clear();
      // Remove our resource references
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
            valid_views.begin(); it != valid_views.end(); it++)
      {
        if (it->first->remove_base_resource_ref(COMPOSITE_NODE_REF))
          LogicalView::delete_logical_view(it->first);
      }
      valid_views.clear();
      if (version_info->remove_reference())
        delete version_info;
    }

    //--------------------------------------------------------------------------
    CompositeNode& CompositeNode::operator=(const CompositeNode &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void* CompositeNode::operator new(size_t count)
    //--------------------------------------------------------------------------
    {
      return legion_alloc_aligned<CompositeNode,true/*bytes*/>(count);
    }

    //--------------------------------------------------------------------------
    void CompositeNode::operator delete(void *ptr)
    //--------------------------------------------------------------------------
    {
      free(ptr);
    }

    //--------------------------------------------------------------------------
    void CompositeNode::capture_physical_state(RegionTreeNode *tree_node,
                                               PhysicalState *state,
                                               const FieldMask &capture_mask,
                                               CompositeCloser &closer,
                                               FieldMask &global_dirt,
                                             const FieldMask &other_dirty_mask,
            const LegionMap<LogicalView*,FieldMask,
                            VALID_VIEW_ALLOC>::track_aligned &other_valid_views)
    //--------------------------------------------------------------------------
    {
      // Capture the global dirt we are passing back
      global_dirt |= (capture_mask & other_dirty_mask);
      // Also track the fields that we have dirty data for
      FieldMask local_dirty = capture_mask & other_dirty_mask;
      // Record the local dirty fields
      dirty_mask |= local_dirty;
      LegionMap<CompositeView*,FieldMask>::aligned to_flatten;
      FieldMask need_flatten = capture_mask;
      // Only need to pull down valid views if we are at the top
      if ((state != NULL) && (parent == NULL))
      {
        LegionMap<LogicalView*,FieldMask>::aligned instances;
        tree_node->find_valid_instance_views(closer.ctx, state, capture_mask,
            capture_mask, closer.version_info, false/*needs space*/, instances);
        capture_instances(capture_mask, need_flatten, to_flatten, instances);  
      }
      else
      {
        capture_instances(capture_mask, need_flatten, 
                          to_flatten, other_valid_views); 
      }
      // This is a very important optimization! We can't just blindly capture
      // all valid views because there might be some composite views in here.
      // If we continue doing this, we may end up with a chain of composite
      // views reaching all the way back to the start of the task which would
      // be bad. Instead we do a flattening procedure which prevents duplicate
      // captures of the same logical nodes for the same fields. This way we
      // only capture the most recent necessary data. Note there can still be
      // nested composite views, but none with overlapping information.
      if (!!need_flatten)
      {
        for (LegionMap<CompositeView*,FieldMask>::aligned::const_iterator it =
              to_flatten.begin(); it != to_flatten.end(); it++)
        {
          FieldMask overlap = need_flatten & it->second;
          if (!overlap)
            continue;
          it->first->flatten_composite_view(global_dirt, overlap, closer, this);
          need_flatten -= overlap;
          if (!need_flatten)
            break;
        }
      }
    }

    //--------------------------------------------------------------------------
    template<typename MAP_TYPE>
    void CompositeNode::capture_instances(const FieldMask &capture_mask,
                                          FieldMask &need_flatten,
                    LegionMap<CompositeView*,FieldMask>::aligned &to_flatten,
                                          const MAP_TYPE &instances)
    //--------------------------------------------------------------------------
    {
      // Capture as many non-composite instances as we can, as long as we
      // have one for each field then we are good, otherwise, we need to 
      // flatten a composite instance to capture the necessary state
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            instances.begin(); it != instances.end(); it++)
      {
        FieldMask overlap = it->second & capture_mask;
        if (!overlap)
          continue;
        // Figure out what kind of view we have
        if (it->first->is_deferred_view())
        {
          DeferredView *def_view = it->first->as_deferred_view();
          if (def_view->is_composite_view())
          {
            CompositeView *comp_view = def_view->as_composite_view();
            to_flatten[comp_view] = overlap;
          }
          else
          {
#ifdef DEBUG_HIGH_LEVEL
            assert(def_view->is_fill_view());
#endif
            update_instance_views(def_view, overlap);
            need_flatten -= overlap;
          }
        }
        else
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(it->first->is_instance_view());
          assert(it->first->as_instance_view()->is_materialized_view());
#endif
          update_instance_views(it->first, overlap);
          need_flatten -= overlap;
        }
      }
    }

    //--------------------------------------------------------------------------
    CompositeNode* CompositeNode::flatten(const FieldMask &flatten_mask,
                                          CompositeCloser &closer,
                                          CompositeNode *parent,
                                          FieldMask &global_dirt,
                                          CompositeNode *target)
    //--------------------------------------------------------------------------
    {
      CompositeNode *result = NULL;
      // If we don't have a target, go ahead and make the clone
      // We're also only allowed to flatten to a node of the same kind
      // so go ahead make a clone if we aren't the same
      if ((target == NULL) || (target->logical_node != logical_node))
        result = create_clone_node(parent, closer); 
      // First capture down the tree  
      for (LegionMap<CompositeNode*,ChildInfo>::aligned::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        FieldMask overlap = flatten_mask & it->second.open_fields;
        if (!overlap)
          continue;
        // Check to see if it has already been handled
        closer.filter_capture_mask(it->first->logical_node, overlap);
        if (!overlap)
          continue;
        CompositeNode *flat_child = it->first->flatten(overlap, closer, 
                                       result, global_dirt, NULL/*no target*/);
        // Make the result if we haven't yet
        if (result == NULL)
          result = create_clone_node(parent, closer);
        result->update_child_info(flat_child, overlap);
      }
      // Then capture ourself if we don't have anyone below, we can capture
      // directly into the target, otherwise we have to capture to our result
      if (result == NULL)
        target->capture_physical_state(logical_node, NULL/*state*/, 
            flatten_mask, closer, global_dirt, dirty_mask, valid_views);
      else
        result->capture_physical_state(logical_node, NULL/*state*/, 
            flatten_mask, closer, global_dirt, dirty_mask, valid_views);
      // Finally update the closer with the capture fields
      closer.update_capture_mask(logical_node, flatten_mask);
      return result;
    }

    //--------------------------------------------------------------------------
    CompositeNode* CompositeNode::create_clone_node(CompositeNode *parent,
                                                    CompositeCloser &closer)
    //--------------------------------------------------------------------------
    {
      // If we're making a copy and we don't have a parent, make a new
      // version info and then capture the needed version infos
      CompositeNode *result;
      if (parent == NULL)
      {
        CompositeVersionInfo *new_info = new CompositeVersionInfo();
        result = legion_new<CompositeNode>(logical_node, parent, new_info);
        // We need to capture the version info for all the nodes 
        // except the ones we know we've already captured
        VersionInfo &new_version_info = new_info->get_version_info();
        const VersionInfo &old_info = version_info->get_version_info();
        new_version_info.clone_from(old_info, closer);
      }
      else
        result = legion_new<CompositeNode>(logical_node, parent,
                                           parent->version_info);
      return result;
    }

    //--------------------------------------------------------------------------
    void CompositeNode::update_parent_info(const FieldMask &capture_mask)
    //--------------------------------------------------------------------------
    {
      if (parent != NULL)
        parent->update_child_info(this, capture_mask);
    }

    //--------------------------------------------------------------------------
    void CompositeNode::update_child_info(CompositeNode *child, 
                                          const FieldMask &mask)
    //--------------------------------------------------------------------------
    {
      std::map<CompositeNode*,ChildInfo>::iterator finder = 
                                                    open_children.find(child); 
      if (finder == open_children.end())
      {
        // If we didn't find it, we have to make it
        // and determine if it is complete
        bool complete = child->logical_node->is_complete();
        open_children[child] = ChildInfo(complete, mask);
        // Add a reference to it
        child->add_reference();
      }
      else
        finder->second.open_fields |= mask;
    }

    //--------------------------------------------------------------------------
    void CompositeNode::update_instance_views(LogicalView *view,
                                              const FieldMask &valid_mask)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(logical_node == view->logical_node);
#endif
      LegionMap<LogicalView*,FieldMask>::aligned::iterator finder = 
        valid_views.find(view);
      if (finder == valid_views.end())
      {
        view->add_base_resource_ref(COMPOSITE_NODE_REF);
        valid_views[view] = valid_mask;
      }
      else
        finder->second |= valid_mask;
    }

    //--------------------------------------------------------------------------
    void CompositeNode::issue_update_copies(const MappableInfo &info,
                                            MaterializedView *dst,
                                            FieldMask traversal_mask,
                                            const FieldMask &copy_mask,
                            const LegionMap<Event,FieldMask>::aligned &preconds,
                            LegionMap<Event,FieldMask>::aligned &postconditions,
                                            CopyTracker *tracker /*= NULL*/)
    //--------------------------------------------------------------------------
    {
      // First check to see if any of our children are complete
      // If they are then we can skip issuing any copies from this level
      LegionMap<Event,FieldMask>::aligned dst_preconditions = preconds;
      if (!valid_views.empty())
      {
        // The fields we need to update are any in the traversal
        // mask plus any from the original copy for which we have dirty data
        FieldMask incomplete_mask = traversal_mask | (dirty_mask & copy_mask);
        // Otherwise, if none are complete, we need to issue update copies
        // from this level assuming we have instances that intersect
        bool already_valid = false;
        // Do a quick check to see if we are done early
        {
          LegionMap<LogicalView*,FieldMask>::aligned::const_iterator finder = 
            valid_views.find(dst);
          if ((finder != valid_views.end()) && 
              !(incomplete_mask - finder->second))
            already_valid = true;
        }
        if (!already_valid && !!incomplete_mask)
        {
          RegionTreeNode *target = dst->logical_node;
          LegionMap<LogicalView*,FieldMask>::aligned valid_instances;
          for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
                valid_views.begin(); it != valid_views.end(); it++)
          {
            FieldMask overlap = incomplete_mask & it->second;
            if (!overlap)
              continue;
            valid_instances[it->first] = overlap;
          }
          LegionMap<MaterializedView*,FieldMask>::aligned src_instances;
          LegionMap<DeferredView*,FieldMask>::aligned deferred_instances;
          // Note that this call destroys valid_instances 
          // and updates incomplete_mask
          target->sort_copy_instances(info, dst, incomplete_mask, 
                      valid_instances, src_instances, deferred_instances);
          if (!src_instances.empty())
          {
            LegionMap<Event,FieldMask>::aligned update_preconditions;
            FieldMask update_mask;
            for (LegionMap<MaterializedView*,FieldMask>::aligned::const_iterator
                  it = src_instances.begin(); it != src_instances.end(); it++)
            {
#ifdef DEBUG_HIGH_LEVEL
              assert(!!it->second);
#endif
              it->first->find_copy_preconditions(0/*redop*/, true/*reading*/,
                                it->second, version_info->get_version_info(), 
                                update_preconditions);
              update_mask |= it->second;
            }
            // Also get the set of destination preconditions
            for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                  preconds.begin(); it != preconds.end(); it++)
            {
              FieldMask overlap = update_mask & it->second;
              if (!overlap)
                continue;
              LegionMap<Event,FieldMask>::aligned::iterator finder = 
                update_preconditions.find(it->first);
              if (finder == update_preconditions.end())
                update_preconditions[it->first] = overlap;
              else
                finder->second |= overlap;
            }
            // Use our version info for the sources
            const VersionInfo &src_info = version_info->get_version_info();
            // Now we have our preconditions so we can issue our copy
            LegionMap<Event,FieldMask>::aligned update_postconditions;
            RegionTreeNode::issue_grouped_copies(context, info, dst, 
                         update_preconditions, update_mask, Event::NO_EVENT,
                         find_intersection_domains(dst->logical_node),
                         src_instances,src_info,update_postconditions,tracker);
            // If we dominate the target, then we can remove
            // the update_mask fields from the traversal_mask
            if (dominates(dst->logical_node))
              traversal_mask -= update_mask;
            // Add all our updates to both the dst_preconditions
            // as well as the actual postconditions.  No need to
            // check for duplicates as we know all these events
            // are brand new and can't be anywhere else.
            if (!update_postconditions.empty())
            {
#ifdef DEBUG_HIGH_LEVEL
              for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                    update_postconditions.begin(); it != 
                    update_postconditions.end(); it++)
              {
                assert(dst_preconditions.find(it->first) == 
                       dst_preconditions.end());
                assert(postconditions.find(it->first) == 
                       postconditions.end());
              }
#endif
              dst_preconditions.insert(update_postconditions.begin(),
                                       update_postconditions.end());
              postconditions.insert(update_postconditions.begin(),
                                    update_postconditions.end());
            }
          }
          // Now if we still have fields which aren't
          // updated then we need to see if we have composite
          // views for those fields
          if (!deferred_instances.empty())
          {
            FieldMask update_mask;
            for (LegionMap<DeferredView*,FieldMask>::aligned::const_iterator
                  it = deferred_instances.begin(); it !=
                  deferred_instances.end(); it++)
            {
              LegionMap<Event,FieldMask>::aligned postconds;
              it->first->issue_deferred_copies(info, dst, it->second,
                                               preconds, postconds, tracker);
              update_mask |= it->second;
              if (!postconds.empty())
              {
#ifdef DEBUG_HIGH_LEVEL
                for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                      postconds.begin(); it != postconds.end(); it++)
                {
                  assert(dst_preconditions.find(it->first) ==
                         dst_preconditions.end());
                  assert(postconditions.find(it->first) ==
                         postconditions.end());
                }
#endif
                dst_preconditions.insert(postconds.begin(), postconds.end());
                postconditions.insert(postconds.begin(), postconds.end());
              }
            }
            // If we dominate the logical node we can remove the
            // updated fields from the traversal mask
            if (dominates(dst->logical_node))
              traversal_mask -= update_mask;
          }
        }
      }

      // Now traverse any open children that intersect with the destination
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        FieldMask overlap = copy_mask & it->second.open_fields;
        // If we have no fields in common or we don't intersect with
        // the child then we can skip traversing this child
        if (!overlap || !it->first->intersects_with(dst->logical_node))
          continue;
        // If we make it here then we need to traverse the child
        it->first->issue_update_copies(info, dst, traversal_mask, 
                                       overlap, dst_preconditions, 
                                       postconditions, tracker);
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::issue_across_copies(const MappableInfo &info,
                                            MaterializedView *dst,
                                            unsigned src_index,
                                            FieldID  src_field,
                                            FieldID  dst_field,
                                            bool    need_field,
                                            std::set<Event> &preconditions,
                                            std::set<Event> &postconditions)
    //--------------------------------------------------------------------------
    {
      std::set<Event> dst_preconditions = preconditions;
      if (!valid_views.empty())
      {
        bool incomplete = need_field || dirty_mask.is_set(src_index);
        if (incomplete)
        {
          FieldMask src_mask; src_mask.set_bit(src_index);
          LegionMap<LogicalView*,FieldMask>::aligned valid_instances;
          for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
                valid_views.begin(); it != valid_views.end(); it++)
          {
            if (it->second.is_set(src_index))
              valid_instances[it->first] = src_mask;
          }
          LegionMap<MaterializedView*,FieldMask>::aligned src_instances;
          LegionMap<DeferredView*,FieldMask>::aligned deferred_instances;
          dst->logical_node->sort_copy_instances(info, dst, src_mask,
                      valid_instances, src_instances, deferred_instances);
          if (!src_instances.empty())
          {
            // There should be at most one of these
#ifdef DEBUG_HIGH_LEVEL
            assert(src_instances.size() == 1);
#endif
            MaterializedView *src = (src_instances.begin())->first;
            LegionMap<Event,FieldMask>::aligned src_preconditions;
            src->find_copy_preconditions(0/*redop*/, true/*reading*/,
                          src_mask, version_info->get_version_info(), 
                          src_preconditions);
            for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
                  src_preconditions.begin(); it != 
                  src_preconditions.end(); it++)
            {
              preconditions.insert(it->first);
            }
            Event copy_pre = Event::merge_events(preconditions);
            std::set<Event> result_events;
            std::vector<Domain::CopySrcDstField> src_fields, dst_fields;
            src->copy_field(src_field, src_fields);
            dst->copy_field(dst_field, dst_fields);
            const std::set<Domain> &overlap_domains = 
              find_intersection_domains(dst->logical_node);
            for (std::set<Domain>::const_iterator it = overlap_domains.begin();
                  it != overlap_domains.end(); it++)
            {
              result_events.insert(context->issue_copy(*it, info.op, src_fields,
                                                       dst_fields, copy_pre));
            }
            Event copy_post = Event::merge_events(result_events);
            if (copy_post.exists())
            {
              // Only need to record the source user as the destination
              // user will be recorded by the copy across operation
              src->add_copy_user(0/*redop*/, copy_post,
                                 version_info->get_version_info(),
                                 src_mask, true/*reading*/);
              // Also add the event to the dst_preconditions and 
              // our post conditions
              dst_preconditions.insert(copy_post);
              postconditions.insert(copy_post);
            }
            // If we dominate then we no longer need to get
            // updates unless they are dirty
            if (dominates(dst->logical_node))
              need_field = false;
          }
          else if (!deferred_instances.empty())
          {
            // There should be at most one of these
#ifdef DEBUG_HIGH_LEVEL
            assert(deferred_instances.size() == 1); 
#endif
            DeferredView *src = (deferred_instances.begin())->first; 
            std::set<Event> postconds;
            Event pre = Event::merge_events(preconditions);
            src->issue_deferred_copies_across(info, dst, src_field,
                                              dst_field, pre, postconds);
            if (!postconds.empty())
            {
              dst_preconditions.insert(postconds.begin(), postconds.end());
              postconditions.insert(postconds.begin(), postconds.end());
            }
            // If we dominate then we no longer need to get
            // updates unless they are dirty
            if (dominates(dst->logical_node))
              need_field = false;
          }
        }
      }
      // Now traverse any open children that intersect with the destination
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        if ((it->second.open_fields.is_set(src_index)) && 
            it->first->intersects_with(dst->logical_node))
        {
          it->first->issue_across_copies(info, dst, src_index,
                                         src_field, dst_field, need_field,
                                         dst_preconditions, postconditions);
        }
      }
    }

    //--------------------------------------------------------------------------
    bool CompositeNode::intersects_with(RegionTreeNode *dst)
    //--------------------------------------------------------------------------
    {
      return logical_node->intersects_with(dst);
    }

    //--------------------------------------------------------------------------
    const std::set<Domain>& CompositeNode::find_intersection_domains(
                                                            RegionTreeNode *dst)
    //--------------------------------------------------------------------------
    {
      return logical_node->get_intersection_domains(dst);
    }

    //--------------------------------------------------------------------------
    void CompositeNode::find_bounding_roots(CompositeView *target,
                                            const FieldMask &bounding_mask)
    //--------------------------------------------------------------------------
    {
      // See if we can fields with exactly one child that dominates
      FieldMask single, multi;
      LegionMap<CompositeNode*,FieldMask>::aligned dominators;
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        FieldMask overlap = it->second.open_fields & bounding_mask;
        if (!overlap)
          continue;
        if (!it->first->dominates(target->logical_node))
          continue;
        LegionMap<CompositeNode*,FieldMask>::aligned::iterator finder = 
          dominators.find(it->first);
        if (finder == dominators.end())
          dominators[it->first] = overlap;
        else
          finder->second |= overlap;
        // Update the multi mask first 
        multi |= (single & overlap);
        // Now update the single mask
        single |= overlap;
      }
      // Subtract any fields from the multi mask from the single mask
      if (!!multi)
        single -= multi;
      // If we still have any single fields then go and issue them
      if (!!single)
      {
        for (LegionMap<CompositeNode*,FieldMask>::aligned::const_iterator it = 
              dominators.begin(); it != dominators.end(); it++)
        {
          FieldMask overlap = single & it->second;
          if (!overlap)
            continue;
          it->first->find_bounding_roots(target, overlap);
        }
        // Now see if we have any leftover fields here
        FieldMask local_mask = bounding_mask - single;
        if (!!local_mask)
          target->add_root(this, local_mask, false/*top*/);
      }
      else
      {
        // There were no single fields so add ourself
        target->add_root(this, bounding_mask, false/*top*/);
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::set_owner_did(DistributedID own_did)
    //--------------------------------------------------------------------------
    {
      owner_did = own_did;
      for (LegionMap<CompositeNode*,ChildInfo>::aligned::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        it->first->set_owner_did(owner_did);
      }
    } 

    //--------------------------------------------------------------------------
    bool CompositeNode::find_field_descriptors(Event term_event,
                                               const RegionUsage &usage, 
                                               const FieldMask &user_mask,
                                               unsigned fid_idx,
                                               Processor local_proc,
                                               LowLevel::IndexSpace target,
                                               Event target_precondition,
                                   std::vector<FieldDataDescriptor> &field_data,
                                   std::set<Event> &preconditions,
                             std::vector<LowLevel::IndexSpace> &already_handled,
                             std::set<Event> &already_preconditions)
    //--------------------------------------------------------------------------
    {
      // We need to find any field descriptors in our children  
      // If any of the children are complete then we are done here too 
      // and can continue on, otherwise, we also need to register at least
      // one local instance if it exists.

      // Keep track of all the index spaces we've handled below
      std::vector<LowLevel::IndexSpace> handled_index_spaces;
      // Keep track of the preconditions for using the handled index spaces
      std::set<Event> handled_preconditions;
      unsigned done_children = 0;
      Event domain_precondition;
      const Domain &local_domain = 
        logical_node->get_domain(domain_precondition);
      bool need_child_intersect = (target != local_domain.get_index_space());
      for (LegionMap<CompositeNode*,ChildInfo>::aligned::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        if (it->second.open_fields.is_set(fid_idx))
        {
          bool done;
          // Compute the low-level index space to ask for from the child
          Event child_precondition;
          const Domain &child_domain = 
            it->first->logical_node->get_domain(child_precondition);
          if (need_child_intersect)
          {
            // Compute the intersection of our target with the child
            std::vector<LowLevel::IndexSpace::BinaryOpDescriptor> ops(1);
            ops[0].op = LowLevel::IndexSpace::ISO_INTERSECT;
            ops[0].parent = local_domain.get_index_space();
            ops[0].left_operand = target;
            ops[0].right_operand = child_domain.get_index_space();
            Event pre = Event::merge_events(target_precondition, 
                                      child_precondition, domain_precondition);
            Event child_ready = LowLevel::IndexSpace::compute_index_spaces(ops,
                                                        false/*mutable*/, pre);
            done = it->first->find_field_descriptors(term_event, usage,
                                                   user_mask,fid_idx,local_proc,
                                                   ops[0].result, child_ready,
                                                   field_data, preconditions,
                                                   handled_index_spaces,
                                                   handled_preconditions);
            // We can also issue the deletion for the child index space
            ops[0].result.destroy(term_event);
          }
          else
            done = it->first->find_field_descriptors(term_event, usage,
                                                user_mask, fid_idx, local_proc,
                                                child_domain.get_index_space(), 
                                                child_precondition,
                                                field_data, preconditions,
                                                handled_index_spaces,
                                                handled_preconditions);
          // If it is complete and we handled everything, then we are done
          if (done)
          {
            done_children++;
            if (it->second.complete && done)
              return true;
          }
        }
      }
      // If we're complete and we closed all the children, then we are
      //also done
      if (logical_node->is_complete() && 
          (done_children == logical_node->get_num_children()))
        return true;
      // If we make it here, we weren't able to cover ourselves, so make an 
      // index space for the remaining set of points we need to handle
      // First compute what we did handle
      LowLevel::IndexSpace local_handled = LowLevel::IndexSpace::NO_SPACE;
      Event local_precondition = Event::NO_EVENT;
      if (handled_index_spaces.size() == 1)
      {
        local_handled = handled_index_spaces.front();
        if (!handled_preconditions.empty())
          local_precondition = *(handled_preconditions.begin());
      }
      else if (handled_index_spaces.size() > 1)
      {
        Event parent_precondition;
        const Domain &parent_dom = 
          logical_node->get_domain(parent_precondition);
        if (parent_precondition.exists())
          handled_preconditions.insert(parent_precondition);
        // Compute the union of all our handled index spaces
        Event handled_pre = Event::merge_events(handled_preconditions);
        local_precondition = LowLevel::IndexSpace::reduce_index_spaces( 
                              LowLevel::IndexSpace::ISO_UNION,
                              handled_index_spaces, local_handled,
                              false/*not mutable*/, 
                              parent_dom.get_index_space(), handled_pre);
        // We can also emit the destruction for this temporary index space now
        local_handled.destroy(term_event);
      }
      // Now we can compute the remaining part of the index space
      LowLevel::IndexSpace remaining_space = target;
      Event remaining_precondition = target_precondition;
      if (local_handled.exists())
      {
        // Compute the set difference
        std::vector<LowLevel::IndexSpace::BinaryOpDescriptor> ops(1);
        ops[0].op = LowLevel::IndexSpace::ISO_SUBTRACT;
        ops[0].parent = local_domain.get_index_space();
        ops[0].left_operand = target;
        ops[0].right_operand = local_handled;
        Event pre = Event::merge_events(target_precondition,
                                        local_precondition,domain_precondition);
        remaining_precondition = LowLevel::IndexSpace::compute_index_spaces(ops,
                                                        false/*mutable*/, pre);
        remaining_space = ops[0].result;
        // We also emit the destruction for this temporary index space
        remaining_space.destroy(term_event);
      }
      // If we make it here we need to register at least one instance
      // from ourself if there are any
      DeferredView *deferred_view = NULL;
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.begin(); it != valid_views.end(); it++)
      {
        // Check to see if the instance is valid for our target field
        if (it->second.is_set(fid_idx))
        {
          // See if this is a composite view
          if (it->first->is_instance_view())
          {
#ifdef DEBUG_HIGH_LEVEL
            assert(it->first->as_instance_view()->is_materialized_view());
#endif
            MaterializedView *view = 
              it->first->as_instance_view()->as_materialized_view();
            // Record the instance and its information
            field_data.push_back(FieldDataDescriptor());
            view->set_descriptor(field_data.back(), fid_idx);
              
            field_data.back().index_space = remaining_space;
            // Register ourselves as a user of this instance
            InstanceRef ref = view->add_user(usage, term_event, user_mask,
                                        version_info->get_version_info());
            Event ready_event = Event::merge_events(ref.get_ready_event(),
                                                    remaining_precondition);
            if (ready_event.exists())
              preconditions.insert(ready_event);
            // Record that we handled the remaining space
            already_handled.push_back(remaining_space);
            if (remaining_precondition.exists())
              already_preconditions.insert(remaining_precondition);
            // We found an actual instance, so we are done
            return true;
          }
          else
          {
            // Save it as a composite view and keep going
#ifdef DEBUG_HIGH_LEVEL
            assert(it->first->is_deferred_view());
            assert(deferred_view == NULL);
#endif
            deferred_view = it->first->as_deferred_view();
          }
        }
      }
      // If we made it here, we're not sure if we covered everything
      // or not, so record what we have handled
      if (local_handled.exists())
      {
        already_handled.push_back(local_handled);
        if (local_precondition.exists())
          already_preconditions.insert(local_precondition);
      }
      // If we still have a composite view, then register that
      if (deferred_view != NULL)
        return deferred_view->find_field_descriptors(term_event, usage, 
                                                     user_mask, fid_idx, 
                                                     local_proc,
                                                     remaining_space, 
                                                     remaining_precondition,
                                                     field_data, preconditions,
                                                     already_handled,
                                                     already_preconditions);
      return false;
    }

    //--------------------------------------------------------------------------
    void CompositeNode::add_gc_references(void)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
            valid_views.begin(); it != valid_views.end(); it++)
      {
        it->first->add_nested_gc_ref(owner_did);
      }
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        it->first->add_gc_references();
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::remove_gc_references(void)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.end(); it != valid_views.end(); it++)
      {
        // Don't worry about deletion condition since we own resource refs
        it->first->remove_nested_gc_ref(owner_did);
      }
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        it->first->remove_gc_references();
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::add_valid_references(void)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it =
            valid_views.begin(); it != valid_views.end(); it++)
      {
        it->first->add_nested_valid_ref(owner_did);
      }
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        it->first->add_valid_references();
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::remove_valid_references(void)
    //--------------------------------------------------------------------------
    {
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.end(); it != valid_views.end(); it++)
      {
        // Don't worry about deletion condition since we own resource refs
        it->first->add_nested_valid_ref(owner_did);
      }
      for (std::map<CompositeNode*,ChildInfo>::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        it->first->remove_valid_references();
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::pack_composite_tree(Serializer &rez, 
                                            AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      rez.serialize(dirty_mask);
      rez.serialize<size_t>(valid_views.size());
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.begin(); it != valid_views.end(); it++)
      {
        // Only need to send the structure for now, we'll check for
        // updates when we unpack and request anything we need later
        DistributedID did = it->first->send_view_base(target);
        rez.serialize(did);
        rez.serialize(it->second);
      }
      rez.serialize<size_t>(open_children.size());
      for (LegionMap<CompositeNode*,ChildInfo>::aligned::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        rez.serialize(it->first->logical_node->get_color());
        rez.serialize<bool>(it->second.complete);
        rez.serialize(it->second.open_fields);
        it->first->pack_composite_tree(rez, target);
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::unpack_composite_tree(Deserializer &derez,
                                              AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      FieldSpaceNode *field_node = logical_node->column_source;
      derez.deserialize(dirty_mask); 
      field_node->transform_field_mask(dirty_mask, source);
      size_t num_valid_views;
      derez.deserialize(num_valid_views);
      for (unsigned idx = 0; idx < num_valid_views; idx++)
      {
        DistributedID did;
        derez.deserialize(did);
        LogicalView *view = logical_node->find_view(did);
        FieldMask &mask = valid_views[view];
        derez.deserialize(mask);
        field_node->transform_field_mask(mask, source);
      }
      size_t num_open_children;
      derez.deserialize(num_open_children);
      for (unsigned idx = 0; idx < num_open_children; idx++)
      {
        ColorPoint child_color;
        derez.deserialize(child_color);
        RegionTreeNode *child_node = logical_node->get_tree_child(child_color);
        CompositeNode *new_node = legion_new<CompositeNode>(child_node, this,
                                                            version_info);
        ChildInfo &info = open_children[new_node];
        derez.deserialize<bool>(info.complete);
        derez.deserialize(info.open_fields);
        field_node->transform_field_mask(info.open_fields, source);
        new_node->unpack_composite_tree(derez, source);
      }
    }

    //--------------------------------------------------------------------------
    void CompositeNode::make_local(std::set<Event> &preconditions,
                                   std::set<DistributedID> &checked_views)
    //--------------------------------------------------------------------------
    {
      // Check all our views for composite instances so we do any
      // recursive checking for up-to-date views
      for (LegionMap<LogicalView*,FieldMask>::aligned::const_iterator it = 
            valid_views.begin(); it != valid_views.end(); it++)
      {
        // If we already checked this view, we are good
        if (checked_views.find(it->first->did) != checked_views.end())
          continue;
        checked_views.insert(it->first->did);
        if (it->first->is_deferred_view())
        {
          DeferredView *def_view = it->first->as_deferred_view();
          if (def_view->is_composite_view())
          {
            def_view->as_composite_view()->make_local(preconditions);
          }
        }
      }
      // Then traverse any children
      for (LegionMap<CompositeNode*,ChildInfo>::aligned::const_iterator it = 
            open_children.begin(); it != open_children.end(); it++)
      {
        it->first->make_local(preconditions, checked_views);
      }
    }
    
    //--------------------------------------------------------------------------
    bool CompositeNode::dominates(RegionTreeNode *dst)
    //--------------------------------------------------------------------------
    {
      return logical_node->dominates(dst);
    }

    /////////////////////////////////////////////////////////////
    // FillView 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    FillView::FillView(RegionTreeForest *ctx, DistributedID did,
                       AddressSpaceID owner_proc, AddressSpaceID local_proc,
                       RegionTreeNode *node, bool reg_now, 
                       FillViewValue *val, FillView *par)
      : DeferredView(ctx, did, owner_proc, local_proc, node, reg_now), 
        parent(par), value(val)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(value != NULL);
#endif
      value->add_reference();
      if (parent != NULL)
        add_nested_resource_ref(did);
      else if (!is_owner())
      {
        add_base_resource_ref(REMOTE_DID_REF);
        send_remote_registration();
      }
#ifdef LEGION_GC
      log_garbage.info("GC Fill View %ld", did);
#endif
    }

    //--------------------------------------------------------------------------
    FillView::FillView(const FillView &rhs)
      : DeferredView(NULL, 0, 0, 0, NULL, false), parent(NULL), value(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }
    
    //--------------------------------------------------------------------------
    FillView::~FillView(void)
    //--------------------------------------------------------------------------
    {
      if (value->remove_reference())
        delete value;
      // Clean up our children and capture their destruction events
      for (std::map<ColorPoint,FillView*>::const_iterator it = 
            children.begin(); it != children.end(); it++)
      {
        recycle_events.insert(it->second->get_destruction_event());
        if (it->second->remove_nested_resource_ref(did))
          legion_delete(it->second);
      }
      if ((parent == NULL) && is_owner())
      {
        UpdateReferenceFunctor<RESOURCE_REF_KIND,false/*add*/> functor(this);
        map_over_remote_instances(functor);
        // If we are the top the tree on the owner node we can recycle 
        // the distributed ID once our destruction event triggers
        runtime->recycle_distributed_id(did, destruction_event);
      }
    }

    //--------------------------------------------------------------------------
    FillView& FillView::operator=(const FillView &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void FillView::notify_active(void)
    //--------------------------------------------------------------------------
    {
      activate_deferred();
    }

    //--------------------------------------------------------------------------
    void FillView::notify_inactive(void)
    //--------------------------------------------------------------------------
    {
      deactivate_deferred();
    }
    
    //--------------------------------------------------------------------------
    void FillView::notify_valid(void)
    //--------------------------------------------------------------------------
    {
      if (parent == NULL)
      {
        if (!is_owner())
          send_remote_valid_update(owner_space, 1/*count*/, true/*add*/);
      }
      else 
        parent->add_nested_valid_ref(did);
      validate_deferred();
    }

    //--------------------------------------------------------------------------
    void FillView::notify_invalid(void)
    //--------------------------------------------------------------------------
    {
      invalidate_deferred();
      if (parent == NULL)
      {
        if (!is_owner())
          send_remote_valid_update(owner_space, 1/*count*/, false/*add*/);
      }
      else if (parent->remove_nested_valid_ref(did))
        legion_delete(parent);
    }

    //--------------------------------------------------------------------------
    DistributedID FillView::send_view_base(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      if (!has_remote_instance(target))
      {
        if (parent == NULL)
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(logical_node->is_region()); // Always regions at the top
#endif
          // Don't take the lock, it's alright to have duplicate sends
          Serializer rez;
          {
            RezCheck z(rez);
            rez.serialize(did);
            rez.serialize(owner_space);
            rez.serialize(logical_node->as_region_node()->handle);
            rez.serialize(value->value_size);
            rez.serialize(value->value, value->value_size);
          }
          runtime->send_fill_view(target, rez);
        }
        else // Ask our parent to do the send
          parent->send_view_base(target);
        // We've now done the send so record it
        update_remote_instances(target);
      }
      return did;
    }

    //--------------------------------------------------------------------------
    void FillView::send_view_updates(AddressSpaceID target, 
                                     const FieldMask &update_mask)
    //--------------------------------------------------------------------------
    {
      // We only need to send updates for our constituent reduction views
      send_deferred_view_updates(target, update_mask);
    }

    //--------------------------------------------------------------------------
    /*static*/ void FillView::handle_send_fill_view(Runtime *runtime,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      DistributedID did;
      derez.deserialize(did);
      AddressSpaceID owner_space;
      derez.deserialize(owner_space);
      LogicalRegion handle;
      derez.deserialize(handle);
      size_t value_size;
      derez.deserialize(value_size);
      void *value = malloc(value_size);
      derez.deserialize(value, value_size);
      
      RegionNode *target_node = runtime->forest->get_node(handle);
      FillView::FillViewValue *fill_value = 
                      new FillView::FillViewValue(value, value_size);
      FillView *new_view = legion_new<FillView>(runtime->forest, did, 
                                  owner_space, runtime->address_space,
                                  target_node, false/*register now*/,
                                  fill_value);
      if (!target_node->register_logical_view(new_view))
        legion_delete(new_view);
      else
        new_view->update_remote_instances(source);
    }

    //--------------------------------------------------------------------------
    LogicalView* FillView::get_subview(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // See if we already have this child
      {
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
        std::map<ColorPoint,FillView*>::const_iterator finder = 
                                                            children.find(c);
        if (finder != children.end())
          return finder->second;
      }
      RegionTreeNode *child_node = logical_node->get_tree_child(c);
      FillView *child_view = legion_new<FillView>(context, did,
                                                  owner_space, local_space,
                                                  child_node, false/*register*/,
                                                  value, this/*parent*/);
      // Retake the lock and try and add the child, see if someone else added
      // the child in the meantime
      bool free_child_view = false;
      FillView *result = child_view;
      {
        AutoLock v_lock(view_lock);
        std::map<ColorPoint,FillView*>::const_iterator finder = 
                                                          children.find(c);
        if (finder != children.end())
        {
          if (child_view->remove_nested_resource_ref(did))
            free_child_view = true;
          result = finder->second;
        }
        else
        {
          children[c] = child_view;
          // Update the subviews while holding the lock
          for (std::deque<ReductionEpoch>::const_iterator rit = 
                reduction_epochs.begin(); rit != reduction_epochs.end(); rit++)
          {
            const ReductionEpoch &epoch = *rit;
            for (std::set<ReductionView*>::const_iterator it = 
                  epoch.views.begin(); it != epoch.views.end(); it++)
            {
              child_view->update_reduction_views(*it, epoch.valid_fields,
                                                 false/*update parent*/);
            }
          }
          // Register the child node
          child_node->register_logical_view(child_view);
        }
      }
      if (free_child_view)
        legion_delete(child_view);
      return result;
    }

    //--------------------------------------------------------------------------
    void FillView::update_child_reduction_views(ReductionView *view,
                                                const FieldMask &valid_mask,
                                                DeferredView *to_skip/*= NULL*/)
    //--------------------------------------------------------------------------
    {
      std::map<ColorPoint,FillView*> to_handle;
      {
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
        to_handle = children;
      }
      std::set<ColorPoint> handled;
      // Keep iterating until we've handled all the children
      while (!to_handle.empty())
      {
        for (std::map<ColorPoint,FillView*>::const_iterator it = 
              to_handle.begin(); it != to_handle.end(); it++)
        {
#ifdef DEBUG_HIGH_LEVEL
          assert(handled.find(it->first) == handled.end());
#endif
          handled.insert(it->first);
          if (it->second == to_skip)
            continue;
          it->second->update_reduction_views(view, valid_mask, false/*parent*/);
        }
        to_handle.clear();
        AutoLock v_lock(view_lock, 1, false/*exclusive*/);
#ifdef DEBUG_HIGH_LEVEL
        assert(handled.size() <= children.size());
#endif
        if (handled.size() == children.size())
          break;
        // Otherwise figure out what additional children to handle
        for (std::map<ColorPoint,FillView*>::const_iterator it = 
              children.begin(); it != children.end(); it++)
        {
          if (handled.find(it->first) == handled.end())
            to_handle.insert(*it);
        }
      }
    }

    //--------------------------------------------------------------------------
    void FillView::issue_deferred_copies(const MappableInfo &info,
                                         MaterializedView *dst,
                                         const FieldMask &copy_mask,
                                         CopyTracker *tracker)
    //--------------------------------------------------------------------------
    {
      LegionMap<Event,FieldMask>::aligned preconditions;
      dst->find_copy_preconditions(0/*redop*/, false/*reading*/,
                                   copy_mask, info.version_info, preconditions);
      // Compute the precondition sets
      LegionList<EventSet>::aligned precondition_sets;
      RegionTreeNode::compute_event_sets(copy_mask, preconditions,
                                         precondition_sets);
      // Iterate over the precondition sets
      for (LegionList<EventSet>::aligned::iterator pit = 
            precondition_sets.begin(); pit !=
            precondition_sets.end(); pit++)
      {
        EventSet &pre_set = *pit;
        // Build the src and dst fields vectors
        std::vector<Domain::CopySrcDstField> dst_fields;
        dst->copy_to(pre_set.set_mask, dst_fields);
        Event fill_pre = Event::merge_events(pre_set.preconditions);
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        if (!fill_pre.exists())
        {
          UserEvent new_fill_pre = UserEvent::create_user_event();
          new_fill_pre.trigger();
          fill_pre = new_fill_pre;
        }
#endif
#ifdef LEGION_LOGGING
        LegionLogging::log_event_dependences(
            Processor::get_executing_processor(), 
            pre_set.preconditions, fill_pre);
#endif
#ifdef LEGION_SPY
        LegionSpy::log_event_dependences(pre_set.preconditions, fill_pre);
#endif
        // Issue the fill commands
        Event fill_post;
        if (dst->logical_node->has_component_domains())
        {
          std::set<Event> post_events; 
          Event dom_pre;
          const std::set<Domain> &fill_domains = 
            dst->logical_node->get_component_domains(dom_pre);
          if (dom_pre.exists())
            fill_pre = Event::merge_events(fill_pre, dom_pre);
          UniqueID op_id = (info.op == NULL) ? 0 : info.op->get_unique_op_id();
          for (std::set<Domain>::const_iterator it = fill_domains.begin();
                it != fill_domains.end(); it++)
          {
            post_events.insert(context->issue_fill(*it, op_id,
                                                   dst_fields, value->value,
                                                   value->value_size,fill_pre));
          }
          fill_post = Event::merge_events(post_events);
        }
        else
        {
          Event dom_pre;
          const Domain &dom = dst->logical_node->get_domain(dom_pre);
          if (dom_pre.exists())
            fill_pre = Event::merge_events(fill_pre, dom_pre);
          UniqueID op_id = (info.op == NULL) ? 0 : info.op->get_unique_op_id();
          fill_post = context->issue_fill(dom, op_id, dst_fields,
                                          value->value, value->value_size, 
                                          fill_pre);
        }
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        if (!fill_post.exists())
        {
          UserEvent new_fill_post = UserEvent::create_user_event();
          new_fill_post.trigger();
          fill_post = new_fill_post;
        }
#endif
        // Now see if there are any reductions to apply
        FieldMask reduce_overlap = reduction_mask & pre_set.set_mask;
        if (!!reduce_overlap)
        {
          // See if we have any reductions to flush
          LegionMap<Event,FieldMask>::aligned reduce_conditions;
          if (fill_post.exists())
            reduce_conditions[fill_post] = pre_set.set_mask;
          flush_reductions(info, dst, reduce_overlap, reduce_conditions);
          // Sort out the post-conditions into different groups 
          LegionList<EventSet>::aligned postcondition_sets;
          RegionTreeNode::compute_event_sets(pre_set.set_mask, 
                                             reduce_conditions,
                                             postcondition_sets);
          // Add each of the different postconditions separately
          for (LegionList<EventSet>::aligned::iterator it = 
               postcondition_sets.begin(); it !=
               postcondition_sets.end(); it++)
          {
            Event reduce_post = Event::merge_events(it->preconditions);
            if (reduce_post.exists())
            {
              if (tracker != NULL)
                tracker->add_copy_event(reduce_post);
              dst->add_copy_user(0/*redop*/, reduce_post, info.version_info,
                                 it->set_mask, false/*reading*/);
            }
          }
        }
        else if (fill_post.exists())
        {
          if (tracker != NULL)
            tracker->add_copy_event(fill_post);
          dst->add_copy_user(0/*redop*/, fill_post, info.version_info,
                             pre_set.set_mask, false/*reading*/);
        }
      }
    }
    
    //--------------------------------------------------------------------------
    void FillView::issue_deferred_copies(const MappableInfo &info,
                                         MaterializedView *dst,
                                         const FieldMask &copy_mask,
             const LegionMap<Event,FieldMask>::aligned &preconditions,
                   LegionMap<Event,FieldMask>::aligned &postconditions,
                                         CopyTracker *tracker)
    //--------------------------------------------------------------------------
    {
      // Do the same thing as above, but no need to add ourselves as user
      // or compute the destination preconditions as they are already included
      LegionList<EventSet>::aligned precondition_sets;
      RegionTreeNode::compute_event_sets(copy_mask, preconditions,
                                         precondition_sets);
      // Iterate over the precondition sets
      for (LegionList<EventSet>::aligned::iterator pit = 
            precondition_sets.begin(); pit !=
            precondition_sets.end(); pit++)
      {
        EventSet &pre_set = *pit;
        // Build the src and dst fields vectors
        std::vector<Domain::CopySrcDstField> dst_fields;
        dst->copy_to(pre_set.set_mask, dst_fields);
        Event fill_pre = Event::merge_events(pre_set.preconditions);
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        if (!fill_pre.exists())
        {
          UserEvent new_fill_pre = UserEvent::create_user_event();
          new_fill_pre.trigger();
          fill_pre = new_fill_pre;
        }
#endif
#ifdef LEGION_LOGGING
        LegionLogging::log_event_dependences(
            Processor::get_executing_processor(), 
            pre_set.preconditions, fill_pre);
#endif
#ifdef LEGION_SPY
        LegionSpy::log_event_dependences(pre_set.preconditions, fill_pre);
#endif
        // Issue the fill commands
        Event fill_post;
        if (dst->logical_node->has_component_domains())
        {
          std::set<Event> post_events; 
          Event dom_pre;
          const std::set<Domain> &fill_domains = 
            dst->logical_node->get_component_domains(dom_pre);
          if (dom_pre.exists())
            fill_pre = Event::merge_events(fill_pre, dom_pre);
          UniqueID op_id = (info.op == NULL) ? 0 : info.op->get_unique_op_id();
          for (std::set<Domain>::const_iterator it = fill_domains.begin();
                it != fill_domains.end(); it++)
          {
            post_events.insert(context->issue_fill(*it, op_id, dst_fields,
                                                   value->value, 
                                                   value->value_size,fill_pre));
          }
          fill_post = Event::merge_events(post_events);
        }
        else
        {
          Event dom_pre;
          const Domain &dom = dst->logical_node->get_domain(dom_pre);
          if (dom_pre.exists())
            fill_pre = Event::merge_events(fill_pre, dom_pre);
          UniqueID op_id = (info.op == NULL) ? 0 : info.op->get_unique_op_id();
          fill_post = context->issue_fill(dom, op_id, dst_fields,
                                          value->value, value->value_size, 
                                          fill_pre);
        }
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
        if (!fill_post.exists())
        {
          UserEvent new_fill_post = UserEvent::create_user_event();
          new_fill_post.trigger();
          fill_post = new_fill_post;
        }
#endif
        FieldMask reduce_overlap = reduction_mask & pre_set.set_mask;
        if (!!reduce_overlap)
          flush_reductions(info, dst, reduce_overlap, postconditions);
      }
    }

    //--------------------------------------------------------------------------
    void FillView::issue_deferred_copies_across(const MappableInfo &info,
                                                MaterializedView *dst,
                                                FieldID src_field,
                                                FieldID dst_field,
                                                Event precondition,
                                          std::set<Event> &postconditions)
    //--------------------------------------------------------------------------
    {
      std::vector<Domain::CopySrcDstField> dst_fields;   
      dst->copy_field(dst_field, dst_fields);
      // Issue the copy to the low-level runtime and get back the event
      std::set<Event> post_events;
      const std::set<Domain> &overlap_domains = 
        logical_node->get_intersection_domains(dst->logical_node);
      UniqueID op_id = (info.op == NULL) ? 0 : info.op->get_unique_op_id();
      for (std::set<Domain>::const_iterator it = overlap_domains.begin();
            it != overlap_domains.end(); it++)
      {
        post_events.insert(context->issue_fill(*it, op_id, dst_fields,
                                              value->value, value->value_size, 
                                              precondition));
      }
      Event post_event = Event::merge_events(post_events); 
      // If we're going to issue a reduction then we can just flush reductions
      // and the precondition will translate naturally
      if (!!reduction_mask)
        flush_reductions_across(info, dst, src_field, dst_field,
                                post_event, postconditions);
      else
        postconditions.insert(post_event);
    }

    //--------------------------------------------------------------------------
    void FillView::find_field_descriptors(Event term_event, 
                                          const RegionUsage &usage,
                                          const FieldMask &user_mask,
                                          unsigned fid_idx,
                                          Processor local_proc,
                                  std::vector<FieldDataDescriptor> &field_data,
                                          std::set<Event> &preconditions)
    //--------------------------------------------------------------------------
    {
      // We should never get here
      assert(false);
    }

    //--------------------------------------------------------------------------
    bool FillView::find_field_descriptors(Event term_event,
                                          const RegionUsage &usage,
                                          const FieldMask &user_mask,
                                          unsigned fid_idx,
                                          Processor local_proc,
                                          LowLevel::IndexSpace target,
                                          Event target_precondition,
                                  std::vector<FieldDataDescriptor> &field_data,
                                          std::set<Event> &preconditions,
                             std::vector<LowLevel::IndexSpace> &already_handled,
                                       std::set<Event> &already_preconditions)
    //--------------------------------------------------------------------------
    {
      // We should never get here
      assert(false);
      return false;
    }
    
    /////////////////////////////////////////////////////////////
    // ReductionView 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    ReductionView::ReductionView(RegionTreeForest *ctx, DistributedID did,
                                 AddressSpaceID own_sp, AddressSpaceID loc_sp,
                                 RegionTreeNode *node, ReductionManager *man,
                                 bool register_now)
      : InstanceView(ctx, did, own_sp, loc_sp, node, register_now), manager(man)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(manager != NULL);
#endif
      manager->add_nested_resource_ref(did);
      if (!is_owner())
      {
        add_base_resource_ref(REMOTE_DID_REF);
        send_remote_registration();
      }
#ifdef LEGION_GC
      log_garbage.info("GC Reduction View %ld %ld", did, manager->did);
#endif
    }

    //--------------------------------------------------------------------------
    ReductionView::ReductionView(const ReductionView &rhs)
      : InstanceView(NULL, 0, 0, 0, NULL, false), manager(NULL)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    ReductionView::~ReductionView(void)
    //--------------------------------------------------------------------------
    {
      if (is_owner())
      {
        // If we're the owner, remove our valid references on remote nodes
        UpdateReferenceFunctor<RESOURCE_REF_KIND,false/*add*/> functor(this);
        map_over_remote_instances(functor);
        // If we are the owner node, then we can recycle the distributed ID
        runtime->recycle_distributed_id(did, destruction_event);
      }
      if (manager->remove_nested_resource_ref(did))
      {
        if (manager->is_list_manager())
          legion_delete(manager->as_list_manager());
        else
          legion_delete(manager->as_fold_manager());
      }
      // Remove any initial users as well
      if (!initial_user_events.empty())
      {
        for (std::set<Event>::const_iterator it = initial_user_events.begin();
              it != initial_user_events.end(); it++)
          filter_local_users(*it);
      }
#if !defined(LEGION_SPY) && !defined(LEGION_LOGGING) && \
      !defined(EVENT_GRAPH_TRACE) && defined(DEBUG_HIGH_LEVEL)
      assert(reduction_users.empty());
      assert(reading_users.empty());
      assert(outstanding_gc_events.empty());
#endif
    }

    //--------------------------------------------------------------------------
    ReductionView& ReductionView::operator=(const ReductionView &rhs)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return *this;
    }

    //--------------------------------------------------------------------------
    void ReductionView::perform_reduction(InstanceView *target,
                                          const FieldMask &reduce_mask,
                                          const VersionInfo &version_info,
                                          Processor local_proc,
                                          Operation *op,
                                          CopyTracker *tracker /*= NULL*/)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, PERFORM_REDUCTION_CALL);
#endif
      std::vector<Domain::CopySrcDstField> src_fields;
      std::vector<Domain::CopySrcDstField> dst_fields;
      bool fold = target->reduce_to(manager->redop, reduce_mask, dst_fields);
      this->reduce_from(manager->redop, reduce_mask, src_fields);

      LegionMap<Event,FieldMask>::aligned preconditions;
      target->find_copy_preconditions(manager->redop, false/*reading*/, 
                                      reduce_mask, version_info, preconditions);
      this->find_copy_preconditions(manager->redop, true/*reading*/, 
                                    reduce_mask, version_info, preconditions);
      std::set<Event> event_preconds;
      for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
            preconditions.begin(); it != preconditions.end(); it++)
      {
        event_preconds.insert(it->first);
      }
      Event reduce_pre = Event::merge_events(event_preconds); 
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
      if (!reduce_pre.exists())
      {
        UserEvent new_reduce_pre = UserEvent::create_user_event();
        new_reduce_pre.trigger();
        reduce_pre = new_reduce_pre;
      }
      IndexSpace reduce_index_space;
#endif
#ifdef LEGION_LOGGING
      LegionLogging::log_event_dependences(
          Processor::get_executing_processor(), event_preconds, reduce_pre);
#endif
#ifdef LEGION_SPY
      LegionSpy::log_event_dependences(event_preconds, reduce_pre);
#endif
      Event reduce_post; 
      if (logical_node->has_component_domains())
      {
        std::set<Event> post_events;
        Event dom_pre;
        const std::set<Domain> &component_domains = 
          logical_node->get_component_domains(dom_pre);
        if (dom_pre.exists())
          reduce_pre = Event::merge_events(reduce_pre, dom_pre);
        for (std::set<Domain>::const_iterator it = 
              component_domains.begin(); it != component_domains.end(); it++)
        {
          Event post = manager->issue_reduction(op, src_fields, dst_fields,
                                                *it, reduce_pre, fold, 
                                                true/*precise*/);
          post_events.insert(post);
        }
        reduce_post = Event::merge_events(post_events);
#if defined(LEGION_SPY) || defined(LEGION_LOGGING)
        reduce_index_space = logical_node->as_region_node()->row_source->handle;
#endif
      }
      else
      {
        Event dom_pre;
        Domain domain = logical_node->get_domain(dom_pre);
        if (dom_pre.exists())
          reduce_pre = Event::merge_events(reduce_pre, dom_pre);
        reduce_post = manager->issue_reduction(op, src_fields, dst_fields,
                                               domain, reduce_pre, fold,
                                               true/*precise*/);
#if defined(LEGION_SPY) || defined(LEGION_LOGGING)
        reduce_index_space = logical_node->as_region_node()->row_source->handle;
#endif
      }
#if defined(LEGION_SPY) || defined(LEGION_LOGGING)
      if (!reduce_post.exists())
      {
        UserEvent new_reduce_post = UserEvent::create_user_event();
        new_reduce_post.trigger();
        reduce_post = new_reduce_post;
      }
#endif
      target->add_copy_user(manager->redop, reduce_post, version_info,
                            reduce_mask, false/*reading*/);
      this->add_copy_user(manager->redop, reduce_post, version_info,
                          reduce_mask, true/*reading*/);
      if (tracker != NULL)
        tracker->add_copy_event(reduce_post);
#ifdef LEGION_LOGGING
      {
        std::set<FieldID> reduce_fields;
        manager->region_node->column_source->to_field_set(reduce_mask,
                                                          reduce_fields);
        LegionLogging::log_lowlevel_copy(
            Processor::get_executing_processor(),
            manager->get_instance(),
            target->get_manager()->get_instance(),
            reduce_index_space,
            manager->region_node->column_source->handle,
            manager->region_node->handle.tree_id,
            reduce_pre, reduce_post, reduce_fields, manager->redop);
      }
#endif
#ifdef LEGION_SPY
      {
        std::set<FieldID> field_set;
        manager->region_node->column_source->to_field_set(reduce_mask,
            field_set);
        LegionSpy::log_copy_operation(manager->get_instance().id,
            target->get_manager()->get_instance().id,
            reduce_index_space.get_id(),
            manager->region_node->column_source->handle.id,
            manager->region_node->handle.tree_id, reduce_pre, reduce_post,
            manager->redop, field_set);
      }
#endif
    } 

    //--------------------------------------------------------------------------
    Event ReductionView::perform_deferred_reduction(MaterializedView *target,
                                                    const FieldMask &red_mask,
                                                const VersionInfo &version_info,
                                                    const std::set<Event> &pre,
                                         const std::set<Domain> &reduce_domains,
                                                    Event dom_precondition,
                                                    Operation *op)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, PERFORM_REDUCTION_CALL);
#endif
      std::vector<Domain::CopySrcDstField> src_fields;
      std::vector<Domain::CopySrcDstField> dst_fields;
      bool fold = target->reduce_to(manager->redop, red_mask, dst_fields);
      this->reduce_from(manager->redop, red_mask, src_fields);

      LegionMap<Event,FieldMask>::aligned src_pre;
      // Don't need to ask the target for preconditions as they 
      // are included as part of the pre set
      find_copy_preconditions(manager->redop, true/*reading*/,
                              red_mask, version_info, src_pre);
      std::set<Event> preconditions = pre;
      if (dom_precondition.exists())
        preconditions.insert(dom_precondition);
      for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
            src_pre.begin(); it != src_pre.end(); it++)
      {
        preconditions.insert(it->first);
      }
      Event reduce_pre = Event::merge_events(preconditions); 
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
      if (!reduce_pre.exists())
      {
        UserEvent new_reduce_pre = UserEvent::create_user_event();
        new_reduce_pre.trigger();
        reduce_pre = new_reduce_pre;
      }
#endif
#ifdef LEGION_LOGGING
      LegionLogging::log_event_dependences(
          Processor::get_executing_processor(), preconditions, reduce_pre);
#endif
#ifdef LEGION_SPY
      LegionSpy::log_event_dependences(preconditions, reduce_pre);
#endif
      std::set<Event> post_events;
      for (std::set<Domain>::const_iterator it = reduce_domains.begin();
            it != reduce_domains.end(); it++)
      {
        Event post = manager->issue_reduction(op, src_fields, dst_fields,
                                              *it, reduce_pre, fold,
                                              false/*precise*/);
        post_events.insert(post);
      }
      Event reduce_post = Event::merge_events(post_events);
      // No need to add the user to the destination as that will
      // be handled by the caller using the reduce post event we return
      add_copy_user(manager->redop, reduce_post, version_info,
                    red_mask, true/*reading*/);
#if defined(LEGION_SPY) || defined(LEGION_LOGGING)
      IndexSpace reduce_index_space =
              target->logical_node->as_region_node()->row_source->handle;
      if (!reduce_post.exists())
      {
        UserEvent new_reduce_post = UserEvent::create_user_event();
        new_reduce_post.trigger();
        reduce_post = new_reduce_post;
      }
#endif
#ifdef LEGION_LOGGING
      {
        std::set<FieldID> reduce_fields;
        manager->region_node->column_source->to_field_set(red_mask,
                                                          reduce_fields);
        LegionLogging::log_lowlevel_copy(
            Processor::get_executing_processor(),
            manager->get_instance(),
            target->get_manager()->get_instance(),
            reduce_index_space,
            manager->region_node->column_source->handle,
            manager->region_node->handle.tree_id,
            reduce_pre, reduce_post, reduce_fields, manager->redop);
      }
#endif
#ifdef LEGION_SPY
      {
        std::set<FieldID> field_set;
        manager->region_node->column_source->to_field_set(red_mask, field_set);
        LegionSpy::log_copy_operation(manager->get_instance().id,
            target->get_manager()->get_instance().id,
            reduce_index_space.get_id(),
            manager->region_node->column_source->handle.id,
            manager->region_node->handle.tree_id, reduce_pre, reduce_post,
            manager->redop, field_set);
      }
#endif
      return reduce_post;
    }

    //--------------------------------------------------------------------------
    Event ReductionView::perform_deferred_across_reduction(
                              MaterializedView *target, FieldID dst_field, 
                              FieldID src_field, unsigned src_index, 
                              const VersionInfo &version_info,
                              const std::set<Event> &preconds,
                              const std::set<Domain> &reduce_domains,
                              Event dom_precondition, Operation *op)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, PERFORM_REDUCTION_CALL);
#endif
      std::vector<Domain::CopySrcDstField> src_fields;
      std::vector<Domain::CopySrcDstField> dst_fields;
      const bool fold = false;
      target->copy_field(dst_field, dst_fields);
      FieldMask red_mask; red_mask.set_bit(src_index);
      this->reduce_from(manager->redop, red_mask, src_fields);

      LegionMap<Event,FieldMask>::aligned src_pre;
      // Don't need to ask the target for preconditions as they 
      // are included as part of the pre set
      find_copy_preconditions(manager->redop, true/*reading*/,
                              red_mask, version_info, src_pre);
      std::set<Event> preconditions = preconds;
      if (dom_precondition.exists())
        preconditions.insert(dom_precondition);
      for (LegionMap<Event,FieldMask>::aligned::const_iterator it = 
            src_pre.begin(); it != src_pre.end(); it++)
      {
        preconditions.insert(it->first);
      }
      Event reduce_pre = Event::merge_events(preconditions); 
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
      if (!reduce_pre.exists())
      {
        UserEvent new_reduce_pre = UserEvent::create_user_event();
        new_reduce_pre.trigger();
        reduce_pre = new_reduce_pre;
      }
#endif
#ifdef LEGION_LOGGING
      LegionLogging::log_event_dependences(
          Processor::get_executing_processor(), preconditions, reduce_pre);
#endif
#ifdef LEGION_SPY
      LegionSpy::log_event_dependences(preconditions, reduce_pre);
#endif
      std::set<Event> post_events;
      for (std::set<Domain>::const_iterator it = reduce_domains.begin();
            it != reduce_domains.end(); it++)
      {
        Event post = manager->issue_reduction(op, src_fields, dst_fields,
                                              *it, reduce_pre, fold,
                                              false/*precise*/);
        post_events.insert(post);
      }
      Event reduce_post = Event::merge_events(post_events);
      // No need to add the user to the destination as that will
      // be handled by the caller using the reduce post event we return
      add_copy_user(manager->redop, reduce_post, version_info,
                    red_mask, true/*reading*/);
#if defined(LEGION_SPY) || defined(LEGION_LOGGING)
      IndexSpace reduce_index_space =
              target->logical_node->as_region_node()->row_source->handle;
      if (!reduce_post.exists())
      {
        UserEvent new_reduce_post = UserEvent::create_user_event();
        new_reduce_post.trigger();
        reduce_post = new_reduce_post;
      }
#endif
#ifdef LEGION_LOGGING
      {
        std::set<FieldID> reduce_fields;
        manager->region_node->column_source->to_field_set(red_mask,
                                                          reduce_fields);
        LegionLogging::log_lowlevel_copy(
            Processor::get_executing_processor(),
            manager->get_instance(),
            target->get_manager()->get_instance(),
            reduce_index_space,
            manager->region_node->column_source->handle,
            manager->region_node->handle.tree_id,
            reduce_pre, reduce_post, reduce_fields, manager->redop);
      }
#endif
#ifdef LEGION_SPY
      {
        std::set<FieldID> field_set;
        manager->region_node->column_source->to_field_set(red_mask, field_set);
        LegionSpy::log_copy_operation(manager->get_instance().id,
            target->get_manager()->get_instance().id,
            reduce_index_space.get_id(),
            manager->region_node->column_source->handle.id,
            manager->region_node->handle.tree_id, reduce_pre, reduce_post,
            manager->redop, field_set);
      }
#endif
      return reduce_post;
    }

    //--------------------------------------------------------------------------
    bool ReductionView::is_materialized_view(void) const
    //--------------------------------------------------------------------------
    {
      return false;
    }

    //--------------------------------------------------------------------------
    bool ReductionView::is_reduction_view(void) const
    //--------------------------------------------------------------------------
    {
      return true;
    }

    //--------------------------------------------------------------------------
    MaterializedView* ReductionView::as_materialized_view(void) const
    //--------------------------------------------------------------------------
    {
      return NULL;
    }

    //--------------------------------------------------------------------------
    ReductionView* ReductionView::as_reduction_view(void) const
    //--------------------------------------------------------------------------
    {
      return const_cast<ReductionView*>(this);
    }

    //--------------------------------------------------------------------------
    PhysicalManager* ReductionView::get_manager(void) const
    //--------------------------------------------------------------------------
    {
      return manager;
    }

    //--------------------------------------------------------------------------
    LogicalView* ReductionView::get_subview(const ColorPoint &c)
    //--------------------------------------------------------------------------
    {
      // Right now we don't make sub-views for reductions
      return this;
    }

    //--------------------------------------------------------------------------
    void ReductionView::find_copy_preconditions(ReductionOpID redop,
                                                bool reading,
                                                const FieldMask &copy_mask,
                                                const VersionInfo &version_info,
                             LegionMap<Event,FieldMask>::aligned &preconditions)
    //--------------------------------------------------------------------------
    {
      Event use_event = manager->get_use_event();
      if (use_event.exists())
      {
        LegionMap<Event,FieldMask>::aligned::iterator finder = 
            preconditions.find(use_event);
        if (finder == preconditions.end())
          preconditions[use_event] = copy_mask;
        else
          finder->second |= copy_mask;
      }
      AutoLock v_lock(view_lock,1,false/*exclusive*/);
      if (reading)
      {
        // Register dependences on any reducers
        for (LegionMap<Event,EventUsers>::aligned::const_iterator rit = 
              reduction_users.begin(); rit != reduction_users.end(); rit++)
        {
          const EventUsers &event_users = rit->second;
          if (event_users.single)
          {
            FieldMask overlap = copy_mask & event_users.user_mask;
            if (!overlap)
              continue;
            LegionMap<Event,FieldMask>::aligned::iterator finder = 
              preconditions.find(rit->first);
            if (finder == preconditions.end())
              preconditions[rit->first] = overlap;
            else
              finder->second |= overlap;
          }
          else
          {
            if (!(copy_mask * event_users.user_mask))
            {
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                    it = event_users.users.multi_users->begin(); it !=
                    event_users.users.multi_users->end(); it++)
              {
                FieldMask overlap = copy_mask & it->second;
                if (!overlap)
                  continue;
                LegionMap<Event,FieldMask>::aligned::iterator finder = 
                  preconditions.find(rit->first);
                if (finder == preconditions.end())
                  preconditions[rit->first] = overlap;
                else
                  finder->second |= overlap;
              }
            }
          }
        }
      }
      else
      {
        // Register dependences on any readers
        for (LegionMap<Event,EventUsers>::aligned::const_iterator rit = 
              reading_users.begin(); rit != reading_users.end(); rit++)
        {
          const EventUsers &event_users = rit->second;
          if (event_users.single)
          {
            FieldMask overlap = copy_mask & event_users.user_mask;
            if (!overlap)
              continue;
            LegionMap<Event,FieldMask>::aligned::iterator finder = 
              preconditions.find(rit->first);
            if (finder == preconditions.end())
              preconditions[rit->first] = overlap;
            else
              finder->second |= overlap;
          }
          else
          {
            if (!(copy_mask * event_users.user_mask))
            {
              for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                    it = event_users.users.multi_users->begin(); it !=
                    event_users.users.multi_users->end(); it++)
              {
                FieldMask overlap = copy_mask & it->second;
                if (!overlap)
                  continue;
                LegionMap<Event,FieldMask>::aligned::iterator finder = 
                  preconditions.find(rit->first);
                if (finder == preconditions.end())
                  preconditions[rit->first] = overlap;
                else
                  finder->second |= overlap;
              }
            }
          }
        }
      }
    }

    //--------------------------------------------------------------------------
    void ReductionView::add_copy_user(ReductionOpID redop, Event copy_term,
                                      const VersionInfo &version_info,
                                      const FieldMask &mask, bool reading)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ADD_COPY_USER_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(redop == manager->redop);
#endif
      
      // Quick test: only need to do this if copy term exists
      bool issue_collect = false;
      if (copy_term.exists())
      {
        PhysicalUser *user;
        // We don't use field versions for doing interference 
        // tests on reductions so no need to record it
        if (reading)
        {
          RegionUsage usage(READ_ONLY, EXCLUSIVE, 0);
          user = legion_new<PhysicalUser>(usage, ColorPoint());
        }
        else
        {
          RegionUsage usage(REDUCE, EXCLUSIVE, redop);
          user = legion_new<PhysicalUser>(usage, ColorPoint());
        }
        AutoLock v_lock(view_lock);
        add_physical_user(user, reading, copy_term, mask);
        // Update the reference users
        if (outstanding_gc_events.find(copy_term) ==
            outstanding_gc_events.end())
        {
          outstanding_gc_events.insert(copy_term);
          issue_collect = true;
        }
      }
      // Launch the garbage collection task if necessary
      if (issue_collect)
        defer_collect_user(copy_term);
    }

    //--------------------------------------------------------------------------
    InstanceRef ReductionView::add_user(const RegionUsage &usage, 
                                        Event term_event,
                                        const FieldMask &user_mask,
                                        const VersionInfo &version_info)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, ADD_USER_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      if (IS_REDUCE(usage))
        assert(usage.redop == manager->redop);
      else
        assert(IS_READ_ONLY(usage));
#endif
      const bool reading = IS_READ_ONLY(usage);
      std::set<Event> wait_on;
      Event use_event = manager->get_use_event();
      if (use_event.exists())
        wait_on.insert(use_event);
      // Who cares just hold the lock in exlcusive mode, this analysis
      // shouldn't be too expensive for reduction views
      bool issue_collect = false;
      PhysicalUser *new_user;
      // We don't use field versions for doing interference 
      // tests on reductions so no need to record it
      if (reading)
        new_user = legion_new<PhysicalUser>(usage, ColorPoint());
      else
        new_user = legion_new<PhysicalUser>(usage, ColorPoint());
      {
        AutoLock v_lock(view_lock);
        if (!reading)
        {
          // Reducing
          for (LegionMap<Event,EventUsers>::aligned::const_iterator rit = 
                reading_users.begin(); rit != reading_users.end(); rit++)
          {
            const EventUsers &event_users = rit->second;
            if (event_users.single)
            {
              FieldMask overlap = user_mask & event_users.user_mask;
              if (!overlap)
                continue;
              wait_on.insert(rit->first);
            }
            else
            {
              if (!(user_mask * event_users.user_mask))
              {
                for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator
                      it = event_users.users.multi_users->begin(); it !=
                      event_users.users.multi_users->end(); it++)
                {
                  FieldMask overlap = user_mask & it->second;
                  if (!overlap)
                    continue;
                  // Once we have one event precondition we are done
                  wait_on.insert(rit->first);
                  break;
                }
              }
            }
          }
          add_physical_user(new_user, false/*reading*/, term_event, user_mask);
        }
        else // We're reading so wait on any reducers
        {
          for (LegionMap<Event,EventUsers>::aligned::const_iterator rit = 
                reduction_users.begin(); rit != reduction_users.end(); rit++)
          {
            const EventUsers &event_users = rit->second;
            if (event_users.single)
            {
              FieldMask overlap = user_mask & event_users.user_mask;
              if (!overlap)
                continue;
              wait_on.insert(rit->first);
            }
            else
            {
              if (!(user_mask * event_users.user_mask))
              {
                for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                      it = event_users.users.multi_users->begin(); it !=
                      event_users.users.multi_users->end(); it++)
                {
                  FieldMask overlap = user_mask & it->second;
                  if (!overlap)
                    continue;
                  // Once we have one event precondition we are done
                  wait_on.insert(rit->first);
                  break;
                }
              }
            }
          }
          add_physical_user(new_user, true/*reading*/, term_event, user_mask);
        }
        // Only need to do this if we actually have a term event
        if (outstanding_gc_events.find(term_event) ==
            outstanding_gc_events.end())
        {
          outstanding_gc_events.insert(term_event);
          issue_collect = true;
        }
      }
      // Launch the garbage collection task if we need to
      if (issue_collect)
        defer_collect_user(term_event);
      // Return our result
      Event result = Event::merge_events(wait_on);
#if defined(LEGION_LOGGING) || defined(LEGION_SPY)
      if (!result.exists())
      {
        UserEvent new_result = UserEvent::create_user_event();
        new_result.trigger();
        result = new_result;
      }
#endif
#ifdef LEGION_LOGGING
      LegionLogging::log_event_dependences(
          Processor::get_executing_processor(), wait_on, result);
#endif
#ifdef LEGION_SPY
      LegionSpy::log_event_dependences(wait_on, result);
#endif
      return InstanceRef(result, this);
    }

    //--------------------------------------------------------------------------
    void ReductionView::add_physical_user(PhysicalUser *user, bool reading,
                                          Event term_event, 
                                          const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      // Better already be holding the lock
      EventUsers *event_users;
      if (reading)
        event_users = &(reading_users[term_event]);
      else
        event_users = &(reduction_users[term_event]);
      if (event_users->single)
      {
        if (event_users->users.single_user == NULL)
        {
          // make it the entry
          event_users->users.single_user = user;
          event_users->user_mask = user_mask;
        }
        else
        {
          // convert to multi
          LegionMap<PhysicalUser*,FieldMask>::aligned *new_map = 
                           new LegionMap<PhysicalUser*,FieldMask>::aligned();
          (*new_map)[event_users->users.single_user] = event_users->user_mask;
          (*new_map)[user] = user_mask;
          event_users->user_mask |= user_mask;
          event_users->users.multi_users = new_map;
          event_users->single = false;
        }
      }
      else
      {
        // Add it to the set 
        (*event_users->users.multi_users)[user] = user_mask;
        event_users->user_mask |= user_mask;
      }
    }

    //--------------------------------------------------------------------------
    void ReductionView::filter_local_users(Event term_event)
    //--------------------------------------------------------------------------
    {
      // Better be holding the lock before calling this
      std::set<Event>::iterator event_finder = 
        outstanding_gc_events.find(term_event);
      if (event_finder != outstanding_gc_events.end())
      {
        LegionMap<Event,EventUsers>::aligned::iterator finder = 
          reduction_users.find(term_event);
        if (finder != reduction_users.end())
        {
          EventUsers &event_users = finder->second;
          if (event_users.single)
          {
            legion_delete(event_users.users.single_user);
          }
          else
          {
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator it
                  = event_users.users.multi_users->begin(); it !=
                  event_users.users.multi_users->end(); it++)
            {
              legion_delete(it->first);
            }
            delete event_users.users.multi_users;
          }
          reduction_users.erase(finder);
        }
        finder = reading_users.find(term_event);
        if (finder != reading_users.end())
        {
          EventUsers &event_users = finder->second;
          if (event_users.single)
          {
            legion_delete(event_users.users.single_user);
          }
          else
          {
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator 
                  it = event_users.users.multi_users->begin(); it !=
                  event_users.users.multi_users->end(); it++)
            {
              legion_delete(it->first);
            }
            delete event_users.users.multi_users;
          }
          reading_users.erase(finder);
        }
        outstanding_gc_events.erase(event_finder);
      }
    }

    //--------------------------------------------------------------------------
    void ReductionView::add_initial_user(Event term_event, 
                                         const RegionUsage &usage,
                                         const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      // We don't use field versions for doing interference tests on
      // reductions so there is no need to record it
      PhysicalUser *user = legion_new<PhysicalUser>(usage, ColorPoint()); 
      add_physical_user(user, IS_READ_ONLY(usage), term_event, user_mask);
      initial_user_events.insert(term_event);
      // Don't need to actual launch a collection task, destructor
      // will handle this case
      outstanding_gc_events.insert(term_event);
    }
 
    //--------------------------------------------------------------------------
    bool ReductionView::reduce_to(ReductionOpID redop, 
                                  const FieldMask &reduce_mask,
                              std::vector<Domain::CopySrcDstField> &dst_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REDUCE_TO_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(redop == manager->redop);
#endif
      // Get the destination fields for this copy
      manager->find_field_offsets(reduce_mask, dst_fields);
      return manager->is_foldable();
    }

    //--------------------------------------------------------------------------
    void ReductionView::reduce_from(ReductionOpID redop,
                                    const FieldMask &reduce_mask,
                              std::vector<Domain::CopySrcDstField> &src_fields)
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_PERF
      PerfTracer tracer(context, REDUCE_FROM_CALL);
#endif
#ifdef DEBUG_HIGH_LEVEL
      assert(redop == manager->redop);
#endif
      manager->find_field_offsets(reduce_mask, src_fields);
    }

    //--------------------------------------------------------------------------
    void ReductionView::copy_to(const FieldMask &copy_mask,
                               std::vector<Domain::CopySrcDstField> &dst_fields)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    void ReductionView::copy_from(const FieldMask &copy_mask,
                               std::vector<Domain::CopySrcDstField> &src_fields)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
    }

    //--------------------------------------------------------------------------
    bool ReductionView::has_war_dependence(const RegionUsage &usage,
                                           const FieldMask &user_mask)
    //--------------------------------------------------------------------------
    {
      // should never be called
      assert(false);
      return false;
    } 

    //--------------------------------------------------------------------------
    void ReductionView::notify_active(void)
    //--------------------------------------------------------------------------
    {
      manager->add_nested_gc_ref(did);
    }

    //--------------------------------------------------------------------------
    void ReductionView::notify_inactive(void)
    //--------------------------------------------------------------------------
    {
      // No need to check for deletion of the manager since
      // we know that we also hold a resource reference
      manager->remove_nested_gc_ref(did);
    }

    //--------------------------------------------------------------------------
    void ReductionView::notify_valid(void)
    //--------------------------------------------------------------------------
    {
      manager->add_nested_valid_ref(did);
    }

    //--------------------------------------------------------------------------
    void ReductionView::notify_invalid(void)
    //--------------------------------------------------------------------------
    {
      manager->remove_nested_valid_ref(did);
    }

    //--------------------------------------------------------------------------
    void ReductionView::collect_users(const std::set<Event> &term_events)
    //--------------------------------------------------------------------------
    {
      // Do not do this if we are in LegionSpy so we can see 
      // all of the dependences
#if !defined(LEGION_SPY) && !defined(LEGION_LOGGING) && \
      !defined(EVENT_GRAPH_TRACE)
      AutoLock v_lock(view_lock);
      for (std::set<Event>::const_iterator it = term_events.begin();
            it != term_events.end(); it++)
      {
        filter_local_users(*it); 
      }
#endif
    }

    //--------------------------------------------------------------------------
    DistributedID ReductionView::send_view_base(AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      if (!has_remote_instance(target))
      {
        // If we are the parent we have to do the send
        // Send the physical manager first
        DistributedID manager_did = manager->send_manager(target);
#ifdef DEBUG_HIGH_LEVEL
        assert(logical_node->is_region()); // Always regions at the top
#endif
        // Don't take the lock, it's alright to have duplicate sends
        Serializer rez;
        {
          RezCheck z(rez);
          rez.serialize(did);
          rez.serialize(manager_did);
          rez.serialize(logical_node->as_region_node()->handle);
          rez.serialize(owner_space);
        }
        runtime->send_reduction_view(target, rez);
        update_remote_instances(target);
      }
      return did;
    }

    //--------------------------------------------------------------------------
    void ReductionView::send_view_updates(AddressSpaceID target,
                                          const FieldMask &update_mask)
    //--------------------------------------------------------------------------
    {
      Serializer reduction_rez, reading_rez;
      std::deque<PhysicalUser*> red_users, read_users;
      unsigned reduction_events = 0, reading_events = 0;
      {
        AutoLock v_lock(view_lock,1,false/*exclusive*/);
        for (LegionMap<Event,EventUsers>::aligned::const_iterator rit = 
              reduction_users.begin(); rit != reduction_users.end(); rit++)
        {
          FieldMask overlap = rit->second.user_mask & update_mask;
          if (!overlap)
            continue;
          reduction_events++;
          const EventUsers &event_users = rit->second;
          reduction_rez.serialize(rit->first);
          if (event_users.single)
          {
            reduction_rez.serialize<size_t>(1);
            reduction_rez.serialize(overlap);
            red_users.push_back(event_users.users.single_user);
          }
          else
          {
            reduction_rez.serialize<size_t>(
                                      event_users.users.multi_users->size());
            // Just send them all
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator
                  it = event_users.users.multi_users->begin(); it != 
                  event_users.users.multi_users->end(); it++)
            {
              reduction_rez.serialize(it->second);
              red_users.push_back(it->first);
            }
          }
        }
        for (LegionMap<Event,EventUsers>::aligned::const_iterator rit = 
              reading_users.begin(); rit != reading_users.end(); rit++)
        {
          FieldMask overlap = rit->second.user_mask & update_mask;
          if (!overlap)
            continue;
          reading_events++;
          const EventUsers &event_users = rit->second;
          reading_rez.serialize(rit->first);
          if (event_users.single)
          {
            reading_rez.serialize<size_t>(1);
            reading_rez.serialize(overlap);
            read_users.push_back(event_users.users.single_user);
          }
          else
          {
            reading_rez.serialize<size_t>(
                                      event_users.users.multi_users->size());
            // Just send them all
            for (LegionMap<PhysicalUser*,FieldMask>::aligned::const_iterator
                  it = event_users.users.multi_users->begin(); it != 
                  event_users.users.multi_users->end(); it++)
            {
              reading_rez.serialize(it->second);
              read_users.push_back(it->first);
            }
          }
        }
      }
      // We've released the lock, so reassemble the message
      Serializer rez;
      {
        RezCheck z(rez);
#ifdef DEBUG_HIGH_LEVEL
        assert(logical_node->is_region());
#endif
        rez.serialize(logical_node->as_region_node()->handle);
        rez.serialize(did);
        rez.serialize<size_t>(red_users.size());
        for (std::deque<PhysicalUser*>::const_iterator it = 
              red_users.begin(); it != red_users.end(); it++)
        {
          (*it)->pack_user(rez);
        }
        rez.serialize<size_t>(read_users.size());
        for (std::deque<PhysicalUser*>::const_iterator it = 
              read_users.begin(); it != read_users.end(); it++)
        {
          (*it)->pack_user(rez);
        }
        rez.serialize(reduction_events);
        size_t reduction_size = reduction_rez.get_used_bytes(); 
        rez.serialize(reduction_rez.get_buffer(), reduction_size);
        rez.serialize(reading_events);
        size_t reading_size = reading_rez.get_used_bytes();
        rez.serialize(reading_rez.get_buffer(), reading_size);
      }
      runtime->send_reduction_update(target, rez);
    }

    //--------------------------------------------------------------------------
    void ReductionView::process_update(Deserializer &derez, 
                                       AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      size_t num_reduction_users;
      derez.deserialize(num_reduction_users);
      std::vector<PhysicalUser*> red_users(num_reduction_users);
      FieldSpaceNode *field_node = logical_node->column_source;
      for (unsigned idx = 0; idx < num_reduction_users; idx++)
        red_users[idx] = PhysicalUser::unpack_user(derez, field_node, 
                                                   source, true/*add ref*/);
      size_t num_reading_users;
      derez.deserialize(num_reading_users);
      std::deque<PhysicalUser*> read_users(num_reading_users);
      for (unsigned idx = 0; idx < num_reading_users; idx++)
        read_users[idx] = PhysicalUser::unpack_user(derez, field_node, 
                                                    source, true/*add ref*/);
      std::deque<Event> collect_events;
      {
        unsigned reduction_index = 0, reading_index = 0;
        unsigned num_reduction_events;
        derez.deserialize(num_reduction_events);
        AutoLock v_lock(view_lock);
        for (unsigned idx = 0; idx < num_reduction_events; idx++)
        {
          Event red_event;
          derez.deserialize(red_event);
          size_t num_users;
          derez.deserialize(num_users);
          for (unsigned idx2 = 0; idx2 < num_users; idx2++)
          {
            FieldMask user_mask;
            derez.deserialize(user_mask);
            field_node->transform_field_mask(user_mask, source);
            add_physical_user(red_users[reduction_index++], false/*reading*/,
                              red_event, user_mask);
          }
          if (outstanding_gc_events.find(red_event) == 
              outstanding_gc_events.end())
          {
            outstanding_gc_events.insert(red_event);
            collect_events.push_back(red_event);
          }
        }
        unsigned num_reading_events;
        derez.deserialize(num_reading_events);
        for (unsigned idx = 0; idx < num_reading_events; idx++)
        {
          Event read_event;
          derez.deserialize(read_event);
          size_t num_users;
          derez.deserialize(num_users);
          for (unsigned idx2 = 0; idx2 < num_users; idx2++)
          {
            FieldMask user_mask;
            derez.deserialize(user_mask);
            field_node->transform_field_mask(user_mask, source);
            add_physical_user(read_users[reading_index++], true/*reading*/,
                              read_event, user_mask);
          }
          if (outstanding_gc_events.find(read_event) ==
              outstanding_gc_events.end())
          {
            outstanding_gc_events.insert(read_event);
            collect_events.push_back(read_event);
          }
        }
      }
      if (!collect_events.empty())
      {
        for (std::deque<Event>::const_iterator it = collect_events.begin();
              it != collect_events.end(); it++)
        {
          defer_collect_user(*it);
        }
      }
    }

    //--------------------------------------------------------------------------
    Memory ReductionView::get_location(void) const
    //--------------------------------------------------------------------------
    {
      return manager->memory;
    }

    //--------------------------------------------------------------------------
    ReductionOpID ReductionView::get_redop(void) const
    //--------------------------------------------------------------------------
    {
      return manager->redop;
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReductionView::handle_send_reduction_view(Runtime *runtime,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez); 
      DistributedID did;
      derez.deserialize(did);
      DistributedID manager_did;
      derez.deserialize(manager_did);
      LogicalRegion handle;
      derez.deserialize(handle);
      AddressSpaceID owner_space;
      derez.deserialize(owner_space);

      RegionNode *target_node = runtime->forest->get_node(handle);
      PhysicalManager *phy_man = target_node->find_manager(manager_did);
#ifdef DEBUG_HIGH_LEVEL
      assert(phy_man->is_reduction_manager());
#endif
      ReductionManager *red_manager = phy_man->as_reduction_manager();

      ReductionView *new_view = legion_new<ReductionView>(runtime->forest,
                                   did, owner_space, runtime->address_space,
                                   target_node, red_manager,
                                   false/*don't register yet*/);
      if (!target_node->register_logical_view(new_view))
        legion_delete(new_view);
      else
        new_view->update_remote_instances(source);
    }

    //--------------------------------------------------------------------------
    /*static*/ void ReductionView::handle_send_update(Runtime *runtime,
                                     Deserializer &derez, AddressSpaceID source)
    //--------------------------------------------------------------------------
    {
      DerezCheck z(derez);
      LogicalRegion handle;
      derez.deserialize(handle);
      RegionTreeNode *node = runtime->forest->get_node(handle);
      DistributedID did;
      derez.deserialize(did);
      LogicalView *view = node->find_view(did);
#ifdef DEBUG_HIGH_LEVEL
      assert(view->is_instance_view());
      assert(view->as_instance_view()->is_reduction_view());
#endif
      ReductionView *red_view = view->as_instance_view()->as_reduction_view();
      red_view->process_update(derez, source);
    }

    /////////////////////////////////////////////////////////////
    // MappingRef 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    MappingRef::MappingRef(void)
      : view(NULL), needed_fields(FieldMask())
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    MappingRef::MappingRef(LogicalView *v, const FieldMask &needed)
      : view(v), needed_fields(needed)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    MappingRef::MappingRef(const MappingRef &rhs)
      : view(rhs.view), needed_fields(rhs.needed_fields)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    MappingRef::~MappingRef(void)
    //--------------------------------------------------------------------------
    {
      view = NULL;
    }

    //--------------------------------------------------------------------------
    MappingRef& MappingRef::operator=(const MappingRef &rhs)
    //--------------------------------------------------------------------------
    {
      view = rhs.view;
      needed_fields = rhs.needed_fields;
      return *this;
    }

    /////////////////////////////////////////////////////////////
    // InstanceRef 
    /////////////////////////////////////////////////////////////

    //--------------------------------------------------------------------------
    InstanceRef::InstanceRef(void)
      : ready_event(Event::NO_EVENT), view(NULL), manager(NULL)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InstanceRef::InstanceRef(Event ready, InstanceView *v)
      : ready_event(ready), view(v), 
        manager((v == NULL) ? NULL : v->get_manager()) 
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    InstanceRef::InstanceRef(Event ready, InstanceView *v,
                             const std::vector<Reservation> &ls)
      : ready_event(ready), view(v), 
        manager((v == NULL) ? NULL : v->get_manager()), needed_locks(ls)
    //--------------------------------------------------------------------------
    {
    }

    //--------------------------------------------------------------------------
    MaterializedView* InstanceRef::get_materialized_view(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(view != NULL);
      assert(view->is_materialized_view());
#endif
      return view->as_materialized_view();
    }

    //--------------------------------------------------------------------------
    ReductionView* InstanceRef::get_reduction_view(void) const
    //--------------------------------------------------------------------------
    {
#ifdef DEBUG_HIGH_LEVEL
      assert(view != NULL);
      assert(view->is_reduction_view());
#endif
      return view->as_reduction_view();
    }

    //--------------------------------------------------------------------------
    void InstanceRef::update_atomic_locks(
                 std::map<Reservation,bool> &atomic_locks, bool exclusive) const
    //--------------------------------------------------------------------------
    {
      for (std::vector<Reservation>::const_iterator it = needed_locks.begin();
            it != needed_locks.end(); it++)
      {
        std::map<Reservation,bool>::iterator finder = 
          atomic_locks.find(*it);
        if (finder == atomic_locks.end())
          atomic_locks[*it] = exclusive;
        else
          finder->second = finder->second || exclusive;
      }
    }

    //--------------------------------------------------------------------------
    Memory InstanceRef::get_memory(void) const
    //--------------------------------------------------------------------------
    {
      return manager->memory;
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic> 
      InstanceRef::get_accessor(void) const
    //--------------------------------------------------------------------------
    {
      return manager->get_accessor();
    }

    //--------------------------------------------------------------------------
    Accessor::RegionAccessor<Accessor::AccessorType::Generic>
      InstanceRef::get_field_accessor(FieldID fid) const
    //--------------------------------------------------------------------------
    {
      return manager->get_field_accessor(fid);
    }

    //--------------------------------------------------------------------------
    void InstanceRef::pack_reference(Serializer &rez, AddressSpaceID target)
    //--------------------------------------------------------------------------
    {
      if (manager != NULL)
      {
        DistributedID did = manager->send_manager(target);
        rez.serialize(did);
        rez.serialize(ready_event);
        rez.serialize<size_t>(needed_locks.size());
        for (std::vector<Reservation>::const_iterator it = 
              needed_locks.begin(); it != needed_locks.end(); it++)
          rez.serialize(*it);
      }
      else
        rez.serialize<DistributedID>(0);
    }

    //--------------------------------------------------------------------------
    void InstanceRef::unpack_reference(Runtime *runtime, Deserializer &derez)
    //--------------------------------------------------------------------------
    {
      DistributedID did;
      derez.deserialize(did);
      if (did == 0)
        return;
      DistributedCollectable *dc = runtime->find_distributed_collectable(did);
#ifdef DEBUG_HIGH_LEVEL
      manager = dynamic_cast<PhysicalManager*>(dc);
      assert(manager != NULL);
#else
      manager = static_cast<PhysicalManager*>(dc);
#endif
      derez.deserialize(ready_event);
      size_t num_locks;
      derez.deserialize(num_locks);
      needed_locks.resize(num_locks);
      for (unsigned idx = 0; idx < num_locks; idx++)
        derez.deserialize(needed_locks[idx]); 
    } 

  }; // namespace HighLevel
}; // namespace LegionRuntime

// EOF

